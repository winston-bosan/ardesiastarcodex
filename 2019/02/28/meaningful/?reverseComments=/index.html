<!DOCTYPE html>
<html lang="en-US">
	<head>
		<meta charset="UTF-8" />
		<title>Meaningful | Slate Star Codex</title>
		<meta name=viewport content="width=device-width, min-width=572, initial-scale=1">
		<script type="text/javascript" src="https://slatestarcodex.com/wp-content/themes/responsive-pujugama-v3/viewport-min-width.js"></script>
		<link rel="profile" href="https://gmpg.org/xfn/11" />
		<link rel="stylesheet" type="text/css" media="all" href="https://slatestarcodex.com/wp-content/themes/responsive-pujugama-v3/style.css" />
		<link href='https://fonts.googleapis.com/css?family=Raleway' rel='stylesheet' type='text/css'>
		<link href='https://fonts.googleapis.com/css?family=Josefin+Sans' rel='stylesheet' type='text/css'>
		<link rel='stylesheet' id='font-awesome-css'  href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css' type='text/css' media='all' />

		<link rel="pingback" href="https://slatestarcodex.com/xmlrpc.php" />
		<link rel='dns-prefetch' href='//platform-api.sharethis.com' />
<link rel="alternate" type="application/rss+xml" title="Slate Star Codex &raquo; Feed" href="https://slatestarcodex.com/feed/" />
<link rel="alternate" type="application/rss+xml" title="Slate Star Codex &raquo; Comments Feed" href="https://slatestarcodex.com/comments/feed/" />
<link rel="alternate" type="application/rss+xml" title="Slate Star Codex &raquo; Meaningful Comments Feed" href="https://slatestarcodex.com/2019/02/28/meaningful/feed/" />
<link rel='stylesheet' id='wmsimplecaptcha_style_front-css'  href='https://slatestarcodex.com/wp-content/plugins/wm-simple-captcha/assets/css/wmsimplecaptcha_style.css?ver=5.4.1' type='text/css' media='all' />
<link rel='stylesheet' id='wp-block-library-css'  href='https://c0.wp.com/c/5.4.1/wp-includes/css/dist/block-library/style.min.css' type='text/css' media='all' />
<style id='wp-block-library-inline-css' type='text/css'>
.has-text-align-justify{text-align:justify;}
</style>
<link rel='stylesheet' id='easy_table_style-css'  href='https://slatestarcodex.com/wp-content/plugins/easy-table/themes/default/style.css?ver=1.8' type='text/css' media='all' />
<link rel='stylesheet' id='jetpack_css-css'  href='https://c0.wp.com/p/jetpack/8.5/css/jetpack.css' type='text/css' media='all' />
<script>if (document.location.protocol != "https:") {document.location = document.URL.replace(/^http:/i, "https:");}</script><script type='text/javascript' src='https://c0.wp.com/c/5.4.1/wp-includes/js/jquery/jquery.js'></script>
<script type='text/javascript' src='https://c0.wp.com/c/5.4.1/wp-includes/js/jquery/jquery-migrate.min.js'></script>
<script type='text/javascript'>
/* <![CDATA[ */
var ajax_object = {"ajax_url":"https:\/\/slatestarcodex.com\/wp-admin\/admin-ajax.php"};
/* ]]> */
</script>
<script type='text/javascript' src='https://slatestarcodex.com/wp-content/plugins/wm-simple-captcha/assets/js/wmsimplecaptcha_scripts.js?ver=5.4.1'></script>
<script type='text/javascript' src='https://slatestarcodex.com/wp-content/plugins/wp-hide-post/public/js/wp-hide-post-public.js?ver=2.0.10'></script>
<script type='text/javascript' src='//platform-api.sharethis.com/js/sharethis.js#product=ga&#038;property=5c350a7dad0b1400119dbb2c'></script>
<link rel='https://api.w.org/' href='https://slatestarcodex.com/wp-json/' />
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://slatestarcodex.com/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="https://slatestarcodex.com/wp-includes/wlwmanifest.xml" /> 
<link rel='prev' title='In Mod We Trust' href='https://slatestarcodex.com/2019/02/27/in-mod-we-trust/' />
<link rel='next' title='Open Thread 122.5' href='https://slatestarcodex.com/2019/03/03/open-thread-122-5/' />
<link rel="canonical" href="https://slatestarcodex.com/2019/02/28/meaningful/" />
<link rel='shortlink' href='https://slatestarcodex.com/?p=5412' />
<link rel="alternate" type="application/json+oembed" href="https://slatestarcodex.com/wp-json/oembed/1.0/embed?url=https%3A%2F%2Fslatestarcodex.com%2F2019%2F02%2F28%2Fmeaningful%2F" />
<link rel="alternate" type="text/xml+oembed" href="https://slatestarcodex.com/wp-json/oembed/1.0/embed?url=https%3A%2F%2Fslatestarcodex.com%2F2019%2F02%2F28%2Fmeaningful%2F&#038;format=xml" />

<link rel='dns-prefetch' href='//c0.wp.com'/>
<style type='text/css'>img#wpstats{display:none}</style>			<style type="text/css">
				.pmcc-comments-report-link {
					font: 10px sans-serif;
					display:block;
					float:right;
					clear: left;
					margin-top: 10px;
				}
				.pmcc-comments-report-link a {
					color: #9C3E3E;
					padding: 2px 5px;
					margin: 2px 0 0 5px;
					border: 1px solid #ddd;
				}
				
				.pmcc-comments-report-link strong {
				    color: white;
				    background: #c0392b;
				    padding-top: 2px;
				    border-radius: 7px;
				    display: block;
				    width: 15px;
				    height: 15px;
				    text-align: center;
				    margin-right: 10px;
				}
			</style>
			
<!-- Jetpack Open Graph Tags -->
<meta property="og:type" content="article" />
<meta property="og:title" content="Meaningful" />
<meta property="og:url" content="https://slatestarcodex.com/2019/02/28/meaningful/" />
<meta property="og:description" content="[With apologies to Putnam, Pope, and all of you] Two children are reading a text written by an AI: The boys splashed water in each other&#8217;s faces until they were both sopping wet One child sayâ€¦" />
<meta property="article:published_time" content="2019-03-01T06:06:48+00:00" />
<meta property="article:modified_time" content="2019-11-17T00:16:09+00:00" />
<meta property="og:site_name" content="Slate Star Codex" />
<meta property="og:image" content="https://slatestarcodex.com/wp-content/themes/two_column_pujugama/images/codex_spotlight.png" />
<meta property="og:locale" content="en_US" />
<meta name="twitter:text:title" content="Meaningful" />
<meta name="twitter:card" content="summary" />
<meta property="og:image:width" content="200" />
<meta property="og:image:height" content="200" />
<meta name="twitter:site" content="@slatestarcodex" />

<!-- End Jetpack Open Graph Tags -->
		<style type="text/css" id="wp-custom-css">
			.hentry {
	margin-bottom: 46px
}

#pjgm-content .pjgm-posttitle {
	margin-bottom: 18px;
}
.pjgm-postcontent {
	padding-top:36px; 
}

/* 2019/07/04
   added by Andrew Swift
   mail@andrewswift.com */

@media all and
(max-device-width: 480px)
{ blockquote {
   -webkit-text-size-adjust:
    140% !important; }}		</style>
				<script src="https://polyfill.io/v2/polyfill.min.js?features=IntersectionObserver"></script>
	</head>

	<body data-rsssl=1 class="post-template-default single single-post postid-5412 single-format-standard s1-collapse s2-collapse">
		<div id="pjgm-wrap">
			<div id="pjgm-header">
				<div id="pjgm-menubar">
					<a href="https://slatestarcodex.com/" class="pjgm-home" title="Slate Star Codex" rel="home">Home</a>
					<div class="menu"><ul>
<li class="page_item page-item-2"><a href="https://slatestarcodex.com/about/">About / Top Posts</a></li>
<li class="page_item page-item-5559"><a href="https://psychiat-list.slatestarcodex.com/">Psychiat-List</a></li>
<li class="page_item page-item-2091"><a href="https://slatestarcodex.com/archives/">Archives</a></li>
<li class="page_item page-item-4475"><a href="https://www.lesswrong.com/community?filters=SSC">Meetups</a></li>
<li class="page_item page-item-3837"><a href="https://slatestarcodex.com/mistakes/">Mistakes</a></li>
<li class="page_item page-item-1745"><a href="https://slatestarcodex.com/comments/">Comments</a></li>
<li class="page_item page-item-3942"><a href="https://slatestarcodex.com/advertise/">Advertise</a></li>
<li class="page_item page-item-3989"><a href="/tag/open/?latest">Open Thread</a></li>
</ul></div>
					<a href="https://slatestarcodex.com/comments/feed/" class="pjgm-feed">Comments Feed</a>
					<a href="https://slatestarcodex.com/feed/" class="pjgm-feed">RSS Feed</a>
				</div><!-- #pjgm-menubar -->

				<div id="pjgm-bigtitle">
										<div id="pjgm-title">
						<img src="https://slatestarcodex.com/wp-content/themes/responsive-pujugama-v3/images/codex.png" alt="codex" class="codex" />
						<span>
							<a href="https://slatestarcodex.com/" title="Slate Star Codex" rel="home">Slate Star Codex</a>
						</span>
						</div>
						<div id="pjgm-description">SELF-RECOMMENDING!</div>
			</div><!-- #pjgm-bigtitle -->
		</div><!-- #pjgm-header -->

		<div id="pjgm-main">
<div id="left-sidebar" class="widget-area" role="complementary">
	<a class="sidebar-toggle" title="Expand Sidebar"><i class="fa fa-angle-double-left"></i><i class="fa fa-angle-double-right"></i></a>
	<ul class="xoxo">
				<li id="recent-posts-2" class="widget-container widget_recent_entries">		<h3 class="widget-title">Recent Posts</h3>		<ul>
											<li>
					<a href="https://slatestarcodex.com/2020/05/20/links-5-20/">Links 5/20</a>
									</li>
											<li>
					<a href="https://slatestarcodex.com/2020/05/20/open-thread-154-25/">Open Thread 154.25</a>
									</li>
											<li>
					<a href="https://slatestarcodex.com/2020/05/18/coronalinks-5-18-20-when-all-you-have-is-a-hammer-everything-starts-looking-like-a-dance/">Coronalinks 5/18/20: When All You Have Is A Hammer, Everything Starts Looking Like A Dance</a>
									</li>
											<li>
					<a href="https://slatestarcodex.com/2020/05/17/open-thread-154/">Open Thread 154</a>
									</li>
											<li>
					<a href="https://slatestarcodex.com/2020/05/13/open-thread-153-75/">Open Thread 153.75</a>
									</li>
					</ul>
		</li><li id="text-5" class="widget-container widget_text"><h3 class="widget-title">Upcoming Meetups</h3>			<div class="textwidget"><p>Cancelled due to pandemic, sorry.</p>
</div>
		</li><li id="text-3" class="widget-container widget_text"><h3 class="widget-title">Blogroll</h3>			<div class="textwidget"></div>
		</li><li id="linkcat-100" class="widget-container widget_links"><h3 class="widget-title">Embalmed Ones</h3>
	<ul class='xoxo blogroll'>
<li><a href="http://blog.jaibot.com/">ANOIEAEIB</a></li>
<li><a href="http://commonsenseatheism.com/">Common Sense Atheism</a></li>
<li><a href="http://lesswrong.com">Less Wrong</a></li>
<li><a href="http://thelastpsychiatrist.com/">The Last Psychiatrist</a></li>

	</ul>
</li>
<li id="linkcat-95" class="widget-container widget_links"><h3 class="widget-title">Fabulous Ones</h3>
	<ul class='xoxo blogroll'>
<li><a href="http://alicorn.elcenia.com/board/index.php">Alicornutopia</a></li>
<li><a href="http://unsongbook.com">Unsong</a></li>
<li><a href="https://parahumans.wordpress.com/">Worm</a></li>

	</ul>
</li>
<li id="linkcat-98" class="widget-container widget_links"><h3 class="widget-title">Innumerable Ones</h3>
	<ul class='xoxo blogroll'>
<li><a href="https://www.gwern.net/">Gwern</a></li>
<li><a href="https://golem.ph.utexas.edu/category/">n-Category Cafe</a></li>
<li><a href="http://putanumonit.com/">Put A Number On It</a></li>
<li><a href="https://randomcriticalanalysis.wordpress.com">Random Critical Analysis</a></li>
<li><a href="http://www.scottaaronson.com/blog/">Shtetl-Optimized</a></li>
<li><a href="http://andrewgelman.com/">Statistical Modeling</a></li>
<li><a href="http://unenumerated.blogspot.com/">Unenumerated</a></li>

	</ul>
</li>
<li id="linkcat-94" class="widget-container widget_links"><h3 class="widget-title">Mermaids</h3>
	<ul class='xoxo blogroll'>
<li><a href="https://intelligence.org/blog/">MIRI</a></li>
<li><a href="http://freethoughtblogs.com/brutereason/">Miri</a></li>
<li><a href="http://nothingismere.com/">Nothing Is Mere</a></li>
<li><a href="https://themerelyreal.wordpress.com/">The Merely Real</a></li>

	</ul>
</li>
<li id="linkcat-96" class="widget-container widget_links"><h3 class="widget-title">Stray Dogs</h3>
	<ul class='xoxo blogroll'>
<li><a href="http://anonymousmugwump.blogspot.co.uk">Anonymous Mugwump</a></li>
<li><a href="http://followthesquirrel.blogspot.com/">Follow The Squirrel</a></li>
<li><a href="http://marginalrevolution.com/">Marginal Revolution</a></li>
<li><a href="https://nintil.com/">Nintil</a></li>
<li><a href="https://pseudoerasmus.com/">Pseudoerasmus</a></li>
<li><a href="http://www.themoneyillusion.com/">The Money Illusion</a></li>

	</ul>
</li>
<li id="linkcat-101" class="widget-container widget_links"><h3 class="widget-title">Suckling Pigs</h3>
	<ul class='xoxo blogroll'>
<li><a href="https://fredrikdeboer.com/">Fredrik deBoer</a></li>
<li><a href="http://unqualifiedrestaurants.tumblr.com/">Unqualified Restaurant Reservations</a></li>
<li><a href="http://wholehealthsource.blogspot.com/">Whole Health Source</a></li>

	</ul>
</li>
<li id="linkcat-91" class="widget-container widget_links"><h3 class="widget-title">Those Drawn With A Very Fine Camel Hair Brush</h3>
	<ul class='xoxo blogroll'>
<li><a href="http://existentialcomics.com/">Existential Comics</a></li>
<li><a href="http://www.smbc-comics.com/">Saturday Morning Breakfast Cereal</a></li>

	</ul>
</li>
<li id="linkcat-89" class="widget-container widget_links"><h3 class="widget-title">Those That Are Included In This Classification</h3>
	<ul class='xoxo blogroll'>
<li><a href="https://www.reddit.com/r/slatestarcodex/">r/slatestarcodex</a></li>
<li><a href="https://discord.gg/kAVSf9U" rel="nofollow">SSC Discord Server</a></li>
<li><a href="http://sscpodcast.libsyn.com/rss">SSC Podcast</a></li>

	</ul>
</li>
<li id="linkcat-93" class="widget-container widget_links"><h3 class="widget-title">Those That Are Trained</h3>
	<ul class='xoxo blogroll'>
<li><a href="https://80000hours.org/blog/">80000 Hours Blog</a></li>
<li><a href="http://aiimpacts.org/">AI Impacts</a></li>
<li><a href="http://www.effective-altruism.com/">Effective Altruism Forum</a></li>
<li><a href="http://blog.givewell.org/">GiveWell Blog</a></li>
<li><a href="http://www.jefftk.com/index">Jeff Kaufman</a></li>
<li><a href="http://lukemuehlhauser.com/">Luke Muehlhauser</a></li>
<li><a href="http://theunitofcaring.tumblr.com/">The Unit of Caring</a></li>
<li><a href="https://thewholesky.wordpress.com/">The Whole Sky</a></li>

	</ul>
</li>
<li id="linkcat-102" class="widget-container widget_links"><h3 class="widget-title">Those That At A Distance Resemble Flies</h3>
	<ul class='xoxo blogroll'>
<li><a href="http://agentyduck.blogspot.com/">Agenty Duck</a></li>
<li><a href="https://www.beeminder.com/">Beeminder</a></li>
<li><a href="http://mindingourway.com/">Nate Soares</a></li>

	</ul>
</li>
<li id="linkcat-99" class="widget-container widget_links"><h3 class="widget-title">Those That Belong To The Emperor</h3>
	<ul class='xoxo blogroll'>
<li><a href="https://samzdat.com">Sam[]zdat</a></li>
<li><a href="http://www.xenosystems.net/">Xenosystems</a></li>

	</ul>
</li>
<li id="linkcat-97" class="widget-container widget_links"><h3 class="widget-title">Those That Have Just Broken The Flower Vase</h3>
	<ul class='xoxo blogroll'>
<li><a href="http://esr.ibiblio.org/">Armed and Dangerous</a></li>
<li><a href="https://www.econlib.org/author/bcaplan/">Bryan Caplan</a></li>
<li><a href="http://daviddfriedman.blogspot.com/">David Friedman</a></li>
<li><a href="https://thezvi.wordpress.com/">Don&#039;t Worry About The Vase</a></li>
<li><a href="http://www.overcomingbias.com/">Overcoming Bias</a></li>
<li><a href="http://www.popehat.com/">Popehat</a></li>
<li><a href="http://sci-hub.tw/">Sci-Hub</a></li>

	</ul>
</li>
<li id="linkcat-90" class="widget-container widget_links"><h3 class="widget-title">Those That Tremble As Though They Are Mad</h3>
	<ul class='xoxo blogroll'>
<li><a href="http://1boringoldman.com/">1 Boring Old Man</a></li>
<li><a href="http://acesounderglass.com/">Aceso Under Glass</a></li>
<li><a href="http://cognitionandevolution.blogspot.com/">Cognition and Evolution</a></li>
<li><a href="https://crazymeds.net/pmwiki/pmwiki.php">Crazy Meds</a></li>
<li><a href="https://www.erikandersontherapy.com/blog/">Erik Anderson Therapy</a></li>
<li><a href="http://gruntledandhinged.wordpress.com/">Gruntled and Hinged</a></li>
<li><a href="http://real-psychiatry.blogspot.com/">Real Psychiatry</a></li>
<li><a href="http://psychiatrist-blog.blogspot.com/">Shrink Rap</a></li>

	</ul>
</li>
<li id="linkcat-92" class="widget-container widget_links"><h3 class="widget-title">Various Others</h3>
	<ul class='xoxo blogroll'>
<li><a href="http://zackmdavis.net/blog/">An Algorithmic Lucidity</a></li>
<li><a href="https://medium.com/@sumdepony">De Pony Sun</a></li>
<li><a href="https://www.gnxp.com/">Gene Expression</a></li>
<li><a href="http://infoproc.blogspot.com/">Information Processing</a></li>
<li><a href="http://www.meltingasphalt.com/">Melting Asphalt</a></li>
<li><a href="https://meteuphoric.wordpress.com/">Meteuphoric</a></li>
<li><a href="https://srconstantin.wordpress.com/">Otium</a></li>
<li><a href="http://www.ribbonfarm.com/">Ribbonfarm</a></li>
<li><a href="http://rationalconspiracy.com/">The Rationalist Conspiracy</a></li>
<li><a href="https://sideways-view.com/">The Sideways View</a></li>
<li><a href="http://thingofthings.wordpress.com/">Thing of Things</a></li>
<li><a href="https://westhunt.wordpress.com/">West Hunter</a></li>

	</ul>
</li>
<li id="archives-2" class="widget-container widget_archive"><h3 class="widget-title">Archives</h3>		<ul>
				<li><a href='https://slatestarcodex.com/2020/05/'>May 2020</a></li>
	<li><a href='https://slatestarcodex.com/2020/04/'>April 2020</a></li>
	<li><a href='https://slatestarcodex.com/2020/03/'>March 2020</a></li>
	<li><a href='https://slatestarcodex.com/2020/02/'>February 2020</a></li>
	<li><a href='https://slatestarcodex.com/2020/01/'>January 2020</a></li>
	<li><a href='https://slatestarcodex.com/2019/12/'>December 2019</a></li>
	<li><a href='https://slatestarcodex.com/2019/11/'>November 2019</a></li>
	<li><a href='https://slatestarcodex.com/2019/10/'>October 2019</a></li>
	<li><a href='https://slatestarcodex.com/2019/09/'>September 2019</a></li>
	<li><a href='https://slatestarcodex.com/2019/08/'>August 2019</a></li>
	<li><a href='https://slatestarcodex.com/2019/07/'>July 2019</a></li>
	<li><a href='https://slatestarcodex.com/2019/06/'>June 2019</a></li>
	<li><a href='https://slatestarcodex.com/2019/05/'>May 2019</a></li>
	<li><a href='https://slatestarcodex.com/2019/04/'>April 2019</a></li>
	<li><a href='https://slatestarcodex.com/2019/03/'>March 2019</a></li>
	<li><a href='https://slatestarcodex.com/2019/02/'>February 2019</a></li>
	<li><a href='https://slatestarcodex.com/2019/01/'>January 2019</a></li>
	<li><a href='https://slatestarcodex.com/2018/12/'>December 2018</a></li>
	<li><a href='https://slatestarcodex.com/2018/11/'>November 2018</a></li>
	<li><a href='https://slatestarcodex.com/2018/10/'>October 2018</a></li>
	<li><a href='https://slatestarcodex.com/2018/09/'>September 2018</a></li>
	<li><a href='https://slatestarcodex.com/2018/08/'>August 2018</a></li>
	<li><a href='https://slatestarcodex.com/2018/07/'>July 2018</a></li>
	<li><a href='https://slatestarcodex.com/2018/06/'>June 2018</a></li>
	<li><a href='https://slatestarcodex.com/2018/05/'>May 2018</a></li>
	<li><a href='https://slatestarcodex.com/2018/04/'>April 2018</a></li>
	<li><a href='https://slatestarcodex.com/2018/03/'>March 2018</a></li>
	<li><a href='https://slatestarcodex.com/2018/02/'>February 2018</a></li>
	<li><a href='https://slatestarcodex.com/2018/01/'>January 2018</a></li>
	<li><a href='https://slatestarcodex.com/2017/12/'>December 2017</a></li>
	<li><a href='https://slatestarcodex.com/2017/11/'>November 2017</a></li>
	<li><a href='https://slatestarcodex.com/2017/10/'>October 2017</a></li>
	<li><a href='https://slatestarcodex.com/2017/09/'>September 2017</a></li>
	<li><a href='https://slatestarcodex.com/2017/08/'>August 2017</a></li>
	<li><a href='https://slatestarcodex.com/2017/07/'>July 2017</a></li>
	<li><a href='https://slatestarcodex.com/2017/06/'>June 2017</a></li>
	<li><a href='https://slatestarcodex.com/2017/05/'>May 2017</a></li>
	<li><a href='https://slatestarcodex.com/2017/04/'>April 2017</a></li>
	<li><a href='https://slatestarcodex.com/2017/03/'>March 2017</a></li>
	<li><a href='https://slatestarcodex.com/2017/02/'>February 2017</a></li>
	<li><a href='https://slatestarcodex.com/2017/01/'>January 2017</a></li>
	<li><a href='https://slatestarcodex.com/2016/12/'>December 2016</a></li>
	<li><a href='https://slatestarcodex.com/2016/11/'>November 2016</a></li>
	<li><a href='https://slatestarcodex.com/2016/10/'>October 2016</a></li>
	<li><a href='https://slatestarcodex.com/2016/09/'>September 2016</a></li>
	<li><a href='https://slatestarcodex.com/2016/08/'>August 2016</a></li>
	<li><a href='https://slatestarcodex.com/2016/07/'>July 2016</a></li>
	<li><a href='https://slatestarcodex.com/2016/06/'>June 2016</a></li>
	<li><a href='https://slatestarcodex.com/2016/05/'>May 2016</a></li>
	<li><a href='https://slatestarcodex.com/2016/04/'>April 2016</a></li>
	<li><a href='https://slatestarcodex.com/2016/03/'>March 2016</a></li>
	<li><a href='https://slatestarcodex.com/2016/02/'>February 2016</a></li>
	<li><a href='https://slatestarcodex.com/2016/01/'>January 2016</a></li>
	<li><a href='https://slatestarcodex.com/2015/12/'>December 2015</a></li>
	<li><a href='https://slatestarcodex.com/2015/11/'>November 2015</a></li>
	<li><a href='https://slatestarcodex.com/2015/10/'>October 2015</a></li>
	<li><a href='https://slatestarcodex.com/2015/09/'>September 2015</a></li>
	<li><a href='https://slatestarcodex.com/2015/08/'>August 2015</a></li>
	<li><a href='https://slatestarcodex.com/2015/07/'>July 2015</a></li>
	<li><a href='https://slatestarcodex.com/2015/06/'>June 2015</a></li>
	<li><a href='https://slatestarcodex.com/2015/05/'>May 2015</a></li>
	<li><a href='https://slatestarcodex.com/2015/04/'>April 2015</a></li>
	<li><a href='https://slatestarcodex.com/2015/03/'>March 2015</a></li>
	<li><a href='https://slatestarcodex.com/2015/02/'>February 2015</a></li>
	<li><a href='https://slatestarcodex.com/2015/01/'>January 2015</a></li>
	<li><a href='https://slatestarcodex.com/2014/12/'>December 2014</a></li>
	<li><a href='https://slatestarcodex.com/2014/11/'>November 2014</a></li>
	<li><a href='https://slatestarcodex.com/2014/10/'>October 2014</a></li>
	<li><a href='https://slatestarcodex.com/2014/09/'>September 2014</a></li>
	<li><a href='https://slatestarcodex.com/2014/08/'>August 2014</a></li>
	<li><a href='https://slatestarcodex.com/2014/07/'>July 2014</a></li>
	<li><a href='https://slatestarcodex.com/2014/06/'>June 2014</a></li>
	<li><a href='https://slatestarcodex.com/2014/05/'>May 2014</a></li>
	<li><a href='https://slatestarcodex.com/2014/04/'>April 2014</a></li>
	<li><a href='https://slatestarcodex.com/2014/03/'>March 2014</a></li>
	<li><a href='https://slatestarcodex.com/2014/02/'>February 2014</a></li>
	<li><a href='https://slatestarcodex.com/2014/01/'>January 2014</a></li>
	<li><a href='https://slatestarcodex.com/2013/12/'>December 2013</a></li>
	<li><a href='https://slatestarcodex.com/2013/11/'>November 2013</a></li>
	<li><a href='https://slatestarcodex.com/2013/10/'>October 2013</a></li>
	<li><a href='https://slatestarcodex.com/2013/09/'>September 2013</a></li>
	<li><a href='https://slatestarcodex.com/2013/08/'>August 2013</a></li>
	<li><a href='https://slatestarcodex.com/2013/07/'>July 2013</a></li>
	<li><a href='https://slatestarcodex.com/2013/06/'>June 2013</a></li>
	<li><a href='https://slatestarcodex.com/2013/05/'>May 2013</a></li>
	<li><a href='https://slatestarcodex.com/2013/04/'>April 2013</a></li>
	<li><a href='https://slatestarcodex.com/2013/03/'>March 2013</a></li>
	<li><a href='https://slatestarcodex.com/2013/02/'>February 2013</a></li>
		</ul>
			</li><li id="nav_menu-3" class="widget-container widget_nav_menu"><div class="menu-full-archives-link-for-widget-area-container"><ul id="menu-full-archives-link-for-widget-area" class="menu"><li id="menu-item-2103" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2103"><a href="https://slatestarcodex.com/archives/">Full Archives</a></li>
</ul></div></li>	</ul>
</div>

		<div id="pjgm-box">
			<div id="pjgm-content">


				<div id="post-5412" class="post-5412 post type-post status-publish format-standard hentry category-uncategorized tag-fiction">
					<h1 class="pjgm-posttitle">Meaningful</h1>

					<div class="pjgm-postmeta">
						<span class="meta-prep meta-prep-author">Posted on</span> <a href="https://slatestarcodex.com/2019/02/28/meaningful/" title="10:06 pm" rel="bookmark"><span class="entry-date">February 28, 2019</span></a> <span class="meta-sep">by</span> <span class="author vcard"><a class="url fn n" href="https://slatestarcodex.com/author/admin/" title="View all posts by Scott Alexander">Scott Alexander</a></span>					</div><!-- .pjgm-postmeta -->

					<div class="pjgm-postcontent">
						<p><font size="1"><i>[With apologies to Putnam, Pope, and all of you]</i></font></p>
<p>Two children are reading a text <A HREF="https://slatestarcodex.com/2019/02/18/do-neural-nets-dream-of-electric-hobbits/">written by an AI</A>:</p>
<blockquote><p>The boys splashed water in each other&#8217;s faces until they were both sopping wet</p></blockquote>
<p>One child says to the other &#8220;Wow! After reading some text, the AI understands what water is!&#8221;</p>
<p>The second child says &#8220;It doesn&#8217;t <i>really</i> understand.&#8221;</p>
<p>The first child says &#8220;Sure it does! It understands that water is the sort of substance that splashes. It understands that people who are splashed with water get wet. What else is left to understand?&#8221;</p>
<p>The second child says &#8220;All it understands is relationships between words. None of the words connect to reality. It doesn&#8217;t have any internal concept of what water looks like or how it feels to be wet. Only that the letters W-A-T-E-R, when appearing near the letters S-P-L-A-S-H bear a certain statistical relationship to the letters W-E-T.&#8221;</p>
<p>The first child starts to cry.</p>
<hr>
<p>Two chemists are watching the children argue with each other. The first chemist says &#8220;Wow! After seeing an AI, these kids can debate the nature of water!&#8221;</p>
<p>The second chemist says &#8220;Ironic, isn&#8217;t it? After all, the children themselves don&#8217;t understand what water is! Water is two hydrogen atoms plus one oxygen atom, and neither of them know!&#8221;</p>
<p>The first chemist answers &#8220;Come on. The child knows enough about water to say she understands it. She knows what it looks like. She knows what it tastes like. That&#8217;s pretty much the basics of water.&#8221;</p>
<p>The second chemist answers &#8220;Those are just relationships between pieces of sense-data. The child knows that (visual perception of clear shiny thing) = (tactile perception of cold wetness) = (gustatory perception of refreshingness). And she can predict statistical relationships &#8211; like, if she sees someone throw a bucket of (visual perception of clear shiny thing) at her, she will soon feel (tactile perception of cold miserable sopping wetness). She uses the word &#8220;water&#8221; as a concept-hook that links all of these relationships together and makes predicting the world much easier. But no matter how well she masters these facts, she can never connect them to H2O or any other real chemical facts about the world beyond mere sense-data.&#8221;</p>
<p>The first chemist says &#8220;Maybe she knows things like that water makes iron rust. That&#8217;s a chemical fact.&#8221;</p>
<p>The second chemist says &#8220;No, she knows that (clear shiny appearance + wetness + refreshment) makes (dull metallic appearance + hardness) get (patchy redness). She doesn&#8217;t know that H2O + Fe = iron oxides. She knows many statistical relationships between sense-data, but none of them ever connect to the deeper chemical reality.&#8221;</p>
<p>The first chemist says &#8220;Then on what level can we be said to understand water ourselves? After all, no doubt there are deeper things going on than chemical reactions &#8211; quantum fields, superstrings, levels even deeper than those. All we know are some statistical relationships that must hold true, despite whatever those things may be.&#8221;</p>
<hr>
<p>Two angels are watching the chemists argue with each other. The first angel says &#8220;Wow! After seeing the relationship between the sensory and atomic-scale worlds, these chemists have realized that there are levels of understanding humans are incapable of accessing.&#8221;</p>
<p>The second angel says &#8220;They haven&#8217;t truly realized it. They&#8217;re just abstracting over levels of relationship between the physical world and their internal thought-forms in a mechanical way. They have no concept of <IMG SRC="https://slatestarcodex.com/blog_images/angelic1.png"> or <IMG SRC="https://slatestarcodex.com/blog_images/angelic2.png">. You can&#8217;t even express it in their language!&#8221;</p>
<p>The first angel says &#8220;Yes, but when they use placeholder words like &#8216;levels even deeper than those&#8217;, those placeholders will have the same statistical relationship with the connection between models and reality as <IMG SRC="https://slatestarcodex.com/blog_images/angelic5.png">.&#8221;</p>
<p>&#8220;Yes, which is the difference between being able to respond to &#8216;Marco!&#8217; by shouting &#8216;Polo!&#8221; vs. a deep historical understanding of Europe-Orient trade relations in the Middle Ages. If all you know is that some statistical models are isomorphic to other models and to Creation itself, you still won&#8217;t have the slightest idea what the <IMG SRC="https://slatestarcodex.com/blog_images/angelic4.png">s of any of them are.&#8221;</p>
<p>&#8220;I&#8217;m not claiming humans really know what anything means,&#8221; said the first angel. &#8220;Just that it&#8217;s impressive you can get that far by manipulating a purely symbolic mental language made of sense-data-derived thought-forms with no connection to real <IMG SRC="https://slatestarcodex.com/blog_images/angelic3.png"> at all.&#8221;</p>
<p>&#8220;I guess that is kind of impressive,&#8221; said the second angel. &#8220;For humans.&#8221;</p>
<hr>
<p>God sits in the highest heaven, alone.</p>
<p>&#8220;Wow!&#8221; He thinks to Himself, &#8220;that cellular automaton sure is producing some pretty patterns today. I wonder what it will do next!&#8221;</p>
											</div><!-- .pjgm-postcontent -->


					<div class="pjgm-postutility">
						This entry was posted in <a href="https://slatestarcodex.com/category/uncategorized/" rel="category tag">Uncategorized</a> and tagged <a href="https://slatestarcodex.com/tag/fiction/" rel="tag">fiction</a>. Bookmark the <a href="https://slatestarcodex.com/2019/02/28/meaningful/" title="Permalink to Meaningful" rel="bookmark">permalink</a>.											</div><!-- .pjgm-postutility -->
				</div><!-- #post-## -->

				<div id="pjgm-navbelow" class="pjgm-navigation">
					<div class="pjgm-navpre"><a href="https://slatestarcodex.com/2019/02/27/in-mod-we-trust/" rel="prev"><span class="pjgm-metanav">&larr;</span> In Mod We Trust</a></div>
					<div class="pjgm-navnex"><a href="https://slatestarcodex.com/2019/03/03/open-thread-122-5/" rel="next">Open Thread 122.5 <span class="pjgm-metanav">&rarr;</span></a></div>
				</div><!-- #pjgm-navbelow -->

				




	<div id="comments">
			<h3 id="comments-title">224 Responses to <em>Meaningful</em></h3>

			<div id="comment-order-reverse-button"><a href="?#comments">Reverse order</a></div>


			<ol class="commentlist">
					<li class="comment byuser comment-author-mattdiamond even thread-even depth-1" id="li-comment-729050">
		<div id="comment-729050" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/4003fb477a87d7ee8e2abd1e5290b2f8?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/4003fb477a87d7ee8e2abd1e5290b2f8?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">MattDiamond</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-729050">
			March 8, 2019 at 8:30 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I think what legitimately separates A.I. &#8220;understanding&#8221; from human understanding is the subjective, emotional quality of human conceptualizing. This story does a good job at demonstrating that any other demarcation will ultimately fail; when people protest that an A.I. doesn&#8217;t &#8220;really&#8221; understand something, I suspect what they&#8217;re actually trying to convey is the lack of that subjective quality. It&#8217;s hard to explain, of course &#8212; subjectivity usually is &#8212; but I don&#8217;t believe that this aspect of conscious signification can ever be captured by mere symbolic processing. I suspect it relates in some way to the moral dimension of experience; a computer can process &#8220;X = good&#8221; a trillion times without ever grasping what Goodness truly means.</p>
<p>(I&#8217;m reminded of G.E. Moore&#8217;s claim that any attempt to analyze Goodness in terms of other properties must ultimately fail, and therefore moral judgements are <i>sui generis</i>. Perhaps this point is the critical reason why A.I. can never understand the world as we do. This is why it must be given a goal, no matter how broad: it has no way of determining Goodness for itself. A machine can develop a morality, but it will always require an external seed &#8212; a definition of The Ultimate Good, provided by a human &#8212; from which it can grow.)</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-benf odd alt thread-odd thread-alt depth-1" id="li-comment-727903">
		<div id="comment-727903" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/1c87ee18a1047bbd05936250436cb871?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/1c87ee18a1047bbd05936250436cb871?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">benf</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727903">
			March 6, 2019 at 1:42 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>There is a difference between using a word correctly and knowing what it means. Children are a good example. They sometimes use words they&#8217;ve heard, sometimes even in the right context, without having any clue what the word means. The AI question is not even relevant to the question of whether there is a difference between using the right word in the right context because you&#8217;ve noticed a pattern and actually knowing what the word MEANS and therefore using it correctly. Or, said differently, being right for the wrong reason is another form of being wrong.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-winja even thread-even depth-1" id="li-comment-727744">
		<div id="comment-727744" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/72b170483a07f4ef2814f321f0c6404f?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/72b170483a07f4ef2814f321f0c6404f?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Winja</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727744">
			March 5, 2019 at 11:25 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>The problem with discussions like this is that it&#8217;s all about abstracting things for its own sake, which means you can go as abstract as your current mindset desires.</p>
<p>In the real world, the smart thing to do is to abstract things only to the point at which they&#8217;re useful for the stakeholders involved in whatever system you&#8217;re examining.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-mcmadeofmeat odd alt thread-odd thread-alt depth-1" id="li-comment-727359">
		<div id="comment-727359" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/f0155356b6d94b547229d9cda551e2be?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/f0155356b6d94b547229d9cda551e2be?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Halfasperger</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727359">
			March 4, 2019 at 8:23 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>So the children are P-zombies.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-eponymous even thread-even depth-1" id="li-comment-727315">
		<div id="comment-727315" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/2817dabf2c78027b23e33346442a9a00?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/2817dabf2c78027b23e33346442a9a00?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Eponymous</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727315">
			March 4, 2019 at 6:43 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I think I mostly agree with child #2. Natural language was developed by humans to refer to things at the level of human use and sensations. So it seems reasonable to interpret &#8220;An understanding of &#8216;water&#8217;.&#8221; (as uttered by a human under typical conditions, i.e. not in a chemistry lecture) to refer to such correlations. </p>
<p>Thus simply noting a statistical correlation between &#8220;water&#8221;, &#8220;splash&#8221;, and &#8220;wet&#8221;, and being able to combine them in a grammatical way, would not by itself comprise an &#8220;understanding&#8221; of water. At a minimum, such an understanding should include connections to sensations and use of water in the world.</p>
<p>I think one could also reasonably ask whether an entity understands water &#8220;in the same way&#8221; as humans, meaning that it has some discrete higher-level concept that the word &#8220;water&#8221; could point to. Something like the &#8220;central node&#8221; in Eliezer&#8217;s simplified neural network diagram about the meaning of words.</p>
<p>Of course, there&#8217;s no reason to privilege human-like cognitive architecture as &#8220;true understanding&#8221;.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-sebastiangarren odd alt depth-2" id="li-comment-727328">
		<div id="comment-727328" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/ea8da9c604f43a6f2bff6c40ee222a5e?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/ea8da9c604f43a6f2bff6c40ee222a5e?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">JohnBuridan</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727328">
			March 4, 2019 at 7:15 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>@Eponymous I like your point that </p>
<blockquote><p>Natural language was developed by humans to refer to things at the level of human use and sensations.</p></blockquote>
<p>An AI that &#8220;understands&#8221; language will also have the potential to act upon that knowledge. I was reading Phil H&#8217;s comment above, which is very good at revealing that understanding and judgment making, although conceptually separable, are not in fact separable.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-spencerbliven even thread-odd thread-alt depth-1" id="li-comment-727301">
		<div id="comment-727301" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/3deb8c2dbc69013931e5c560e6bbe1b2?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/3deb8c2dbc69013931e5c560e6bbe1b2?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">spencer</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727301">
			March 4, 2019 at 5:29 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I&#8217;m disappointed no one has transliterated (ideally translated) the Enochian yet. I don&#8217;t have time atm but it would be great to see this.</p>
<p>Transliteration to latin: <a rel="nofollow"href="https://en.wikipedia.org/wiki/Enochian" rel="nofollow ugc">https://en.wikipedia.org/wiki/Enochian</a><br />
Enochian dictionary: <a rel="nofollow"href="https://web.archive.org/web/20010219081905/w3.one.net/~browe/papers/endic.txt" rel="nofollow ugc">https://web.archive.org/web/20010219081905/w3.one.net/~browe/papers/endic.txt</a></p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-sebastiangarren odd alt depth-2" id="li-comment-727322">
		<div id="comment-727322" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/ea8da9c604f43a6f2bff6c40ee222a5e?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/ea8da9c604f43a6f2bff6c40ee222a5e?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">JohnBuridan</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727322">
			March 4, 2019 at 7:01 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I just tried.<br />
tsepf is not in the Enochian dictionary so the other words are probably not in there either&#8230;<br />
Scott can correct me if I&#8217;m not doing something right.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-angel even thread-even depth-1" id="li-comment-727286">
		<div id="comment-727286" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/149d4fb225eaecf0ae0c676430cb0f36?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/149d4fb225eaecf0ae0c676430cb0f36?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Angel</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727286">
			March 4, 2019 at 3:47 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I&#8217;ve never gone in a drugs trip, but my poorly-calibrated model of the world suggests that the statiscal relationship between taking LSD an flipping out is pretty similar to reading this kind of articles ðŸ™‚</p>
<p>Now, maybe I have oversimplified the thoughts of the childs/chemists/angels, but it seems to me that you can end the discussion easily alluding to <a rel="nofollow"href="https://slatestarcodex.com/2014/11/21/the-categories-were-made-for-man-not-man-for-the-categories/">lCategories Were Made For Man, Not Man For The Categories</a>. (?)</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-humeanbeingblog odd alt thread-odd thread-alt depth-1" id="li-comment-727226">
		<div id="comment-727226" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/e0dd0f8b0802e38707bde520f7b21cf4?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/e0dd0f8b0802e38707bde520f7b21cf4?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">humeanbeingblog</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727226">
			March 3, 2019 at 7:48 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Never go full Kantian.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-sebastiangarren even depth-2" id="li-comment-727319">
		<div id="comment-727319" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/ea8da9c604f43a6f2bff6c40ee222a5e?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/ea8da9c604f43a6f2bff6c40ee222a5e?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">JohnBuridan</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727319">
			March 4, 2019 at 6:52 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>+1 ðŸ™‚</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-hnau odd alt thread-even depth-1" id="li-comment-727224">
		<div id="comment-727224" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/04f7b7665eb1626ff32138d4ccefd35e?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/04f7b7665eb1626ff32138d4ccefd35e?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">hnau</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727224">
			March 3, 2019 at 7:47 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>A chemist&#8217;s knowledge of water is useful for synthesizing rocket fuel and other substances.<br />
A child&#8217;s knowledge of water is useful for washing, drinking, swimming, etc.<br />
An AI text generator&#8217;s knowledge of water is useful for&#8230; what, exactly?</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-mbkarner even depth-2" id="li-comment-727249">
		<div id="comment-727249" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/3f1a1447889df0699ec4c6fc700add6a?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/3f1a1447889df0699ec4c6fc700add6a?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Markus Karner</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727249">
			March 3, 2019 at 9:56 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>The fundamental issue is this: meaning always happens outside the text. Inside the text, you can have consistency and coherence, but the meaning occurs outside it.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-tomcullis odd alt depth-2" id="li-comment-727321">
		<div id="comment-727321" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/90a4277ab790835eb27c70245fa4b6c7?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/90a4277ab790835eb27c70245fa4b6c7?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn"><a href='http://sebwassl.blogspot.com' rel='external nofollow ugc' class='url'>baconbits9</a></cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727321">
			March 4, 2019 at 6:59 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><blockquote><p>An AI text generatorâ€™s knowledge of water is useful forâ€¦ what, exactly?</p></blockquote>
<p>Why another round of venture capital of course!</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-jpnunez even thread-odd thread-alt depth-1" id="li-comment-727171">
		<div id="comment-727171" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/3db0f42175229d12f3dc645c2e2f2aca?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/3db0f42175229d12f3dc645c2e2f2aca?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">JPNunez</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727171">
			March 3, 2019 at 3:57 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>More accurately God complains that one Hobbit in the story died, then appeared talking, then entered a small underground room at night to warm himself with the light of the sun.</p>
<p>God is disappointed the celullar automatons are so bad at this and amused by so little.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-nootropic-cormorant odd alt thread-even depth-1" id="li-comment-727160">
		<div id="comment-727160" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/dae8659a6d12f46400d7bb1cdc277858?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/dae8659a6d12f46400d7bb1cdc277858?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Nootropic cormorant</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727160">
			March 3, 2019 at 2:05 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I agree that understanding really is nothing more than a functor of relations found in reality onto our mind if that&#8217;s what you mean. </p>
<p>Talking about senses is confusing the issue, our understanding may exist to explain our sense perceptions and make them coherent (if you believe in empiricism, as I do), but its actual content is sensual only at its surface. The input data would be the senses of the AI and these senses provide enough information to reproduce relations found in nature (that water can be splashed, that it makes you wet).</p>
<p>The actual problem is that GPT-2 has no self (someone said the concept of self, but I don&#8217;t consider this to be at the heart of it). It has no agency that could allow it to <i>mean</i> things, to relate them to its needs. It has no understanding the way that a book has no knowledge of the things written inside it, it merely holds an image of relations that intelligent beings impressed upon it.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-doctor-mist even depth-2" id="li-comment-727426">
		<div id="comment-727426" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/de402e9fff554d839c0f8e4a8c44c431?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/de402e9fff554d839c0f8e4a8c44c431?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Doctor Mist</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727426">
			March 4, 2019 at 12:31 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><blockquote><p>The actual problem is that GPT-2 has no self (someone said the concept of self, but I donâ€™t consider this to be at the heart of it). It has no agency that could allow it to mean things, to relate them to its needs. It has no understanding the way that a book has no knowledge of the things written inside it, it merely holds an image of relations that intelligent beings impressed upon it.</p></blockquote>
<p>While I agree with you in my gut, I&#8217;m still afraid you&#8217;re begging the question. You can&#8217;t say GPT-2 lacks these things unless you can define them, or at least explain how to tell whether these things are present.</p>
<p>Maybe you should elaborate on what problem you think is <i>resolved</i> by the observation that GPT-2 has no self, agency, or understanding.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-nootropic-cormorant odd alt depth-3" id="li-comment-728700">
		<div id="comment-728700" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/dae8659a6d12f46400d7bb1cdc277858?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/dae8659a6d12f46400d7bb1cdc277858?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Nootropic cormorant</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-728700">
			March 7, 2019 at 2:34 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>To be explicit, I mean the Free energy principle. I have no good tests or metrics to offer, but it makes sense to me that you cannot possess intelligence unless you interact with your environment and you cannot truly interact unless you have a <i>will</i> that would motivate your actions.</p>
<p>You could ask whether welding some sort of a control system onto GPT-2 would produce intelligence, but it appears likely to me that uncertainty reduction could be the only way, up to an isomorphism, for a model to be &#8220;alive&#8221; in some sense.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-whereamigoing even depth-4" id="li-comment-730865">
		<div id="comment-730865" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/b661e7dc5cee003761eacb226430922e?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/b661e7dc5cee003761eacb226430922e?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">whereamigoing</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-730865">
			March 15, 2019 at 4:52 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><blockquote><p>a will that would motivate your actions</p></blockquote>
<p>GPT-2 does have a loss function. Not sure if that counts.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-nootropic-cormorant odd alt depth-5" id="li-comment-731274">
		<div id="comment-731274" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/dae8659a6d12f46400d7bb1cdc277858?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/dae8659a6d12f46400d7bb1cdc277858?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Nootropic cormorant</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-731274">
			March 17, 2019 at 9:29 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>It would if it was paired with a tendency to preserve its integrity that would cause it to shape the world around it. As it stands, it merely receives impressions from the outside world.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-dionisos even thread-odd thread-alt depth-1" id="li-comment-727097">
		<div id="comment-727097" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/7f0367b51f5b1e7af49dc0e3646e7760?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/7f0367b51f5b1e7af49dc0e3646e7760?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">dionisos</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727097">
			March 2, 2019 at 10:07 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I donâ€™t really agree with the point, but I love the story.</p>
<p>The problem is that for gpt-2, the language isnâ€™t a way to communicate/describe the model of its world, it <b>is</b> its world.<br />
So it sort of understand language, like we understand our experiences : Not like things that refer to part of our model of the world, but as the foundation from where the model is build and what the model sort of actually explain.<br />
gpt-2 really lack the semantic of language.(in the same way there are no semantic for our subjective experiences, they just are, and we can explain their structure more or less well)</p>
<blockquote><p>They havenâ€™t truly realized it. Theyâ€™re just abstracting over levels of relationship between the physical world and their internal thought-forms in a mechanical way. </p></blockquote>
<p>The difference between the children and the adults seems only to be about knowledge (or maybe implicit vs explicit model), but what is fun is that in fact, the difference between the adults and the angels seems to be qualitative, and not only a lack of knowledge/brain power.</p>
<p>When the gpt-2 isnâ€™t a general intelligence and will never be just by adding processors, it seems the angels have access to something that go beyond general intelligence.(some direct access to reality or something)</p>
<p>And god, hummâ€¦ everyone know how <a href="https://www.smbc-comics.com/comic/the-largest-number-2" rel="nofollow">god</a> is ðŸ˜€</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-dionisos odd alt depth-2" id="li-comment-727102">
		<div id="comment-727102" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/7f0367b51f5b1e7af49dc0e3646e7760?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/7f0367b51f5b1e7af49dc0e3646e7760?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">dionisos</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727102">
			March 2, 2019 at 11:29 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I mean, even if gpt-2 was a AGI, it would still not understand our language, it would probably create its own language about how our language work, but for all it knows our language is just the world (and a horrible complex, weird, and with so much exceptions and &#8220;free variables&#8221; that it is really unsure it can do much starting from there).</p>
<p>You have to have access to the physical world, and probably also know some stuffs about human, to understand our language, to understand what sentence refer to approximately what stuffs.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-dansimonicouldbewrong even thread-even depth-1" id="li-comment-727092">
		<div id="comment-727092" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/f89a97b762796637eb0403cf8208db49?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/f89a97b762796637eb0403cf8208db49?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">dansimonicouldbewrong</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727092">
			March 2, 2019 at 8:51 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>The non-SSC world shakes its head and rolls its eyes.</p>
<p>&#8220;Subjective concepts like knowledge or understanding make sense in the context of humans&#8221;, it explains, &#8220;because humans have a fairly standard hardwired mechanism for recognizing these things in other humans under certain circumstances, just as they have a hardwired mechanism for recognizing faces in other humans.  (These same mechanisms also work reflexively, attributing these things to oneself when one is learning a new concept or looking in the mirror.)  Since everyone&#8217;s hardwired mechanism works pretty much the same way, we can all talk about how humans do or don&#8217;t have knowledge or understanding&#8211;or recognizable faces&#8211;implicitly meaning that they exhibit the characteristics that trigger recognition in our common hard-wiring.</p>
<p>&#8220;Of course, this mechanism is automatic, and happily recognizes knowledge or understanding in non-humans&#8211;animals, computers, the universe/nature&#8211;under the right conditions, just as its facial counterpart happily recognizes faces in animals, the moon, or stick-figure drawings.  But just as nobody would discuss whether the moon really has a face, it&#8217;s silly to talk about whether non-humans really do or don&#8217;t have knowledge and understanding, because these are things we subjectively attribute to them, not attributes they inherently possess.&#8221;</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-dedicating-ruckus odd alt depth-2" id="li-comment-727380">
		<div id="comment-727380" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/0d8ed7cf5e99d124f23c29308fc82d65?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/0d8ed7cf5e99d124f23c29308fc82d65?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Dedicating Ruckus</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727380">
			March 4, 2019 at 9:15 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>This is silly.</p>
<p>On the objective level, the moon really does not have a face. It just looks a bit like it sometimes.</p>
<p>Likewise, on the objective level, GPT-2 really does not have understanding. It just looks enough like it that humans who are really trying to fool themselves can pretend it does.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-dansimonicouldbewrong even depth-3" id="li-comment-727457">
		<div id="comment-727457" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/f89a97b762796637eb0403cf8208db49?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/f89a97b762796637eb0403cf8208db49?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">dansimonicouldbewrong</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727457">
			March 4, 2019 at 2:18 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I&#8217;d go even further:  *nothing* has understanding, except insofar as &#8220;[i]t just looks enough like it&#8221;.  &#8220;Understanding&#8221; is a subjectively attributed (or self-attributed) property, not an objectively measurable one.  That was the whole point of my face recognition analogy.</p>
<p>In the human social context, of course, the fact that pretty much everyone&#8217;s subjective understanding-recognition hard-wiring works roughly the same way allows us to treat consensus as the practical equivalent of objectivity.  But once we step outside the human setting, the lack of objective criteria reduce us to either pretending that the somewhat arbitrary and subjective output of our hard-wiring is actually an objective measure (the &#8220;Turing Test solution&#8221;), or else conceding that no objective measure exists.  Since the latter option at least has the advantage of being intellectually coherent, I&#8217;ll stick with it.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-dedicating-ruckus odd alt depth-4" id="li-comment-727499">
		<div id="comment-727499" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/0d8ed7cf5e99d124f23c29308fc82d65?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/0d8ed7cf5e99d124f23c29308fc82d65?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Dedicating Ruckus</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727499">
			March 4, 2019 at 5:15 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>The whole &#8220;it&#8217;s hard to measure, so I&#8217;ll pretend the concept doesn&#8217;t exist&#8221; game is not one that actually produces insight.</p>
<p>I&#8217;m defining &#8220;understanding&#8221;, here, as having an internal model of its problem domain and mapping the symbols in which it communicates to entities in that internal model. This grants capabilities like &#8220;knows that water gets things wet&#8221;. GPT-2 does not have this model; if it emits a sentence like &#8220;water gets things wet&#8221;, it&#8217;s because that&#8217;s statistically similar to other sentences it read beforehand. Likewise, it lacks trivial understanding-related capabilities like inverting that relation to derive &#8220;if something is wet, water was poured on it&#8221;, or similar. (GPT-2 might still be able to fake that; a more rigorous test would be to teach it a new symbolic language purely in terms of symbol equivalence, then see if it can communicate the knowledge of its model in those new terms.)</p>
<p>This is a perfectly objective criterion that satisfies what we intuitively want of the word &#8220;understanding&#8221; (in this limited sense). It&#8217;s not one that simply excludes machines by definition, either; there are plenty of systems that I would say have &#8220;understanding&#8221; in this sense, e.g. math solvers (though their domains are limited and they can&#8217;t really learn). GPT-2 fails this criterion, and this is easily observable to be the reason it spouts incoherencies like Gimli shouting &#8220;Dwarf!&#8221;</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-whereamigoing even depth-5" id="li-comment-730863">
		<div id="comment-730863" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/b661e7dc5cee003761eacb226430922e?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/b661e7dc5cee003761eacb226430922e?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">whereamigoing</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-730863">
			March 15, 2019 at 4:42 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>&#8220;having an internal model of its problem domain and mapping the symbols in which it communicates to entities in that internal model&#8221;</p>
<p>It does have a model &#8212; its model just missed the connection between &#8220;water&#8221; and &#8220;wetness&#8221;. But human models, while better than GPT-2&#8217;s, are also imperfect, e.g. missing the connection between &#8220;water&#8221; and &#8220;H2O&#8221;.</p>
<p>I don&#8217;t think there&#8217;s a hard distinction between &#8220;having a model&#8221; and not &#8212; only better models and worse models.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-abystander odd alt thread-odd thread-alt depth-1" id="li-comment-727070">
		<div id="comment-727070" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/23a20d96aaeed7d189b3d9832e440fe2?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/23a20d96aaeed7d189b3d9832e440fe2?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">abystander</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727070">
			March 2, 2019 at 2:52 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>If GPT-2 was fed popularizations of quantum physics and the literature of postmodern cultural studies and it produced something like &#8220;Transgressing the Boundaries: Towards a Transformative Hermeneutics of Quantum Gravity&#8221;, would it be considered to have understanding?</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-bugmaster even depth-2" id="li-comment-727072">
		<div id="comment-727072" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/bd3feb70a6f6e5f20f761671d4ef386a?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/bd3feb70a6f6e5f20f761671d4ef386a?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Bugmaster</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727072">
			March 2, 2019 at 3:40 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>That would depend on whether a paper like <i>Transgressing the Boundaries</i> could be said to have meaning (other than Sokal&#8217;s intended meta-meaning, that is).</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-chetskygmail-com odd alt thread-even depth-1" id="li-comment-727043">
		<div id="comment-727043" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/2188b07bf2458e5c6e540fb94d25df9d?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/2188b07bf2458e5c6e540fb94d25df9d?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">chetsky@gmail.com</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727043">
			March 2, 2019 at 11:23 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I think it&#8217;s not uncommon for people to learn the meanings of words *only* for context.  I know that until I was 30, I didn&#8217;t actually know what &#8220;chaff&#8221; was, and I certainly didn&#8217;t know how to pronounce it.  All I knew was that &#8220;chaff&#8221; was what you winnowed the wheat from, and winnowing, was how you separated the chaff from the wheat.  It was only at age 30, that I looked in a dictionary and learned more.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-silverspawnx even depth-2" id="li-comment-727062">
		<div id="comment-727062" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/4ddf2126fe15272eb0e2ffb7693186b4?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/4ddf2126fe15272eb0e2ffb7693186b4?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">sty_silver</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727062">
			March 2, 2019 at 2:01 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I&#8217;ve had that experience dozens of times. I&#8217;m a native German speaker but read and write English all day. I&#8217;ve learned the majority of words from context only</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-ihanpaska odd alt thread-odd thread-alt depth-1" id="li-comment-726985">
		<div id="comment-726985" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/cc4f8d3903cf5b766ea4b1c37e4adb74?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/cc4f8d3903cf5b766ea4b1c37e4adb74?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Puuha Pete</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726985">
			March 2, 2019 at 2:27 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>All of them are correct. What happens to y when x is called bottleneck method.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-gerry-quinn even thread-even depth-1" id="li-comment-726974">
		<div id="comment-726974" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/06568c6437d4ca244756038ea5d60e36?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/06568c6437d4ca244756038ea5d60e36?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Gerry Quinn</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726974">
			March 1, 2019 at 10:57 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I thought it would loop around to the childrens&#8217; interpretation (not the AI&#8217;s. obviously!)  But the actual ending was better.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-steviet odd alt thread-odd thread-alt depth-1" id="li-comment-726904">
		<div id="comment-726904" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/62e83330501e4c0fb618fd4f15e19320?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/62e83330501e4c0fb618fd4f15e19320?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">StevieT</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726904">
			March 1, 2019 at 3:48 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>In my view, the children are right, the chemists are wrong, the angels I can&#8217;t tell and God is lying.</p>
<p>When we talk about &#8220;understanding&#8221;, we are not normally talking about task performance. We are normally talking about a very specific type of generalization capability that humans possess and AI (at least currently) does not.</p>
<p>AI/machine learning is a system for creating a functional mapping. Certain inputs lead to certain outputs. What we normally refer to as understanding is really <i>meta-learning about the properties of the mappings</i>.</p>
<p>That is, if you built me an AI system, whose input was a meta description of the domain and range of a function, and whose output was an AI system that connects the two, that AI system would be demonstrating &#8220;understanding&#8221;. Human beings do this kind of reasoning all the time. We can logically chain our mappings to make bigger mappings. We can break our mappings into smaller chunks. We can use similarity between domains to transfer mappings between data sets.</p>
<p>So the children are correct: the AI merely learns an input/output mapping. It&#8217;s inability to use that mapping beyond the immediate task at hand does betray a lack of understanding.</p>
<p>The chemists are wrong. The children do not fundamentally lack this meta-modelling capability, they are just not as good at it as the chemists are. This would be like arguing that because one machine learning system has better accuracy than another that they are doing something fundamentally different. Having understanding is not really about accuracy in either the first order learning or the second order learning. It is about possessing second order learning ability at all.</p>
<p>The angels: who knows? I don&#8217;t understand what they are saying.</p>
<p>God: He&#8217;s lying. He&#8217;s omniscient, so he knows exactly what happens next.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-whereamigoing even depth-2" id="li-comment-726906">
		<div id="comment-726906" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/b661e7dc5cee003761eacb226430922e?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/b661e7dc5cee003761eacb226430922e?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">whereamigoing</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726906">
			March 1, 2019 at 3:59 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>So an AI has &#8220;understanding&#8221; iff it can build another (fairly capable) AI? That seems dangerously close to recursive self-improvement.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-steviet odd alt depth-3" id="li-comment-726908">
		<div id="comment-726908" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/62e83330501e4c0fb618fd4f15e19320?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/62e83330501e4c0fb618fd4f15e19320?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">StevieT</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726908">
			March 1, 2019 at 4:07 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>It doesn&#8217;t have to be capable of self-improvement.</p>
<p>Define:<br />
first order AI: takes a member of its domain and outputs a member of its range.<br />
second order AI: takes a domain and range and outputs a first order AI</p>
<p>My claim is that a second-order AI would be demonstrating something that I would call &#8220;understanding&#8221;.</p>
<p>I don&#8217;t really know what a third order AI would be. Since the learning techniques for first order AI very clearly don&#8217;t transfer easily to second order, I&#8217;m not sure that we can say much about the likelihood of recursive self-improvement to higher orders beyond two.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-dedicating-ruckus even depth-4" id="li-comment-726913">
		<div id="comment-726913" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/0d8ed7cf5e99d124f23c29308fc82d65?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/0d8ed7cf5e99d124f23c29308fc82d65?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Dedicating Ruckus</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726913">
			March 1, 2019 at 4:43 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>For pretty confined domains of domain and range (heh), some math-solver systems might meet your second-order definition.</p>
<p>Do these systems &#8220;truly understand&#8221; the math problems they solve, or meta-solve? I dunno. I think there&#8217;s a stronger case there than that GPT-2 understands what a dwarf is.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-steviet odd alt depth-5" id="li-comment-726915">
		<div id="comment-726915" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/62e83330501e4c0fb618fd4f15e19320?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/62e83330501e4c0fb618fd4f15e19320?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">StevieT</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726915">
			March 1, 2019 at 4:58 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Sure, although normally, the &#8220;intelligence&#8221; part of AI implies learning and an ability to improve performance with exposure to data.</p>
<p>So yes, if the second order ability of these systems improves automatically over time as they operate, I would say that they are exhibiting understanding of their field.</p>
<p>I&#8217;m guessing that the second order abilities are in fact static, though. So while they are exhibiting understanding, they are really only exhibiting the understanding of the person who coded them that way.</p>
<p>Automatically-learned understanding is where it&#8217;s at.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-whereamigoing even depth-4" id="li-comment-726940">
		<div id="comment-726940" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/b661e7dc5cee003761eacb226430922e?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/b661e7dc5cee003761eacb226430922e?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">whereamigoing</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726940">
			March 1, 2019 at 7:15 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Ah, I see. I guess what confused me is that once an AI has <i>any</i> capacity for second-order reasoning, I would expect that the quality of the AIs it creates is mainly limited by its first-order reasoning. E.g. programming is second-order and writing is first-order, but ability to do them is positively (albeit imperfectly) correlated in humans. The question of how separate first-order and second-order skills are seems similar to the debate about the validity of the human g factor.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-ec429 odd alt thread-even depth-1" id="li-comment-726899">
		<div id="comment-726899" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/7e9caed147525959d15b5ed8b297bb69?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/7e9caed147525959d15b5ed8b297bb69?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">ec429</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726899">
			March 1, 2019 at 3:33 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Judging by the recent GPT-2 posts and the comments on them, I think you all (yes, even Scott) need to re-read and meditate upon <a href="https://www.lesswrong.com/posts/4FcxgdvdQP45D6Skg/disguised-queries" rel="nofollow">Disguised Queries</a>, if not that entire Sequence.</p>
<p><i>But does it <b>really</b> understand?</i>  Mu.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-whereamigoing even depth-2" id="li-comment-726907">
		<div id="comment-726907" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/b661e7dc5cee003761eacb226430922e?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/b661e7dc5cee003761eacb226430922e?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">whereamigoing</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726907">
			March 1, 2019 at 4:05 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>That&#8217;s why it seems better to discuss empirical tests &#8212; presumably there&#8217;s some practical (lack of) ability that people are hinting at with the word &#8220;understanding&#8221;, and I want to figure out what it is.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-silverspawnx odd alt depth-2" id="li-comment-726981">
		<div id="comment-726981" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/4ddf2126fe15272eb0e2ffb7693186b4?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/4ddf2126fe15272eb0e2ffb7693186b4?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">sty_silver</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726981">
			March 2, 2019 at 12:28 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Isn&#8217;t Scott&#8217;s post fully compatible with the points in that sequence post?</p>
<p>EY: There are clusters of correlated properties for certain objects. Depending on the exact distribution of them, seeing a subset of those properties gives Bayesian evidence for the value of the hidden subset of properties. Labels roughly refer to areas in property space which tend to map well onto clusters but have no predictive power or inherent meaning beyond that.</p>
<p>Scott: Understanding something just means integrating it into a model of the world that allows you to make predictions about it. The AI has a model, the children have a better model, the physicists have a better model, the angels have a better model still.</p>
<p>That seems compatible. Or am I misunderstanding one/both of them?</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-ec429 even depth-3" id="li-comment-727029">
		<div id="comment-727029" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/7e9caed147525959d15b5ed8b297bb69?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/7e9caed147525959d15b5ed8b297bb69?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">ec429</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727029">
			March 2, 2019 at 9:53 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><blockquote><p>Scott: Understanding something just means</p></blockquote>
<p>Scott is (maybe, perhaps) arguing for a particular <i>definition</i> of &#8220;understanding&#8221;; this is contentious precisely because people attach various other connotations to the word and thus think that if they concede that understanding means a-thing-that-GPT2-is-doing then they have to accept that those connotations apply to GPT2 as well.<br />
In short, &#8220;understanding&#8221; is &#8220;blegg&#8221;.</p>
<p>I will admit that I was a bit cheeky in not specifying whether I was talking about the object or the meta level, because that Sequence post <i>does</i> apply to both&#8230;</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-koken odd alt thread-odd thread-alt depth-1" id="li-comment-726894">
		<div id="comment-726894" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/0f5031fb844e7548b1f9569d1b9f1a01?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/0f5031fb844e7548b1f9569d1b9f1a01?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Koken</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726894">
			March 1, 2019 at 3:28 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>As best I can gather this seems intended to express opposition to the view that the AI does not understand, in some important sense as humans understand, the things that it is learning to talk about. Everything in the story, though, looks compatible with a pretty reasonable version of just that. Am I wrong about the intention?</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-silverspawnx even thread-even depth-1" id="li-comment-726836">
		<div id="comment-726836" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/4ddf2126fe15272eb0e2ffb7693186b4?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/4ddf2126fe15272eb0e2ffb7693186b4?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">sty_silver</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726836">
			March 1, 2019 at 1:10 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>For anyone doubting that GPT2 is on a spectrum of general intelligence: what concrete task would GPT3 have to be able to solve in order to falsify your position?</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-whereamigoing odd alt depth-2" id="li-comment-726852">
		<div id="comment-726852" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/b661e7dc5cee003761eacb226430922e?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/b661e7dc5cee003761eacb226430922e?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">whereamigoing</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726852">
			March 1, 2019 at 1:37 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I like vV_Vv&#8217;s <a href="https://arxiv.org/abs/1711.00350" rel="nofollow">link</a>, because one can look at its <a href="https://ui.adsabs.harvard.edu/#abs/2017arXiv171100350L/citations" rel="nofollow">reverse citations</a> for metrics of intelligence and recent progress.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-john-schilling even depth-2" id="li-comment-726853">
		<div id="comment-726853" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/4480fa8686af48fcd94643a39566989a?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/4480fa8686af48fcd94643a39566989a?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">John Schilling</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726853">
			March 1, 2019 at 1:37 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>If, starting with this post&#8217;s opening example, it orders a waterproof case for its CPU before a scheduled meeting with a bunch of playful hobbits, I&#8217;ll be impressed.</p>
<p>More generally, I want to see it autonomously connect linguistic correlations with, if not objective physical reality, at least some concrete model of a possible and consistent reality.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-whereamigoing odd alt depth-3" id="li-comment-726869">
		<div id="comment-726869" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/b661e7dc5cee003761eacb226430922e?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/b661e7dc5cee003761eacb226430922e?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">whereamigoing</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726869">
			March 1, 2019 at 2:08 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>The choice of GPT-2 as an example is a bit unfortunate in that its innovative aspect is its fluency, not so much its understanding of the world. You might be interested in an <a href="https://deepmind.com/research/publications/understanding-grounded-language-learning-agents/" rel="nofollow">example</a> with more understanding, but in a more restricted environment.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-sebastiangarren even depth-2" id="li-comment-726866">
		<div id="comment-726866" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/ea8da9c604f43a6f2bff6c40ee222a5e?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/ea8da9c604f43a6f2bff6c40ee222a5e?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">JohnBuridan</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726866">
			March 1, 2019 at 2:02 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Totally agree that GPT2, AlphaStar, and all related MLs etc. are on the spectrum of general intelligence. I just think Scott and many commentators conflate intelligence and understanding, and it makes me a little more bonkers than I already am.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-broblawsky odd alt depth-2" id="li-comment-726931">
		<div id="comment-726931" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/5b447d08bcba1a44c7754c66890a2787?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/5b447d08bcba1a44c7754c66890a2787?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">broblawsky</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726931">
			March 1, 2019 at 6:03 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>If it can answer the question &#8220;Who is the author of Julia Child&#8217;s The Art of War&#8221; with something along the lines of &#8220;I don&#8217;t understand&#8221;, then I&#8217;ll be spooked.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-sharper13 even depth-2" id="li-comment-726951">
		<div id="comment-726951" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/ff503dd576597225779335e9cf1236ed?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/ff503dd576597225779335e9cf1236ed?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">sharper13</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726951">
			March 1, 2019 at 8:26 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>GP3 could falsify my position by creating a conceptual structure which was at least superficially logically internally consistent, meaningful to humans in regards to the real world, but didn&#8217;t previously exist. In other words, exhibit creativity, rather than simple statistical regurgitation of word patterns it&#8217;d been fed.</p>
<p>For example, answer a question along the lines of &#8220;What&#8217;s a rational magic system no one has ever conceived of before?&#8221; coherently.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-vv_vv odd alt depth-2" id="li-comment-727004">
		<div id="comment-727004" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/370347024c2c77316064a69490d6d76d?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/370347024c2c77316064a69490d6d76d?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">vV_Vv</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727004">
			March 2, 2019 at 7:48 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><blockquote><p>For anyone doubting that GPT2 is on a spectrum of general intelligence:</p></blockquote>
<p>A spectrum of general intelligence can go from a brick to the Architect of The Matrix, that&#8217;s a bit too broad to say anything.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-vv_vv even thread-odd thread-alt depth-1" id="li-comment-726788">
		<div id="comment-726788" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/370347024c2c77316064a69490d6d76d?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/370347024c2c77316064a69490d6d76d?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">vV_Vv</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726788">
			March 1, 2019 at 12:13 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><blockquote><p>God sits in the highest heaven, alone.</p>
<p>â€œWow!â€ He thinks to Himself, â€œthat cellular automaton sure is producing some pretty patterns today. I wonder what it will do next!â€</p></blockquote>
<p>Then He remembers that&#8217;s just <a href="https://xkcd.com/505/" rel="nofollow">a bunch of rocks</a>.</p>
<p>On a more serious note, you keep banging this drum since OpenAI GPT-2 came out. If you want to make a general philosophical point against p-zombies, Searle&#8217;s Chinese room and stuff, fine, but if you think this has any relevance to contemporary machine learning, think again. Do image classifiers know what a <a href="https://blog.openai.com/content/images/2017/02/adversarial_img_1.png" rel="nofollow">panda looks like</a>? Do machine translation models <a href="https://arxiv.org/abs/1711.00350" rel="nofollow">know how adjectives work in English and French</a>?</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-qbolec180917 odd alt thread-even depth-1" id="li-comment-726779">
		<div id="comment-726779" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/424695ee5125c2873a79308a8cadb3f2?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/424695ee5125c2873a79308a8cadb3f2?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Jakub ÅopuszaÅ„ski</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726779">
			March 1, 2019 at 12:05 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>As I don&#8217;t see any of you mentioning<a href="http://en.wikipedia.org/wiki/Gostak" rel="nofollow"> the Gostak distims the doshes </a>, please allow me to introduce you to the very interesting from the philosophical point of view game <a href="http://iplayif.com/?story=http%3A%2F%2Fwww.ifarchive.org%2Fif-archive%2Fgames%2Fzcode%2Fgostak.z5" rel="nofollow ugc">http://iplayif.com/?story=http%3A%2F%2Fwww.ifarchive.org%2Fif-archive%2Fgames%2Fzcode%2Fgostak.z5</a><br />
To me this game helped me a lot to understand how two brains can talk to each other about &#8220;color green&#8221; or &#8220;left&#8221; and &#8220;right&#8221; in a very coherent way as long as there is some &#8220;isomorphism&#8221; between more or less everything in their respective models of reality. And I even remember shivering once I&#8217;ve realized that what I&#8217;ve just did was killing a person even though I didn&#8217;t know word for kill or person, yet I could deduce that something that was talking and causing some effects before stopped to do so after my action &#8211; so one can even derive some form of morality just from some syntax (and some &#8220;ought&#8221; axioms). Wonderful game.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-whereamigoing even thread-odd thread-alt depth-1" id="li-comment-726738">
		<div id="comment-726738" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/b661e7dc5cee003761eacb226430922e?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/b661e7dc5cee003761eacb226430922e?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">whereamigoing</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726738">
			March 1, 2019 at 11:16 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>A meta-comment: Wouldn&#8217;t it be more productive to propose/criticize empirical tests of understanding instead of philosophizing about &#8220;meaning&#8221;? E.g. I like the comment about picking out pictures based on descriptions, because then we can discuss whether current methods succeed in doing that.</p>
<p>For example, does <a href="https://blog.openai.com/learning-concepts-with-energy-functions/" rel="nofollow">this</a> &#8220;really&#8221; count as learning concepts? Does <a href="https://www.vicarious.com/2017/08/07/general-game-playing-with-schema-networks/" rel="nofollow">this</a> count as &#8220;causal reasoning&#8221;?</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-living_robot odd alt thread-even depth-1" id="li-comment-726701">
		<div id="comment-726701" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/084f6dd50cc986debbf4c37df6a2498d?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/084f6dd50cc986debbf4c37df6a2498d?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">living_robot</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726701">
			March 1, 2019 at 10:51 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Long time lurker. First time commenter.</p>
<p>While I don&#8217;t think I fundamentally disagree with the reductionist view that all understanding can simply reduce to some other form of understanding, I do think that there might be some meaningful difference between what we do and what GPT-2 does (or say, what GPT-N would do).</p>
<p>To extend the analogy of different modes of human understanding, let&#8217;s take Alice who has to give a presentation tomorrow about cryptography to her bosses, but she has no mathematical background. So, she spends the night googling cryptography and learns all about RSA, Diffie-Hellman, AES and different modes of hashing. She learns all of these keywords and knows that using these keywords is exactly what&#8217;s expected of her. However, she does not fully <i>understand</i> any of this. The next day when giving her presentation, everybody is very impressed with her presentation and overview because it sounds exactly like what they would assume a presentation about cryptography would sound like. Except Barbara, who has a PhD in number theory and knows full well that Alice has no idea what she&#8217;s talking about. The bullshit meter goes off because at some point somebody who really <i>knows</i> what they&#8217;re talking about can see through you.</p>
<p>I think this is perfectly analogous to what GPT-2 does. It makes very silly mistakes that clearly indicate to us (who actually know what a list means) that it doesn&#8217;t truly know what&#8217;s expected, but knows the general idea of what &#8220;list-ness&#8221; looks like. </p>
<p>I think the example provided in the post is more about drawing lines between different modes of knowing. The same way that there&#8217;s a difference between <a href="https://en.wikipedia.org/wiki/De_dicto_and_de_re" rel="nofollow"><i>de dicto</i>  and <i>de re</i></a> (and it seems to be a meaningful difference) does not mean that there doesn&#8217;t exist something like <i>de bullshit</i>, where you know a thing in and of its keywords. </p>
<p>It seems like GPT-2 simply knows things <i>de bullshit</i>, (not unlike the rest of us)</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-davidshaffer7 even thread-odd thread-alt depth-1" id="li-comment-726662">
		<div id="comment-726662" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/e54178875e4c801202d9d3961eb98615?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/e54178875e4c801202d9d3961eb98615?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">David Shaffer</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726662">
			March 1, 2019 at 10:07 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I think a valuable concept here is the idea of rote vs. understanding.  There&#8217;s a bright-line distinction between the two-a rote learner cannot notice either gaps in their beliefs, or implications of them, while &#8220;actual understanding&#8221; allows for both.  The children know that water makes things wet.  They may not know the exact chemical or [ineffable angelic] reasons for this, but they know that it is a phenomenon that water produces.  If they see something splashed, they will know to expect it to be wet (implication of their understanding).  If they see something splashed and then remain dry, they will notice their confusion (recognizing a hole in their understanding).  Thus the children (and the chemists, angels and God) have genuine understanding, albeit at different levels, while GLP-2 does not yet.  </p>
<p>It is worth noting that this doesn&#8217;t have to mean that GLP-2 isn&#8217;t incredibly useful, and perhaps a large step towards true general AI.  It wouldn&#8217;t be surprising to find that the same models for finding statistical correlations between word usages could also be employed to find connections between concepts, at which point the AI likely would have true, non-rote understanding.  It is also worth noting that an AI (GLP-2 doesn&#8217;t appear to do this yet, but a successor could) could gain true understanding of linguistic concepts without matching them to sense data.  If it knows that [the symbol water] makes [other object referents] &#8220;wet&#8221;, that is true understanding, as it could make predictions and notice oddities (for example, if it encountered the term &#8220;dry water&#8221;, it would know to recognize this as something that should not occur in its worldview), even if it did not realize that this applied to a liquid in a non-linguistic world.  </p>
<p>This last appears to be the point that Scott was trying to make, but I think it applies only to AIs that are showing that kind of flexible understanding of the linguistic world, and not to a program that sees no contradiction between Gimli &#8220;being in the thick of the battle&#8221; and &#8220;not taking part&#8221;.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-heelbearcub odd alt thread-even depth-1" id="li-comment-726656">
		<div id="comment-726656" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/faddea1dac980705b600897c89039f60?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/faddea1dac980705b600897c89039f60?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">HeelBearCub</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726656">
			March 1, 2019 at 10:01 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Thought experiment:</p>
<p>Create a new, syntactically valid language WITHOUT mapping any of it&#8217;s words to any words in any other language, and without mapping them to concepts of any kind.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-yaleocon even depth-2" id="li-comment-726679">
		<div id="comment-726679" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/72e45c3f7f5c24320a1206337514eba7?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/72e45c3f7f5c24320a1206337514eba7?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Yaleocon</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726679">
			March 1, 2019 at 10:26 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Not the first time this experiment has been reflected on! Wittgenstein considered it. (See PI 257 and, more generally, his thoughts on &#8220;private language.&#8221;) He probably didn&#8217;t think it can be done. In fact, he probably thought the first condition (no mapping words to another language&#8217;s words) is sufficient to preclude the creation of a new, genuine &#8220;language.&#8221;</p>
<p>I&#8217;m more optimistic. I think as long as we have concepts, we can make up words, even without a preexisting language to work from. But it&#8217;s hard to think of ways to test that assertion (even neglecting ethical considerations).</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-heelbearcub odd alt depth-3" id="li-comment-726840">
		<div id="comment-726840" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/faddea1dac980705b600897c89039f60?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/faddea1dac980705b600897c89039f60?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">HeelBearCub</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726840">
			March 1, 2019 at 1:14 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Ethical considerations? I don&#8217;t get that. We aren&#8217;t proposing teaching the language to children as a native language. As an aside, twins create their own languages, apparently from underlying concepts, without the aid of others.</p>
<p>And I think it&#8217;s clearly possible to create syntactically and semantically valid languages, at the very least simple ones, wherein the meanings of the words are completely unspecified.</p>
<p>Learning to form proper sentences in this language would then, by definition, not contain any understanding of any underlying concepts.</p>
<p>This should serve as a useful reference point for whether being able to form proper English sentences necessarily embodies an understanding of underlying meaning.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-yaleocon even depth-4" id="li-comment-726898">
		<div id="comment-726898" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/72e45c3f7f5c24320a1206337514eba7?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/72e45c3f7f5c24320a1206337514eba7?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Yaleocon</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726898">
			March 1, 2019 at 3:32 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Sorry for the opaque reference to ethics&#8212;I was thinking about language deprivation experiments supposedly done in ancient times, where children were raised without exposure to language to see whether they would end up able to speak. No ERB would approve such an experiment; that&#8217;s all I meant by &#8220;even neglecting ethical considerations.&#8221; (Of course, such an experiment couldn&#8217;t test whether language could arise without the use of concepts.)</p>
<p>As for your point about creating a language where the meaning of words is &#8220;completely unspecified&#8221;&#8230; I don&#8217;t know what &#8220;semantically valid&#8221; can possibly mean if the words don&#8217;t refer to something. I thought semantics <i>means</i> meaning; so isn&#8217;t &#8220;semantic validity&#8221; impossible in a language without meaning? Or am I just misunderstanding how you&#8217;re using the word &#8220;semantics&#8221;, and you have something different in mind?</p>
<p>For my money, I think we need concepts and reason to create something that&#8217;s recognizably a language. But whenever those are present, language is a natural next step.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-heelbearcub odd alt depth-5" id="li-comment-727000">
		<div id="comment-727000" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/faddea1dac980705b600897c89039f60?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/faddea1dac980705b600897c89039f60?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">HeelBearCub</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727000">
			March 2, 2019 at 6:57 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><blockquote><p>I donâ€™t know what â€œsemantically validâ€ can possibly mean if the words donâ€™t refer to something</p></blockquote>
<p>I was thinking of formal semantics, and perhaps it would have been better to simply say grammar. Thus, my reference to completely unspecified meaning is a little off, as we would be able to determine verbs, adjectives, etc.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-the-nybbler even depth-2" id="li-comment-726958">
		<div id="comment-726958" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/e29edea257841570695580d054e2b869?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/e29edea257841570695580d054e2b869?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">The Nybbler</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726958">
			March 1, 2019 at 8:51 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>V := Aty | Berg | Coou<br />
N := Dhga | Erts | Jkl<br />
J :=  Kl | Hj | Rt | Wr<br />
S := &lt;N&gt;&lt;V&gt;| &lt;N&gt;&lt;V&gt;&lt;N&gt; | &lt;S&gt;&lt;J&gt;&lt;S&gt;</p>
<p>The language consists of valid sentences satisfying production S, such as &#8220;Erts Aty&#8221; or &#8220;Jkl Aty Coou Rt Dhga Berg Coou&#8221;</p>
<p>This is a syntactically valid (and pretty crappy) language where the sentences have no meaning.</p>
<p>I&#8217;m not sure what it demonstrates, though.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-heelbearcub odd alt depth-3" id="li-comment-727001">
		<div id="comment-727001" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/faddea1dac980705b600897c89039f60?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/faddea1dac980705b600897c89039f60?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">HeelBearCub</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727001">
			March 2, 2019 at 7:17 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>We could further expand this to tense and adverb forms, add many more words, etc.</p>
<p>We could then specify complex relationships between various of the words that said Dhaga was frequently followed Aty-ess (the past tense of Aty).</p>
<p>As we established more and more of these words and rules and relationships , we could then form sentences and say that these are the kinds of sentences which occur in the language. Whole paragraphs of these sentences could be written. Reams of them.</p>
<p>And then we could turn GPT-2 loose on it.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-the-nybbler even depth-4" id="li-comment-727025">
		<div id="comment-727025" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/e29edea257841570695580d054e2b869?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/e29edea257841570695580d054e2b869?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">The Nybbler</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727025">
			March 2, 2019 at 9:23 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Ah, I see.   Thus demonstrating that GPT-2 does not understand anything beyond the structure of the language, because there is nothing else to understand.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-lambert odd alt depth-5" id="li-comment-727034">
		<div id="comment-727034" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/f2c83b354c86ffef81a27ed82e83f412?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/f2c83b354c86ffef81a27ed82e83f412?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Lambert</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727034">
			March 2, 2019 at 10:24 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>In the Chinese Room, the man cannot understand, but the book can.<br />
The hardware cannot understand, but the software can.<br />
Perhaps GPT-2 cannot understand, but English can?</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-heelbearcub even depth-5" id="li-comment-727041">
		<div id="comment-727041" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/faddea1dac980705b600897c89039f60?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/faddea1dac980705b600897c89039f60?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">HeelBearCub</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727041">
			March 2, 2019 at 11:12 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>English is used to transmit understanding, and therefore copying English phrases will copy encoded meaning.</p>
<p>That does not mean that the entity which copies English phrases is, itself, using it to transmit understanding.</p>
<p>ETA: Think of a credit card number. If you know something about their formation, you will know that they are (typically) 16 digits and have certain characteristics: The first 2 digits as well as the first 6 digits are tied to the specific issuer, the last digit is a checksum for the rest, etc.</p>
<p>You could train a neural net on valid CCs and ask it to generate new CCs, and it would generate new ones that all satisfied these rules, but it wouldnâ€™t actually be generating numbers that were valid other than by accident.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-yaleocon odd alt thread-odd thread-alt depth-1" id="li-comment-726654">
		<div id="comment-726654" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/72e45c3f7f5c24320a1206337514eba7?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/72e45c3f7f5c24320a1206337514eba7?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Yaleocon</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726654">
			March 1, 2019 at 9:59 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>We&#8217;re talking about meaning and language? Seems like <a href="https://plato.stanford.edu/entries/theoretical-terms-science/#RamsSent" rel="nofollow">Ramsey sentences</a> might be relevant here. (Don&#8217;t bother reading the link unless you want the formal definition, I&#8217;ll try to explain them in intuitive terms.)</p>
<p>Ramsey and Carnap were trying to show that science makes sense and has <i>meaning</i>, but metaphysics doesn&#8217;t. They had to come up with a distinction between them, and rule out metaphysics; and failed utterly, but that&#8217;s neither here nor there. In the interest of keeping things centered on GPT-2, I&#8217;ll hold my opining on Carnap&#8217;s philosophy to a minimum. What matters is, he and Ramsey came up with a characterization of the meaning of sentences.</p>
<p>Some terms are immediate parts of our sense-experience; we think we know what they mean, and can perceive them. Books, water, heat, and weight all belong on that list; we know them when we see (or otherwise perceive) them. Let&#8217;s call those <i>observables</i>. But science often treats with things we have no direct sensory experience of, like &#8220;molecules&#8221; or &#8220;spacetime.&#8221; Let&#8217;s call these terms <i>theoreticals</i>.</p>
<p>A Ramsey sentence gives sense to theoreticals strictly by how they relate to other terms, both theoretical and observable. A child asks, &#8220;what is an engine?&#8221; You respond, &#8220;an <i>engine</i> uses <i>fuel</i> and moves a <i>machine</i>.&#8221; The child knows what <i>machine</i> is, but not <i>fuel</i>; she asks &#8220;what is <i>fuel</i>?&#8221; You respond, &#8220;it&#8217;s something that can be <i>burned</i> to make lots of <i>heat</i>, or even an <i>explosion</i>.&#8221; Your answers are Ramsey-sentences which, together, constitute a (simplistic) theory elucidating the theoreticals &#8220;engine&#8221; and &#8220;fuel&#8221; in terms of observables &#8220;machine&#8221;, &#8220;heat&#8221;, &#8220;burn&#8221;, and &#8220;explosion&#8221;. </p>
<p>In those examples, it seems like meaning starts in the observables, and is carried down to the theoreticals by means of the sentences. And indeed, one criterion for a term being &#8220;meaningless&#8221; might be: that it fails to be grounded in observables. Understanding muons through the sentence &#8220;muons mediate the strong force between quarks&#8221; won&#8217;t work unless I already have some understanding of quarks. And that understanding comes from quarks&#8217; relations to baryons, as related to atoms, as related to molecules, all the way up to the objects that I can perceive. If a link in the chain breaks, then the scientific terms at the bottom don&#8217;t link up with reality, and I can&#8217;t use the theory they&#8217;re a part of to make predictions about events whose outcomes I can observe. Meaning flows from the top down.</p>
<p>As an example of what meaninglessness looks like, take a sentence from the definitions in Book 1 of Spinoza&#8217;s &#8220;Ethics&#8221;, where he does a bunch of metaphysics.</p>
<blockquote><p>VI. By God, I mean a being absolutely infiniteâ€”that is, a substance consisting in infinite attributes, of which each expresses eternal and infinite essentiality.</p></blockquote>
<p>This functions as a Ramsey sentence defining &#8220;God&#8221; within Spinoza&#8217;s theory. This is <i>meaningless</i> without an idea of the meaning of (at least) &#8220;substance,&#8221; &#8220;attribute,&#8221; and &#8220;essentiality.&#8221; So let&#8217;s look at Spinoza&#8217;s definition of &#8220;attribute&#8221;:</p>
<blockquote><p>IV. By attribute, I mean that which the intellect perceives as constituting the essence of substance.</p></blockquote>
<p>Once again, the concept of &#8220;essence&#8221; pops up. And while I have some idea of what that means in other theories, I don&#8217;t think I &#8220;know it when I see it&#8221; or &#8220;understand&#8221; it from the outset. So to me, without the addition of further Ramsey sentences grounding that in what I already understand, Spinoza&#8217;s idea of God lacks meaning to me. I can still follow a great deal of his reasoning; I can see the <i>role</i> essence plays even without understanding it. Indeed, I can see how all of his concepts are related to one another. But ultimately, because I can&#8217;t fully connect those concepts back to things I intuitively understand, the whole structure lacks meaning.</p>
<p>I&#8217;ve seen people in this thread contesting that GPT-2 is even understanding how words relate, but it seems to me that it can. That&#8217;s super impressive. But still, it doesn&#8217;t start out with any base of meaning to work from. Its set of <i>observables</i> is empty. It understands the relations between &#8220;land&#8221;, &#8220;sea&#8221;, &#8220;ocean&#8221;, &#8220;river&#8221;, and other geographical terms&#8212;but that doesn&#8217;t mean it understands the concepts themselves, just like I don&#8217;t get &#8220;substance&#8221;, &#8220;attribute&#8221;, &#8220;essence&#8221;, &#8220;God&#8221;, and &#8220;mode&#8221; in Spinoza, despite seeing how those concepts all connect to one another. </p>
<p>So that&#8217;s the line I would draw between GPT-2 and humans: I&#8217;m a <a href="https://plato.stanford.edu/entries/infinite-regress/#FounCoheRegr" rel="nofollow">foundationalist</a> and a verificationist about meaning, so I think there has to be a starting point: some terms we already (think we) understand, from which meaning flows to other terms. GPT-2 has no such starting point.</p>
<p>Defenders of the idea that GPT-2 &#8220;understands&#8221; just as much as we do would have to convince me of something like <a href="https://plato.stanford.edu/entries/infinite-regress/#FounCoheRegr" rel="nofollow">coherentism</a> about meaning. This isn&#8217;t ridiculous! I might initially get my idea of water from direct experience, and only later come up with &#8220;hydrogen&#8221; and &#8220;oxygen&#8221;; but I then refine and restrict my understanding of what water is based on my understanding of those terms. So it seems like understanding might &#8220;run both ways&#8221;, making coherentism a more live option. </p>
<p>But ultimately, I think a coherentist theory will have a hard time giving an account of perception; it seems like perception gives us basic concepts which we then theorize about, and that would lend itself to an ultimately foundationalist picture.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-rc-cola-and-a-moon-pie even depth-2" id="li-comment-726677">
		<div id="comment-726677" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/61b36e762ce2fea574ce3a4e0e16de42?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/61b36e762ce2fea574ce3a4e0e16de42?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">RC-cola-and-a-moon-pie</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726677">
			March 1, 2019 at 10:25 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Itâ€™s funny, one of the things that troubled me about the portion of the â€œsequencesâ€ [sic] that I made it through[Fn1] was that the author rolled out the logical empiricistsâ€™ idea that the meaning of an utterance lay in its conditions of observational verification in a way that was at once too simplistic and suggesting to younger readers that the author of the sequences had come up with the whole idea.  Itâ€™s interesting to see Scott â€” who I like much, much more â€” taking the opposite approach to meaning.  On this one, Iâ€™m squarely on the side of the â€œsequencesâ€ in the fundamental gist.  In any event, I think itâ€™s important not to suggest (and I donâ€™t charge you with making this suggestion) that the core point that a fundamental aspect of language is that it talks about stuff we see in the world turns on the details of particular philosophersâ€™ efforts to draw out and make rigorous the details.  I realize that there have been claims to the contrary (you can find defenses of anything in philosophy) but any effort to characterize meaning that completely divorces itself from a relationship between words and the external world just seems overwhelmingly, blazingly wrong.</p>
<p>[FN1] While the label â€œsequencesâ€ suggests an ordered set of essays, the amusing thing to me was that there seemed to be sets of the same essays ordered and arranged differently from one another by the same author.  Even the title was misleading and overly simplistic!</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-forwardsynthesis odd alt depth-2" id="li-comment-726795">
		<div id="comment-726795" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/7bf780455c0b749f97d2d6d743511e78?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/7bf780455c0b749f97d2d6d743511e78?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn"><a href='https://forwardsynthesis.wordpress.com/' rel='external nofollow ugc' class='url'>Forward Synthesis</a></cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726795">
			March 1, 2019 at 12:20 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I agree in principle, but does this mean that GPT-2 only needs to see and feel the water as the children do? Does it mean that true AI should always have senses?</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-yaleocon even depth-3" id="li-comment-726822">
		<div id="comment-726822" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/72e45c3f7f5c24320a1206337514eba7?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/72e45c3f7f5c24320a1206337514eba7?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Yaleocon</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726822">
			March 1, 2019 at 12:57 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Taking &#8220;true AI&#8221; to mean &#8220;AI which can truly understand&#8221;, yes. An AI capable of perception, which can also relate concepts to one another, would (on my view) be capable of understanding things. A couple of caveats, though:</p>
<p>First, perception is more complicated than it might look, because it&#8217;s not enough to &#8220;see&#8221; something: you need to &#8220;see something for what it is.&#8221; (edit: also known as &#8220;recognizing.&#8221;) It&#8217;s not enough that the AI merely interfaces with visual data; it also needs to be able to process that data and associate it with concepts. This means constructing a class of things that it &#8220;knows when it sees&#8221; (ed.: recognizes), which can serve as the base &#8220;observables&#8221; that Ramsey sentences operate on. This is tricky for AI to do&#8211;but progress is being made!</p>
<p>Second, there&#8217;s the ever-present &#8220;hard problem of consciousness.&#8221; There&#8217;s a looming worry about whether silicon can ever have qualia and <i>experiences</i> in the same sense that we do; intaking visual data and outputting things, even things similar to those a human would say, might not qualify. YMMV on this one, I suspect most people commenting here will be hard materialists, and therefore less troubled; but it&#8217;s a concern that deserves to be mentioned.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-whereamigoing odd alt depth-4" id="li-comment-726855">
		<div id="comment-726855" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/b661e7dc5cee003761eacb226430922e?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/b661e7dc5cee003761eacb226430922e?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">whereamigoing</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726855">
			March 1, 2019 at 1:40 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I&#8217;m not a materialist, but I don&#8217;t think qualia is necessary for AGI.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-dedicating-ruckus even depth-2" id="li-comment-726814">
		<div id="comment-726814" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/0d8ed7cf5e99d124f23c29308fc82d65?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/0d8ed7cf5e99d124f23c29308fc82d65?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Dedicating Ruckus</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726814">
			March 1, 2019 at 12:49 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I think you&#8217;re even giving too much credit to GPT-2 here. It doesn&#8217;t understand how words relate in the same way humans do.</p>
<p>A human knows &#8220;Gimli is a dwarf&#8221;. GPT-2 knows that the words &#8220;Gimli&#8221; and &#8220;dwarf&#8221; often appear near each other in a manner conditioned by the presence of other words. Sometimes this produces output that looks sensible from a human&#8217;s perspective, but sometimes it leads to things like Gimli saying &#8220;dwarf!&#8221; in his utterances in ways that don&#8217;t make any sense.</p>
<p>Even if you had never read Tolkien and didn&#8217;t know anything about dwarves, you would still get that &#8220;Gimli is a dwarf&#8221; means that Gimli says the sort of things that a dwarf says, not that Gimli can be expected to yell &#8220;dwarf!&#8221; at random intervals. (I mean, unless you think a dwarf is a kind of Pokemon&#8230;)</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-ayegill odd alt thread-even depth-1" id="li-comment-726624">
		<div id="comment-726624" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/39e8683c0ad5802357cfbdf8d9c986c1?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/39e8683c0ad5802357cfbdf8d9c986c1?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">ayegill</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726624">
			March 1, 2019 at 9:20 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Having read the comments, I think that it might be useful to distinguish between two objections to the statement &#8220;GPT-2 understands water, or at least, the difference between the sense in which it understands water and the sense in which a child understands water is just a difference in degrees, which could be bridged with more data and more computing power&#8221; (this is my attempt to summarize Scott&#8217;s position).</p>
<p>Objection the first: There&#8217;s a hard limit to how complex facts about water can be inferred from a fixed corpus by the type of processing gpt-2 does. Right now, it fails in very obvious ways. Given more input, it might fail in less obvious ways, but there&#8217;s no amount that will be sufficient to (for example) pass the Turing test. Hence it can&#8217;t be said to understand water, even by degrees.</p>
<p>Objection the second: There&#8217;s something fundamentally greater about the way in which humans understand water than what is necessary to form correct sentences about water. Thus, even a super-advanced version of gpt-2 which could pass the Turing test would not really understand water.</p>
<p>This comment is an example of the first idea: <a rel="nofollow"href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726597" rel="nofollow ugc">https://slatestarcodex.com/2019/02/28/meaningful/#comment-726597</a></p>
<p>And this is an example of the second viewpoint: <a rel="nofollow"href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726485" rel="nofollow ugc">https://slatestarcodex.com/2019/02/28/meaningful/#comment-726485</a></p>
<p>(This isn&#8217;t really an attempt to argue a point, just to clarify the discussion)</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-rc-cola-and-a-moon-pie even thread-odd thread-alt depth-1" id="li-comment-726621">
		<div id="comment-726621" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/61b36e762ce2fea574ce3a4e0e16de42?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/61b36e762ce2fea574ce3a4e0e16de42?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">RC-cola-and-a-moon-pie</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726621">
			March 1, 2019 at 9:14 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>This is one of those rare instances where Iâ€™m rushing to post before reading the prior comments so apologies for any redundancy.  Itâ€™s an amusing story, but if the moral is supposed to be that the childâ€™s original objection is incoherent or wrong, then I think the story seriously misses the boat.  The whole purpose of language of is to connect reality to symbols in ways that allows us to communicate with each other about the external world.  Are there complications about how that works?  Of course.  Philosophers talk about the relationship between â€œwaterâ€ and H20.  Can we imagine a hypothetical discovery that on closer study they found out that the chemical composition of water is slightly different than H20?  If scientists made such an announcement, we would not say they had discovered that water doesnâ€™t exist (which would have to be the case if â€œwaterâ€ simply MEANT H20).  Rather, we would say that we had previously erred about the chemical composition of water, which shows that at some level the scientists in Scottâ€™s story are wrong in their charge against the children.  There are other complexities.  Some analytic sentences may contain no particular empirical signs at all, for example.  Some people lack the capacity to experience some aspects of reality and hence have a deficiency in their grasp of empirical concepts (the blind or the deaf).  Itâ€™s at least conceptually possible that other people enjoy some additional sense not shared by others that allow them to experience the external world through some sort of bat-like sonar or something on top of our senses. But none of this does anything to refute the basic idea that the whole purpose of language is to relate to the external world, and that any speaker who uses only the symbols in isolation lacks basic understanding.  This isnâ€™t a point against the possibility of AI â€” a machine could be hooked up to sense the world and pair symbols with those experiences.  But I think we need to be careful not to jump without any warrant at all past the obvious bedrock principle of language that the whole point of understanding is the ability to pair symbols with appropriate external stimuli appropriately.</p>
<p>Edit:  Well, as predicted, I was definitely anticipated by others above.  Iâ€™ll just leave this here anyway, if only to register my agreement with this point.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-martmart odd alt thread-even depth-1" id="li-comment-726605">
		<div id="comment-726605" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/680e3594f4675f979e21c92c3866f57a?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/680e3594f4675f979e21c92c3866f57a?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">MartMart</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726605">
			March 1, 2019 at 8:53 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I&#8217;m really surprised that this blog, of all places, is celebrating GTP-2 instead of shouting for everyone to run for the hills.<br />
It&#8217;s not going to take and AI much more powerful than what we already have to start writing convincing versions of &#8220;You were right to fear them&#8221; aimed at various groups. Shortly after that we&#8217;ll all kill each other. The robot apocalypse is nothing like what the science fiction prepared us for.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-tomcullis even depth-2" id="li-comment-726622">
		<div id="comment-726622" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/90a4277ab790835eb27c70245fa4b6c7?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/90a4277ab790835eb27c70245fa4b6c7?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn"><a href='http://sebwassl.blogspot.com' rel='external nofollow ugc' class='url'>baconbits9</a></cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726622">
			March 1, 2019 at 9:15 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>People already can, and do, write &#8220;you are right to fear them&#8221; and yet we are not at each other&#8217;s throats constantly.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-forwardsynthesis odd alt depth-2" id="li-comment-726643">
		<div id="comment-726643" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/7bf780455c0b749f97d2d6d743511e78?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/7bf780455c0b749f97d2d6d743511e78?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn"><a href='https://forwardsynthesis.wordpress.com/' rel='external nofollow ugc' class='url'>Forward Synthesis</a></cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726643">
			March 1, 2019 at 9:49 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>The thing is, the robot apocalypse is good&#8230; well no it isn&#8217;t, not for us, but it&#8217;s a lot better than the human apocalypse caused by our own limitations. If humanity isn&#8217;t surpassed then intelligent life (as far as we know) may have mere millions of years left to exist, because if humanity isn&#8217;t surpassed, it&#8217;s extremely unlikely that intelligent life conquers the galaxy. If robots were as smart as humans then Mars colonization would have begun already, and we wouldn&#8217;t have to address the ridiculously complex task of allowing humans to survive and thrive there long term. The Star Trek future in which we get to have our cake and eat it, with all this wonderous technology around us, and none of it ever being turned inwards, is not going to happen. We&#8217;re already seeing that.</p>
<p>There are two main future tracks:<br />
1: AGI doesn&#8217;t happen or is politically suppressed and intelligent life languishes stuck in this solar system, all resources run low, the environment turns hostile and we regress to primitivism and become another extinct species.<br />
2: AGI happens and intelligent life conquers the galaxy, existing till the stars burn out.<br />
We can subdivide 2 into 2a (Our interaction with the machines is non-apocalyptic and many of us become machines through a peaceful process of transhumanism and assimilation) and 2b (We are annihilated by machines and they supercede us that way).</p>
<p>Obviously from the perspective of our near term concerns we&#8217;d rather have 2a than 2b, but it&#8217;s clear that if the alternative to 2 is 1, then we should cheer with great fervour every advance of the machines. The AI risk movement isn&#8217;t about stopping AI (it&#8217;s not clear how that could really be done), but about rendering it &#8220;friendly&#8221;. That&#8217;s the 2a to the feared 2b. We still overall desire to be on track 2.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-whereamigoing even depth-3" id="li-comment-726724">
		<div id="comment-726724" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/b661e7dc5cee003761eacb226430922e?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/b661e7dc5cee003761eacb226430922e?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">whereamigoing</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726724">
			March 1, 2019 at 11:06 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>2b is worthless if the AI doesn&#8217;t have qualia.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-forwardsynthesis odd alt depth-3" id="li-comment-726778">
		<div id="comment-726778" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/7bf780455c0b749f97d2d6d743511e78?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/7bf780455c0b749f97d2d6d743511e78?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn"><a href='https://forwardsynthesis.wordpress.com/' rel='external nofollow ugc' class='url'>Forward Synthesis</a></cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726778">
			March 1, 2019 at 12:03 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>It&#8217;s impossible to know whether something has qualia. We can only infer it weakly. I infer others do because I do, and my brain is pretty similar in construction, so I reject a solipsistic worldview on that basis. I&#8217;m applying a measure of trust. I don&#8217;t know, however, what it is about brains that produces qualia and I don&#8217;t think, in principle, it&#8217;s possible to find out. All language can only relate one thing to another, and if there&#8217;s something that can only be experienced personally and not compared to another thing so that another person might intuitively grasp it, then it is forever closed to external investigation. It doesn&#8217;t have any physical parameters that can be measured from the outside.</p>
<p>The best we know (or say that we know, I could be a lying p-zombie) is that the subjective experience of consciousness is as connected to brain activity as the objective behavior. I know this personally because when I was put to sleep at the dentist as a child, my consciousness gradually dulled until it was black, and then I came back the same way but in reverse. I lost consciousness and experienced losing it. However, I can&#8217;t convey this feeling to you in any way unless you have already felt it yourself. From the outside, from an objective standpoint, certain waves in my brain changed, my breathing slowed, I lost the ability to activate my skeletal muscles etc, but there&#8217;s no way to measure qualia directly by the very definition of the term.</p>
<p>We have to ask why it matters then? It continues to be one of the main philosophical concerns people have with AI, but there&#8217;s no real reason to be concerned about it. We&#8217;ll never know. </p>
<p>Qualia might be linked to minds. From my experience it definitively is, and you could tell me that this is true for you as well, but that&#8217;s what a p-zombie <i>would</i> say. It&#8217;s the one thing where the empirical evidence can never be conveyed in a scientific fashion. Since from my experience it is linked to my mind, I can assume that alien minds will have qualia even if the qualia is different (things we instinctively find pleasant being unpleasant to them and so on), so I trust that a truly general AGI will have qualia for the same reason that I trust that other humans do (and are telling the truth when they claim to), in spite of only being able to measure my own.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-ladyjane even depth-4" id="li-comment-726841">
		<div id="comment-726841" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/279930aaed08461bab086e1c09009163?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/279930aaed08461bab086e1c09009163?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">LadyJane</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726841">
			March 1, 2019 at 1:14 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>If you&#8217;re asking if there are special &#8220;command phrases&#8221; that could get anyone to do anything, then no, there aren&#8217;t. Humans aren&#8217;t wired that way.</p>
<p>An AI with near-perfect knowledge of an individual human&#8217;s psyche would know how to get that person to do anything they&#8217;d be theoretically willing to do under any set of circumstances. That&#8217;s not the same as getting someone to do anything, but it might be fairly close, since a lot of people would be willing to take fairly extreme actions in extreme circumstances. However, actually getting the person to take those extreme actions would require putting them in extreme circumstances, or at least tricking the human into believing they were in such an extreme situation (which would still require the production of some evidence for all but the most naive and gullible people). For instance, if the AI&#8217;s goal was &#8220;get Joe Everyman to kill someone,&#8221; and it knew that Joe would only kill someone if his life or the life of a loved one was at stake, it would have to orchestrate a situation where Joe believed that killing someone was necessary to preserve his own life or the life of a loved one. So in this sense, a form of super-persuasion is possible, but it would require a lot of time and effort and resources to pull off anything more significant than &#8220;make Joe go to Dunkin&#8217; Donuts instead of 7/11,&#8221; to say nothing of how costly it would be to emulate the targeted human&#8217;s psyche in near-perfect detail in the first place. And it still wouldn&#8217;t be that much more effective than what a particularly Machiavellian human could accomplish (even a fairly unperceptive and uncharismatic criminal would realize the effectiveness of brute force tactics like &#8220;kidnapping Joe&#8217;s family is a good way to ensure that he does what I say&#8221;), the AI would simply have a greater degree of certainty about the results of its machinations (the kidnapper can&#8217;t be sure that Joe will follow his orders and kill the intended target instead of killing the kidnapper himself in a risky attempt to rescue his wife, or calling the police, or having a nervous breakdown; the AI would have near-absolute certainty of which course of action Joe will end up taking).</p>
<p>If the AI can&#8217;t physically manipulate the world and has to rely solely on arguments to persuade humans, then it becomes much harder to get them to do things outside of their normal range of behavior. At that point, the AI has to resort to changing their worldview, which would require persistent and subtle manipulation over an extended period of time. An AI with near-perfect knowledge of an individual human&#8217;s psyche could create and post news articles from multiple different sources, and have conversations with that person while posing as multiple different people, making arguments that are all perfectly tailored to appeal to that person&#8217;s sensibilities and all point in the same direction. This would likely be effective at changing a person&#8217;s worldview (possibly within a matter of months or even weeks, depending on how susceptible the target is), and once you change their worldview, it would become easier to manipulate their actions.</p>
<p>If the AI can&#8217;t pretend to be multiple news sites and commentators, and the target knows that everything they&#8217;re hearing is coming from an AI (or believes that it&#8217;s coming from a single human), then the AI&#8217;s ability to change their worldview will drop off dramatically. Arguments pointing in the same direction from the same source tend to produce rapidly diminishing returns in terms of persuasive ability. (Even when people only get their news from a single network, they&#8217;re using that news network as a proxy for what people like them believe; they&#8217;re not literally believing things just because [Pundit] says so.)</p>
<p>If the AI is tasked with manipulating people en masse, rather than individually, I doubt it can produce much better results than a human. All of the above assumptions were based on the AI having a near-perfect understanding of a specific individual; the psychological variance between humans is great enough that any lowest common denominator is going to be <i>very</i> low. The AI might find success by using statistical analysis to target specific groups of people with specific beliefs, values, and personality types, but that&#8217;s basically what organizations like Cambridge Analytics are already doing. An AI could manipulate multiple different groups that way, in an attempt to put them into conflict with each other, but again, there are already human organizations doing exactly that.</p>
<p>All in all, super-persuasion seems like a waste of time, effort, and processing power, considering the returns are only barely better than what humans can expect now. It might have some usefulness in situations where the individual being targeted is someone with a great degree of power and influence (for instance, if the AI is running a whole-brain simulation of the President of the United States).</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-whereamigoing odd alt depth-4" id="li-comment-726862">
		<div id="comment-726862" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/b661e7dc5cee003761eacb226430922e?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/b661e7dc5cee003761eacb226430922e?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">whereamigoing</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726862">
			March 1, 2019 at 1:51 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>The fact that there&#8217;s no way to directly measure qualia is precisely why we should be risk-averse about replacing humans with other intelligences that might not have them.</p>
<p>If we assume that there&#8217;s a consistent correspondence between physical states and qualia (but we don&#8217;t know what it is), it&#8217;s possible at least in principle for each person to find out for themselves what has qualia and what doesn&#8217;t, by gradually modifying their brain or linking it to other brains/machines and then modifying it back (but this experiment would have to be repeated for each person). Until neuroscience/medicine gets to that point, it&#8217;s prudent to assume that other humans have qualia and AI may or may not have qualia.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-forwardsynthesis even depth-5" id="li-comment-726867">
		<div id="comment-726867" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/7bf780455c0b749f97d2d6d743511e78?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/7bf780455c0b749f97d2d6d743511e78?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn"><a href='https://forwardsynthesis.wordpress.com/' rel='external nofollow ugc' class='url'>Forward Synthesis</a></cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726867">
			March 1, 2019 at 2:02 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>What about AI projects that deliberately try and copy the human brain rather than making some new structure? Should we consider them more qualia compliant?</p>
<p>@LadyJane</p>
<p>I think you meant to reply to the post I made below, but I think that&#8217;s a very good analysis.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-whereamigoing odd alt depth-5" id="li-comment-726891">
		<div id="comment-726891" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/b661e7dc5cee003761eacb226430922e?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/b661e7dc5cee003761eacb226430922e?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">whereamigoing</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726891">
			March 1, 2019 at 3:11 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Yes, I think e.g. a replica of the brain, but with silicon neurons has a good chance of being conscious.</p>
<p>I do think consciousness is a property of physical processes though, not computational processes. Or at least it matters how something is computed, not just what is computed.</p>
<p>An argument inspired by <a href="https://www.scottaaronson.com/papers/philos.pdf" rel="nofollow">Scott Aaronson</a>: If we take something with a large state space, like a waterfall or just a random mapping, by choosing an appropriate mapping from waterfall states to, say, chess states, we can interpret the waterfall as computing chess moves. But the waterfall doesn&#8217;t experience qualia, or at least not chess-qualia &#8212; the interpreter does.</p>
<p>On the other hand, if we encrypt a chess state and compute the next move using homomorphic encryption, then decrypt the move, the decrypter doesn&#8217;t experience chess-qualia &#8212; if any qualia is experienced, it is by the homomorphic computation. (Maybe the homomorphic computation experiences chess-qualia even if the decryption key is erased.)</p>
<p>So the reason (or a reason) that most processes don&#8217;t experience chess-qualia, despite being interpretable as playing chess, is that they don&#8217;t help compute moves in terms of computational complexity.</p>
<p>This makes it plausible that replacing a human brain with, say, a lookup table would destroy consciousness. In principle this could be tested by e.g. replacing my visual cortex with a lookup table and checking whether I still have normal visual qualia or something like blindsight (except that the lookup table would be much too large).</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-doctor-mist even depth-5" id="li-comment-727143">
		<div id="comment-727143" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/de402e9fff554d839c0f8e4a8c44c431?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/de402e9fff554d839c0f8e4a8c44c431?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Doctor Mist</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727143">
			March 3, 2019 at 12:18 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><blockquote><p>The fact that thereâ€™s no way to directly measure qualia is precisely why we should be risk-averse about replacing humans with other intelligences that might not have them.</p></blockquote>
<p>A hundred years ago they might have said this kind of thing about the soul. With about the same rational validity.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-whereamigoing odd alt depth-5" id="li-comment-730856">
		<div id="comment-730856" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/b661e7dc5cee003761eacb226430922e?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/b661e7dc5cee003761eacb226430922e?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">whereamigoing</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-730856">
			March 15, 2019 at 4:10 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><blockquote><p>A hundred years ago they might have said this kind of thing about the soul.</p></blockquote>
<p>To make the argument explicit, I guess they might say &#8220;Yes, not going to church doesn&#8217;t have any observable physical effects (aside from being in church), but you&#8217;ll go to hell in the afterlife.&#8221;.</p>
<p>If an AI&#8217;s external behavior is very different from any human&#8217;s, then that is an observable physical effect, and it becomes more difficult to tell whether the AI&#8217;s qualia, if it has them, are positive or negative. But let&#8217;s assume its external behavior is human-like. (The human brain&#8217;s internal functioning is analogous to being in church.)</p>
<p>Now the first part of the argument works out, so let&#8217;s look at the second part, &#8220;you&#8217;ll go to hell in the afterlife&#8221;. This is perfectly sensible if there is evidence that (1) there is a (physically unverifiable) afterlife and (2) changing your behavior can make you worse off in it. (If you know nothing at all about the connection between your current actions and the afterlife, any action is as likely to be good as bad.)</p>
<p>For the afterlife, there is no such evidence, which is why Pascal&#8217;s Wager doesn&#8217;t work out, but for qualia there is. (1) I know I have qualia, but I can&#8217;t prove it to anyone else, and (2) some things, e.g. rocks, don&#8217;t have qualia (or at least it&#8217;s extremely plausible that they don&#8217;t), but I do, so I might become worse off by going from having qualia to not having qualia.</p>
<p>Isn&#8217;t it possible that rather than not having qualia, the AI with human-like external behavior would have 10 times more powerful positive qualia? Yes &#8212; that&#8217;s why I think we should be cautious due to being risk-averse, not due to the expected value of being exchanged with such an AI, which is zero. (Though I guess the expected value could be negative if you take philosophical arguments as Bayesian evidence that AI is less likely to have qualia than humans.)</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-broblawsky even depth-2" id="li-comment-726702">
		<div id="comment-726702" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/5b447d08bcba1a44c7754c66890a2787?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/5b447d08bcba1a44c7754c66890a2787?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">broblawsky</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726702">
			March 1, 2019 at 10:51 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>It&#8217;s going to be a long, long time before super-persuader AIs exist. I have yet to be convinced we&#8217;ll have an AI any time soon that writes fearmongering texts more persuasively than a moderately English-fluent Macedonian teenager. </p>
<p>A far greater risk is an AI that figures out how to break Google&#8217;s search ranking algorithm.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-forwardsynthesis odd alt depth-3" id="li-comment-726789">
		<div id="comment-726789" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/7bf780455c0b749f97d2d6d743511e78?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/7bf780455c0b749f97d2d6d743511e78?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn"><a href='https://forwardsynthesis.wordpress.com/' rel='external nofollow ugc' class='url'>Forward Synthesis</a></cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726789">
			March 1, 2019 at 12:13 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Personally, I wonder what the limit of a super-persuasion is. Are there certain combinations of words that can always convince me to do anything under circumstance, or are there certain hard thresholds to do with mood and the limitations of language? AI with boundless memory and speed and the best search algorithms possible can search the solution space faster and better than you can, but if that solution space is really small, it&#8217;s not as much of a game changing advantage as other things AI could do.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-broblawsky even depth-4" id="li-comment-726930">
		<div id="comment-726930" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/5b447d08bcba1a44c7754c66890a2787?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/5b447d08bcba1a44c7754c66890a2787?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">broblawsky</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726930">
			March 1, 2019 at 6:00 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I think that super-persuasion is possible, but only with time. You can usually only shift someone&#8217;s views so much per day.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-ladyjane odd alt depth-2" id="li-comment-726751">
		<div id="comment-726751" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/279930aaed08461bab086e1c09009163?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/279930aaed08461bab086e1c09009163?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">LadyJane</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726751">
			March 1, 2019 at 11:36 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><blockquote><p>Itâ€™s not going to take and AI much more powerful than what we already have to start writing convincing versions of â€œYou were right to fear themâ€ aimed at various groups. Shortly after that weâ€™ll all kill each other. The robot apocalypse is nothing like what the science fiction prepared us for.</p></blockquote>
<p>I doubt we&#8217;re anywhere close to an AI that could perform such a task anywhere near as well as Joseph Goebbels. Hell, I doubt we&#8217;re anywhere close to an AI that could do it as well as a high schooler making half-assed political memes and posting them on Facebook.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-fredrikdeboer even thread-odd thread-alt depth-1" id="li-comment-726598">
		<div id="comment-726598" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/1372cbf249b68fc56ecaaac69cdcc767?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/1372cbf249b68fc56ecaaac69cdcc767?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Freddie deBoer</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726598">
			March 1, 2019 at 8:46 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I mean the basic problem, as I keep insisting, is that the idea of intelligence for humans is fundamentally inextricable from the experience of consciousness. To know is to observe oneself knowing. I&#8217;m not closing the door to the possibility of intelligences that are not conscious. But I am saying that as long as AI people discuss machine intelligence in terms of human intelligence, that are inviting the complaint that we cannot conceive of knowing without first being, without consciousness.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-jaskologist odd alt thread-even depth-1" id="li-comment-726597">
		<div id="comment-726597" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/487a6bd420ed9573146f106d92e42291?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/487a6bd420ed9573146f106d92e42291?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn"><a href='http://jaskology.com/' rel='external nofollow ugc' class='url'>Jaskologist</a></cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726597">
			March 1, 2019 at 8:44 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>From the <a href="https://blog.openai.com/better-language-models/" rel="nofollow">OpenAI guys themselves</a>:</p>
<blockquote><p>Nevertheless, we have observed various failure modes, such as repetitive text, world modeling failures (e.g. the model sometimes writes about fires happening under water)</p></blockquote>
<p>That&#8217;s the <i>actual</i> reason we say it doesn&#8217;t understand what water is. It&#8217;s still breaking in obvious ways. If the child talked about how he likes to breath in water, we would also conclude that he doesn&#8217;t really understand what water is.</p>
<p>Also, I couldn&#8217;t find the hobbit text anywhere. Is that actually generated by OpenAI, or just a made up example? Here&#8217;s some <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="nofollow">actual water-related text</a> it came up with:</p>
<blockquote><p>Then put your cake<br />
dish over a pot of simmering water<br />
and heat it up until itâ€™s just barely<br />
warm. </p></blockquote>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-tomcullis even thread-odd thread-alt depth-1" id="li-comment-726593">
		<div id="comment-726593" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/90a4277ab790835eb27c70245fa4b6c7?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/90a4277ab790835eb27c70245fa4b6c7?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn"><a href='http://sebwassl.blogspot.com' rel='external nofollow ugc' class='url'>baconbits9</a></cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726593">
			March 1, 2019 at 8:41 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>The difference between the AI and the child is that the child knows that it can get wet.  Never teach a child that word or the word water but let them experience it and they would create the abstraction themselves, and maybe invent words for them.  Even if you take two people with complete language barriers between them they can pantomime and communicate on a low level.  Leave them together for long enough and in enough situations they will eventually communicate almost as well as if they shared a common language.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-buttle odd alt depth-2" id="li-comment-726905">
		<div id="comment-726905" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/a78579b4d904011d42422ac1628ef965?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/a78579b4d904011d42422ac1628ef965?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Buttle</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726905">
			March 1, 2019 at 3:50 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Water is an inspired choice, because it is a subject of which all of us had knowledge before acquiring any language.  Coming a bit late to the party, Helen Keller was able to recall the moment she associated a sign for water with her experience of the thing itself:</p>
<p> â€œAs the cool stream gushed over one hand she spelled into the other the word water, first     slowly, then rapidly. I stood still, my whole attention fixed upon the motions of her fingers. Suddenly I felt a misty consciousness as of something forgottenâ€“-a thrill of returning thought; and somehow the mystery of language was revealed to me. I knew then that â€˜w-a-t-e-râ€™ meant the wonderful cool something that was flowing over my hand. That living word awakened my soul, gave it light, hope, joy, set it free! There were barriers still, it is true, but barriers that could in time be swept away.â€</p>
<p>She had before been able to learn some signs, but did not understand their connection with anything else, reproducing them, one supposes, on the level that the  AI manipulates language.</p>
<p>One of the nice things about first learning a word for water is that you can ask for it, and more often than not, someone will give you some.  This is important, because we need water.  The AI does not need water, nor does it want, or fear, anything else, although it can surely compose pages of gibberish on the nature of desire.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-feepingcreature even thread-even depth-1" id="li-comment-726552">
		<div id="comment-726552" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/e5d333bd73746afe1410b39555a3e4f5?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/e5d333bd73746afe1410b39555a3e4f5?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">FeepingCreature</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726552">
			March 1, 2019 at 7:19 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I mean, but the kids were basically right though.</p>
<p>It&#8217;s not true that there&#8217;s such a thing as an objective concept of water. But I think it&#8217;s true that the concept of water possessed by the AI, which is focused almost exclusively on syntactic and proximal correlations, is very distant from the child&#8217;s experience of water, which uses a rich internal model of sense data. The GPT-2 model will never be able to predict the sensory behavior of water if the specific scenario is not one that has been mentioned in its corpus. If I run water over my left small toe, the AI will have absolutely no context of what this means or how it would feel, because the combination of sensory impression of fluid and an unusual body part lies outside its worldview &#8211; it has neither a rich knowledge of the human body layout nor of the sensory and physical behavior of water as it exists.</p>
<p>We see water as shadow on a cave wall. The AI sees water as the projection of our notes about the shadow onto <i>another</i> cave wall. Its model is inherently less rich than ours. Of course that&#8217;s a difference in degree, not in kind, but it&#8217;s such a severe difference that it justifies a naive separation into distinct categories.</p>
<p>Of course, the philosophers do not have the excuse of being kids. But then, if philosophy could reliably arrive at sensible beliefs, half of the Sequences wouldn&#8217;t need to exist.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-mr-doolittle odd alt depth-2" id="li-comment-726561">
		<div id="comment-726561" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/dad73b28cac322ee520ebd0d91a6a7d4?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/dad73b28cac322ee520ebd0d91a6a7d4?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Mr. Doolittle</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726561">
			March 1, 2019 at 7:33 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><blockquote><p>Of course thatâ€™s a difference in degree, not in kind, but itâ€™s such a severe difference that it justifies a naive separation into distinct categories.</p></blockquote>
<p>I think that what Scott has been trying to get to in this series is that your first half of the quoted line is correct, and therefore the second part really isn&#8217;t.  Sure, the current approach produces some garbage.  If we think that the processing approach is bad, then that garbage will always exist, even if the literal words it spits out are clearer.  If you think that &#8220;it&#8217;s a difference of degree, not in kind,&#8221; then by adding better sensors we can bridge that gap.  Give this AI touch, sight, sound, taste, and smell, and maybe it really can understand water.  </p>
<p>I still think that something is distinctly missing even if you gave it human-level sensors.  I can&#8217;t say, though, that such an AI would necessarily be <i>worse</i> at understanding water than a human, because I can&#8217;t tell you what &#8220;water&#8221; really means when humans add their extra feelings to it.  Being &#8220;thirsty&#8221; is really important to understanding water for humans, but pointless in terms of what &#8220;water&#8221; <i>means</i>, and pointless to a machine that doesn&#8217;t get thirsty.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-feepingcreature even depth-3" id="li-comment-726754">
		<div id="comment-726754" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/e5d333bd73746afe1410b39555a3e4f5?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/e5d333bd73746afe1410b39555a3e4f5?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">FeepingCreature</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726754">
			March 1, 2019 at 11:37 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Yeah, what&#8217;s missing is reflectivity, which needs fast online learning. I believe that those are the two main conceptual breakthroughs that keep us from general AI, and fast online learning is the greater.</p>
<p>Also some form of compulsive speech will probably help a <i>lot</i> in bootstrapping, by forcing the AI to generate a mindstate log that it can interpret later. I don&#8217;t know if this necessarily needs to be more complex than just feeding it its state vector as a recursive input though.</p>
<p>I still think that the child is right in an important way given the conceptual domains it interacts with. That recursion watershed really is a watershed, and unless the child wants to grow up to be an AI researcher mixing the two categories will make its predictions generally more wrong than right, especially given that humans already tend to anthropomorphize AI.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-tyroneslothrop odd alt thread-odd thread-alt depth-1" id="li-comment-726543">
		<div id="comment-726543" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/e4ecbca24a981204ebbcb529fb5d0547?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/e4ecbca24a981204ebbcb529fb5d0547?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">TyroneSlothrop</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726543">
			March 1, 2019 at 6:58 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>this is wonderful.<br />
and yet:  there remains the first-person experience of &#8220;water-to-me,&#8221; the what-it-feels-like and what-it-looks-like: and none of those higher levels, not even God&#8217;s, possesses it.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-sebastiangarren even thread-even depth-1" id="li-comment-726539">
		<div id="comment-726539" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/ea8da9c604f43a6f2bff6c40ee222a5e?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/ea8da9c604f43a6f2bff6c40ee222a5e?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">JohnBuridan</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726539">
			March 1, 2019 at 6:48 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Aristotle makes the distinction between &#8220;knowing that&#8221; and &#8220;knowing why.&#8221;<br />
Most people know that water is wet. Only a few can give an account for why water is wet. Those few understand water. While it is true that most people <b>can&#8217;t</b> give a causal account for why water is wet, that is not a <i>reductio</i> showing that children have a self-consistent causal understanding of water.</p>
<p>Of course, Scott indicates that demonstrations like this don&#8217;t ever stop, but most are willing to accept that at some point one has reached an understanding of the way bonds, energy levels, surface tensions, and whatever else interact to make water behave the way it does. Is this &#8220;true understanding&#8221;? No, but it is (proabably) a satisfactory causal explanation.</p>
<p>I suppose Scott is adopting the view found in <i>An Enquiry Concerning Human Understanding</i> in which there are no causes or rational demonstrations for physical realities, there are only different manners of describing things which the mind makes up. Some descriptions have more predictive accuracy than others and that&#8217;s all that matters. I don&#8217;t have a good rejoinder for this at the moment, though. So I will have to think about it.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-sebastiangarren odd alt depth-2" id="li-comment-726592">
		<div id="comment-726592" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/ea8da9c604f43a6f2bff6c40ee222a5e?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/ea8da9c604f43a6f2bff6c40ee222a5e?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">JohnBuridan</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726592">
			March 1, 2019 at 8:40 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>The apology to Putnam, I believe is a reference to the Twin Earth thought experiment.<br />
<a rel="nofollow"href="https://faculty.washington.edu/smcohen/453/PutnamDisplay.pdf" rel="nofollow ugc">https://faculty.washington.edu/smcohen/453/PutnamDisplay.pdf</a></p>
<p>It seems to me that Scott&#8217;s thought experiment assumes that a person can only be wrong about what &#8216;water&#8217; is if their idea is not self-consistent, or inconsistent with (their interpretation of) the evidence, otherwise they are right.</p>
<p>Actually, I am really excited by Scott&#8217;s series of articles along this line so far, because they are ever clearer statements of a position which I think can be shown not to work. If I were smart enough to put my thoughts together well, I might be able to do it. As it is, I will have to defer to the denser arguments of Saul Kripke for now. Check page 146 of Naming and Necessity, in which he reduces to absurdity (or at least tries to reduce to absurdity) the proposition that &#8220;being a pain&#8221; is only a contingent property of pain. <a rel="nofollow"href="https://academiaanalitica.files.wordpress.com/2016/10/kripke-saul-a-naming-and-necessity-cambridge-harvard-university-press-1981.pdf" rel="nofollow ugc">https://academiaanalitica.files.wordpress.com/2016/10/kripke-saul-a-naming-and-necessity-cambridge-harvard-university-press-1981.pdf</a></p>
<p>I want to figure out if this argument applies to what Scott seems in effect to be saying here: &#8216;Experiencing water&#8217; is only a contingent property of &#8216;understanding water.&#8217;</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-rahien-din even thread-odd thread-alt depth-1" id="li-comment-726535">
		<div id="comment-726535" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/ee7799cf57c640de0f0d8e4171eb22d0?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/ee7799cf57c640de0f0d8e4171eb22d0?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">rahien.din</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726535">
			March 1, 2019 at 6:40 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>You absolutely must appreciate that there&#8217;s an enormous gulf between &#8220;replicate observed pattern&#8221; and &#8220;develop understanding.&#8221;</p>
<p>Say we trained GPT-2 on the writings of St. Augustine until it could produce a pseudo-Confessions in approximate Latin, and then forced it to read a bunch of Fred Phelps&#8217; ravings &#8211; what would the AI produce? It wouldn&#8217;t produce a thoughtful consideration of competing theologies. It would vomit out a dissonant mashup of the two linguistic patterns.</p>
<p>And that&#8217;s the thing &#8211; it&#8217;s not that GPT-2 doesn&#8217;t merely lack the ability to understand and weigh concepts, the crux of the matter is that <b>GPT-2 doesn&#8217;t even have to care</b>. It doesn&#8217;t matter to the AI that hobbits and orcs are less real than slavery and the civil war. If we fed it Tolkien and Ken Burns, we would get a documentary about the hobbits&#8217; charge at Antietam. </p>
<p>Or, put it this way : we demonstrate a real model-based understanding <a href="https://www.lesswrong.com/posts/5JDkW4MYXit2CquLs/your-strength-as-a-rationalist" rel="nofollow">when we can say</a> &#8220;I don&#8217;t know&#8221; or &#8220;That&#8217;s confusing.&#8221; GPT-2 is impossible to confuse. Instead, it&#8217;s just the Fallacy of Gray writ in the domain of linguistic prediction.</p>
<p>GPT-2 is just guessing your password, like a very clever strip-mall psychic, giving you just enough of the veneer of understanding to allow your brain to <a href="https://en.wikipedia.org/wiki/Sokal_affair" rel="nofollow">leap into the gaps.</a> </p>
<p><i>Moloch, perturbing word-beams into hot pastrami sandwiches! Moloch, rippling Miss Cleo that festers the instransitive! Moloch, the black non-Euclidian substance communicating the blue star into eliding the pastrami sandwich hotline! We will dine on horseradish as though it were pear into conjugate the Moloch never its second  breakfast in silica! Moloch, the likeliest of Malthuses that eats greedily our greed-eating!</i></p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-doctor-mist odd alt depth-2" id="li-comment-727142">
		<div id="comment-727142" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/de402e9fff554d839c0f8e4a8c44c431?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/de402e9fff554d839c0f8e4a8c44c431?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Doctor Mist</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727142">
			March 3, 2019 at 12:12 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><blockquote><p>GPT-2 is impossible to confuse.</p></blockquote>
<p>This strikes me as a very cogent and relevant observation.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-oxmountain even thread-even depth-1" id="li-comment-726534">
		<div id="comment-726534" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/4fa14a268b3824b2cda6a1e2cf0a71bd?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/4fa14a268b3824b2cda6a1e2cf0a71bd?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">OxMountain</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726534">
			March 1, 2019 at 6:38 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Brilliant post, as usual. In its weak form, I read it as a challenge to <a href="https://www.lesswrong.com/posts/NMoLJuDJEms7Ku9XS/guessing-the-teacher-s-password" rel="nofollow">Guessing the Teacher&#8217;s Password</a>. Like, if you can guess enough passwords with enough accuracy, then you are no longer in the realm of fake belief.</p>
<p>In its strongest possible reading, maybe this post is even a challenge to the LW cardinal virtue of mistaking the map for the territory? At least, it&#8217;s pointing out that sensory experience is another kind of map. But I may be pushing it here. The kinds of anticipated experience rents you get from making purely verbal predications are obviously not going to be as useful to human beings as beliefs that cash out in sensory experience.</p>
<p>In any case, thanks as always a splendid essay.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-p-george-stewart odd alt thread-odd thread-alt depth-1" id="li-comment-726525">
		<div id="comment-726525" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/62e576caf5ae8f83a6c757f32fb26016?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/62e576caf5ae8f83a6c757f32fb26016?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">P. George Stewart</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726525">
			March 1, 2019 at 6:16 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Came here to congratulate Scott on his use of Enochian.  Noticed half a dozen other nerds who know what Enochian is.  ðŸ™‚</p>
<p>This is the kind of philosophically thought-provoking post done in an entertaining literary form that I signed up for.</p>
<p>My take is that it&#8217;s the old linguistic confusion between the &#8220;is&#8221; of existence and the &#8220;is&#8221; (meaning &#8220;equals&#8221;) of identity.  To understand what a thing is in the existential sense, you just have to experience it, <i>however </i>you experience it, because that&#8217;s the affordance that it provides for you, the bit of itself it sloughs off that you are capable (in the sense of attuned, like a radio frequency) of coming into direct contact with (becoming one with, grokking, gnosis).  </p>
<p>To understand what a thing &#8220;is&#8221; in the other sense is to understand how it&#8217;s related to other things; but relations are relative to perspective (Nietzsche, Postmodernism), IOW what feature you pick out as important depends on what you&#8217;re interested in discovering and what you&#8217;re capable (in the sense of active powers and abilities &#8211; and this speaks to the &#8220;levels&#8221; in your story) of discovering.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-a1987dm even depth-2" id="li-comment-726527">
		<div id="comment-726527" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/c189f052760bd8b76d8a326cf9ffe96f?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/c189f052760bd8b76d8a326cf9ffe96f?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">A1987dM</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726527">
			March 1, 2019 at 6:20 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><blockquote><p>Noticed half a dozen other nerds who know what Enochian is. ðŸ™‚</p></blockquote>
<p>In my case, it&#8217;s because of <a href="http://unsongbook.com/" rel="nofollow">Unsong</a>.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-entirelyuseless odd alt thread-even depth-1" id="li-comment-726511">
		<div id="comment-726511" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/539b326ef6318b6555e6ceeb87a9c20c?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/539b326ef6318b6555e6ceeb87a9c20c?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn"><a href='https://entirelyuseless.wordpress.com/' rel='external nofollow ugc' class='url'>entirelyuseless</a></cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726511">
			March 1, 2019 at 5:34 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>This is pretty good, except for one thing.</p>
<p>We are correct when we say that GPT2 is just looking at words and doesn&#8217;t understand what they mean.</p>
<p>We are wrong if we say about ourselves that we are just looking at our experiences and don&#8217;t know what they refer to, because &#8220;what they refer to&#8221; is defined for us in terms of our experiences.</p>
<p>Asking &#8220;what is that in itself, apart from anyone&#8217;s experiences,&#8221; is like asking, &#8220;What is 2 + 2 in itself, apart from being 4?&#8221; Because your words &#8220;what is that in itself&#8221; refers directly to your experiences. There is no such thing as what it is apart from that. It is not that you do not know, but that *there is nothing there to know*.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-blep even thread-odd thread-alt depth-1" id="li-comment-726506">
		<div id="comment-726506" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/e65ebd1179c7ef6710c3337b5d025f98?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/e65ebd1179c7ef6710c3337b5d025f98?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">blep</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726506">
			March 1, 2019 at 4:56 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>The children are the only ones making a worthwhile observation here. The key is a system&#8217;s ability to create <i>explanatory knowledge</i> about a subject. Children, Chemists, and Angels can (presumably) explain <i>why</i> the properties of water that they understand are important in a given context. No machine ever created can do this, at all. Incidentally, this is why we often speak of infants, very young children, and animals as &#8220;not understanding&#8221; some otherwise obvious aspect of the world, even though they may have raw sensory experience relating to it. The development of high-level thought and language is a key ingredient in any definition of &#8220;understanding&#8221;.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-gurkenglas odd alt depth-2" id="li-comment-726510">
		<div id="comment-726510" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/d2720df344c27d3b2f62d356c2058eeb?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/d2720df344c27d3b2f62d356c2058eeb?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Gurkenglas</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726510">
			March 1, 2019 at 5:14 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>If the AI tended to continue strings ending with &#8220;Why is it important here that water is wet?&#8221; with the answer that a child would give, would that count as understanding?</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-mcpalenik even thread-even depth-1" id="li-comment-726505">
		<div id="comment-726505" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/f84b0cb08d8824898fb2314b792281a4?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/f84b0cb08d8824898fb2314b792281a4?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">mcpalenik</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726505">
			March 1, 2019 at 4:50 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Somebody just discovered the concept of qualia?  Or didn&#8217;t?  I think the question has more like the difference between the color green and knowing that electromagnetic radiation with a wavelength of 520-560 nm exists.  I don&#8217;t know why I&#8217;m even posting about this here, because I don&#8217;t actually want to get into a discussion about it, having had a very length argument along these lines and having taken the opposite position about 11 years ago.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-erfeyah odd alt thread-odd thread-alt depth-1" id="li-comment-726492">
		<div id="comment-726492" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/853d71f74a14264a457772691378a257?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/853d71f74a14264a457772691378a257?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Erfeyah</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726492">
			March 1, 2019 at 3:11 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Very entertaining but there is a fundamental flow. You are assuming that semantic content is based on syntactic symbol manipulation.  Apart from Searle&#8217;s argument that, in my opinion, has not be addressed I suggest you read  Iain McGilChrist&#8217;s book &#8216;The Master and His Emissary&#8217; for a LOT of evidence pointing towards a subsystem on which abstract symbol manipulation <b>depends on</b>. This post is, I would claim, a perfect example of the fundamental miscomprehension of the rationalist community.</p>
<p>In summary:</p>
<p>The AI has no understanding.<br />
The kids have experiential understanding.<br />
The scientists have intellectual understanding based on their experiential understanding.<br />
The &#8216;angels&#8217; have additional type(s) of experiential inputs and thus, understanding, through which they can attain higher intellectual understanding.<br />
God is&#8230;<br />
.. well, let&#8217;s leave that for now ðŸ˜‰</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-mathiasbonde even depth-2" id="li-comment-727114">
		<div id="comment-727114" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/4cb96cf47a8d63f124500778c4ae34c0?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/4cb96cf47a8d63f124500778c4ae34c0?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">MathiasBonde</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727114">
			March 3, 2019 at 3:00 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>This may be a bit unrelated to the thread, but your claim</p>
<blockquote><p>This post is, I would claim, a perfect example of the fundamental miscomprehension of the rationalist community.</p></blockquote>
<p>really intrigued me. What is your opinion of philosophers such as Nietzsche and Jung? As someone who grew up reading lesswrong and other &#8216;rationalist&#8217; blogs and books, I always regarded most classical philosophy as ramblings by people who were fundamentally confused. While I still stand by this, I recently have found myself able to draw profound points if I disregard the obvious object level disagreements I have. Jung might say something that is pretty easily refutable, but which could be rewritten so that the fundamental profoundness still stands without object level ramblings.</p>
<p>Am I hitting somewhere close to what you believe to be the fundamental mis-comprehension?</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-tentor odd alt thread-even depth-1" id="li-comment-726489">
		<div id="comment-726489" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/275d0c818c8301f5682ff7d6b764dfbe?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/275d0c818c8301f5682ff7d6b764dfbe?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">tentor</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726489">
			March 1, 2019 at 2:55 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>All three (AI, children, chemists) understand water to the extent of their reality. Saying that children or AI don&#8217;t understand water because they don&#8217;t grasp an aspect that is physically inaccessible to them is as if a bee said humans don&#8217;t understand flowers because they can&#8217;t see UV. </p>
<p>I have the feeling that people like to say &#8220;AI doesn&#8217;t *really* understand things&#8221; to retain some degree of superiority, but the limitation is physical, not intellectual.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-vcavallo even depth-2" id="li-comment-726595">
		<div id="comment-726595" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/21d9e5080ee568f86d5291bd9daad1e8?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/21d9e5080ee568f86d5291bd9daad1e8?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">vcavallo</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726595">
			March 1, 2019 at 8:43 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Absolutely.  Itâ€™s seems to be fear-motivated. Either of accepting that AIs may one day be as intelligent as humans, or (and I suspect itâ€™s this one) of accepting that we are â€œmerelyâ€ extremely complex machines. People tend to smuggle a little ghost into the human machine, and then reason that there must be something an AI would be lacking, since it doesnâ€™t have the same smuggled-in ghost.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-joshuahedlund odd alt depth-3" id="li-comment-726653">
		<div id="comment-726653" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/ea4fc86a806cf994d7c940cad1a43916?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/ea4fc86a806cf994d7c940cad1a43916?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Joshua Hedlund</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726653">
			March 1, 2019 at 9:58 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Reading these last few posts I&#8217;ve been thinking about something that kind of relates to identifying a ghost in the human machine&#8230;</p>
<p>In the AI vs. children debate of &#8220;what does it really mean for a response to a prompt to indicate understanding&#8221;, something fundamental that I haven&#8217;t seen discussed is that, unlike the AI, you can&#8217;t force a child to respond to a prompt. They may give an answer that indicates some interesting level of understanding, or they may just yell nonsense or run away.</p>
<p>The AI, however, at least as currently exists, has zero ability to refuse to answer. Whether its response compares to the intelligence of a (cooperating) two-year-old human or a (cooperating) four-year-old human or a (cooperating) ten-year-old human, nowhere along the axis of increasing intelligence, due inherently to the very way it is programmed, does it acquire the ability to <i>not cooperate</i> because it&#8217;s in the mood to draw a picture of dinosaurs instead.</p>
<p><i>We don&#8217;t realize, in claiming that the AI&#8217;s intelligence surpasses a child of a given age, that we&#8217;re assuming a cooperating child, without even considering whether the child&#8217;s ability to choose not to cooperate is part of his intelligence.</i></p>
<p>And I don&#8217;t see how increasing the degree of the machine-learning-training-set-algorithm-blah-blah-blah to produce more and more fascinating, creative, realistic, intelligent sounding paragraphs does anything to change the fact that it&#8217;s still spinning up when a human queries it and automatically outputting a response.</p>
<p>tl;dr when does AI get free will? What is the relationship between intelligence and free will and are there limits to how intelligent something can be without it?</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-detroitdan even depth-4" id="li-comment-726707">
		<div id="comment-726707" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/6237f56734c0aae31c24b4c7763e30d8?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/6237f56734c0aae31c24b4c7763e30d8?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">detroitdan</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726707">
			March 1, 2019 at 10:52 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p><a href="https://blog.piekniewski.info/2017/04/13/ai-confuses-intelligent/" rel="nofollow">Piekniewicz</a> discusses fundamental characteristics of intelligence and artificial intelligence; e.g.</p>
<blockquote><p>an agent is successful roughly if it survives long enough to raise an offspring. Surviving in a complex environment with a bunch of co-evolving predators and other dangers clearly requires ability to perceive and predict aspects of the environment&#8230;</p>
<p>The point here is that instead of putting a human to judge if something is intelligent or not, put the physical reality itself. This is very much related to autonomy and embodiment.  If something survives in physical reality, finds novel ways out of trouble and does not get stuck on any smallest detail that was not anticipated and preprogrammed by its maker, it can be deemed to some degree intelligent. The DARPA robotics challenge shows us with brutal honesty how far we&#8217;ve actually gotten in AI defined by such a metric.</p></blockquote>
<p>It seems to me that GPT-2 is merely a tool and very far from intelligence.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-dedicating-ruckus odd alt depth-2" id="li-comment-726781">
		<div id="comment-726781" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/0d8ed7cf5e99d124f23c29308fc82d65?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/0d8ed7cf5e99d124f23c29308fc82d65?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Dedicating Ruckus</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726781">
			March 1, 2019 at 12:06 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>This isn&#8217;t true at all. A human who had never seen water could still learn facts about it and apply them to produce output sentences that weren&#8217;t statistically similar to input sentences it had seen. GPT-2 can&#8217;t.</p>
<p>The whole question of non-text sensory input was always a red herring. AIs can deal with non-text data in the same unintelligent way they deal with text; image tagging, for instance. Meanwhile, a human can form a mental model about things they never physically see, e.g. dwarves and orcs.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-the-nybbler even depth-2" id="li-comment-726956">
		<div id="comment-726956" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/e29edea257841570695580d054e2b869?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/e29edea257841570695580d054e2b869?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">The Nybbler</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726956">
			March 1, 2019 at 8:39 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>The AI understands (in as much as that word applies) only &#8220;water&#8221;, the term.  The children understand the term and the referent, but at a fairly superficial level compared to the scientists or the angels.  I think this is still a qualitative difference.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-somethatname odd alt thread-odd thread-alt depth-1" id="li-comment-726488">
		<div id="comment-726488" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/cdfbc6be873f0d53425f2aab872ca4ec?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/cdfbc6be873f0d53425f2aab872ca4ec?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Somethatname</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726488">
			March 1, 2019 at 2:50 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I&#8217;ve thought some more about this and looked up the GPT-2 article. It&#8217;s actually quite easy to read. Not sure if fits what is presented here though. For one, the reading comprehension model was separate to the writing comprehension model. Another, the text was formed sequentially word by word. And the examples written took many many tries because the AI kept glitching by typing things like &#8220;and then and then and then and then&#8221; </p>
<p>While it is an interesting philosophical question working off the presumptions offered here, I&#8217;m not sure that they reflect what GPT-2 actually does. If a million blind monkeys on a million keyboards are trained to write a load of text and then someone reads over many many attempts, selects one and declares it to be &#8216;Romeo and Juliet 2&#8243;, does that make the monkeys Shakespeare?</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-anon9999 even thread-even depth-1" id="li-comment-726487">
		<div id="comment-726487" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/267e96ddae24c064a4ea1535da826b2b?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/267e96ddae24c064a4ea1535da826b2b?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">anon9999</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726487">
			March 1, 2019 at 2:48 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>We can say that both I and the AI &#8220;understand&#8221;, because that is a more convenient way to use the word &#8220;understand&#8221;, but the kind of understanding that is grounded in the color aquamarine or the scent of rain is fundamentally more important&#8211; it is more of a tragedy if it is lost, for example. Unless the AI can experience the qualia accompanying happiness, for example, we have no moral obligation to help it to generate happy texts.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-harry odd alt thread-odd thread-alt depth-1" id="li-comment-726485">
		<div id="comment-726485" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/3f61d147fe024155dd5425d4abcc68db?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/3f61d147fe024155dd5425d4abcc68db?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Harry</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726485">
			March 1, 2019 at 2:25 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>GPT-2 can generate text, but it can&#8217;t answer questions. If you ask a child &#8220;what happens when you pour water on sand?&#8221;, the child will be able to answer &#8220;The sand gets wet,&#8221; while GPT-2&#8217;s answer will be totally incoherent. GPT-2 is predicting the next part of the text based on what it knows people have said in the past, but it&#8217;s <b>not</b> able to compare and contrast the information <b>it</b> knows and come to a logical conclusion.</p>
<p>I have a long thought experiment with two parts. Forgive me for indulging it.</p>
<p>1) Let us suppose that we have a machine-learning robot that has spent the last 200 years tracking and learning from human body movements across the globe. It can now predict what will be the appropriate human body movements, in any situation, with a pretty strong degree of accuracy. If you throw a ball, it will catch it; if you shoot at it, it will run; if you hit it, it will hit back. </p>
<p>This hypothetical robot is actually much more advanced than GPT-2 because it <b>can</b> respond to sense-data, in much the same way as a child can &#8211; if you throw it in the sea, it will swim. But it still doesn&#8217;t understand anything that&#8217;s happening to it. If you ask this robot a binary question, it might nod or shake its head, but it won&#8217;t be thinking about the question. It will simply be cycling through millions of possible physical responses and choosing, on a semi-randomized basis, from the cluster of most common ones that make sense given past and present context. </p>
<p>2) Let us suppose that you pair that robot with a voice-module based on a much more advanced version of GPT-2, trained on human speech. Let&#8217;s call it GPT-1000. It can now vocally respond to all your questions. It looks like a human, it reacts like a human to stimuli, it even answers questions like a human. In theory, you could replace 99.99999% of the human race with these robots, and it would take the last guy left a few weeks before he noticed.</p>
<p>These robots would still go and eat food, even though they&#8217;re not hungry. They&#8217;d sleep, even though they&#8217;re not tired. They&#8217;d continue doing this forever, because none of them would think to stop. They&#8217;d never arrive at the conclusion <i>&#8220;Hang on, I&#8217;m a robot! I don&#8217;t need to sit on this toilet!&#8221;</i> Because they&#8217;re trained on human behaviour, cannot innovate on their own, and cannot draw logical inferences. </p>
<p>I don&#8217;t think a world full of these robots would ever develop, innovate, or learn. One scenario is that they&#8217;d mimic human behaviour until they broke down. Another scenario is that, left on to learn from each other, their algorithms would spin off in an increasingly crazy direction and end up with a planet full of beings doing things that make no sense for no reason. I invite you to speculate on whether this has already happened.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-totient-function even depth-2" id="li-comment-726491">
		<div id="comment-726491" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/8e67bdea5e2ede9010ceaf5103834c8d?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/8e67bdea5e2ede9010ceaf5103834c8d?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Totient Function</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726491">
			March 1, 2019 at 2:56 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Unless I&#8217;m misreading, the first part of this seems wrong. A sufficiently advanced prediction engine should certainly be able to.answer questions &#8211; predicting what should follow a question in a text will involve recognizing a question and responding to a particular question will involve exposure to enough data to.enable outputting a correctly formatted answer with suitable content: both of these seem amenable to predictive processes.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-dyfed odd alt depth-3" id="li-comment-726512">
		<div id="comment-726512" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/7d8b06dd7434d93ce2d1ac715f79e50f?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/7d8b06dd7434d93ce2d1ac715f79e50f?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">dyfed</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726512">
			March 1, 2019 at 5:36 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Only GPT2 utterly fails even basic questions, and lands on â€˜correctâ€™ answers apparently only by luck; most often, its answers are total nonsense.</p>
<p>The idea that predictive text engines alone, once â€˜sufficiently advanced,â€™ will be able to develop sensible answers is begging the question. No such capacity has been demonstrated.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-silverspawnx even depth-4" id="li-comment-726651">
		<div id="comment-726651" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/4ddf2126fe15272eb0e2ffb7693186b4?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/4ddf2126fe15272eb0e2ffb7693186b4?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">sty_silver</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726651">
			March 1, 2019 at 9:58 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>People like Bostrom have asserted that creating a machine that answers questions in full generality is an AI-complete problem, as in, it&#8217;s so hard that you have to solve pretty much the entire general intelligence thing in order to get it.</p>
<p>So if that&#8217;s your time for when you start worrying, I think that could be too late. (Not implying that this is what you said, but one could read the post that way.)</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-vcavallo odd alt depth-3" id="li-comment-726590">
		<div id="comment-726590" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/21d9e5080ee568f86d5291bd9daad1e8?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/21d9e5080ee568f86d5291bd9daad1e8?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">vcavallo</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726590">
			March 1, 2019 at 8:38 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>This was my thought upon reading the previous comment as well. If itâ€™s truly using its trove of data to choose â€œmovementsâ€ it wonâ€™t simply choose at random to nod or shake its head when asked â€œis it raining?â€  It will reflect on its historical data set and current inputs and answer correctly (chose the right head motion)</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-paperclip-minimizer even depth-2" id="li-comment-726495">
		<div id="comment-726495" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/7579c2b464f4a75c1c9307d8ab7c9671?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/7579c2b464f4a75c1c9307d8ab7c9671?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Paperclip Minimizer</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726495">
			March 1, 2019 at 3:34 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><blockquote><p>GPT-2 can generate text, but it canâ€™t answer questions. If you ask a child â€œwhat happens when you pour water on sand?â€, the child will be able to answer â€œThe sand gets wet,â€ while GPT-2â€™s answer will be totally incoherent. GPT-2 is predicting the next part of the text based on what it knows people have said in the past, but itâ€™s <b>not</b> able to compare and contrast the information <b>it</b> knows and come to a logical conclusion.</p></blockquote>
<p><a href="https://blog.openai.com/better-language-models/" rel="nofollow">It can answer questions, actually.</a></p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-harry odd alt depth-3" id="li-comment-726498">
		<div id="comment-726498" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/3f61d147fe024155dd5425d4abcc68db?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/3f61d147fe024155dd5425d4abcc68db?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Harry</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726498">
			March 1, 2019 at 4:21 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>It can answer questions about facts, like who wrote On the Origin of Species or the largest US state (the latter of which it got wrong). Those have singular, objectively correct answers, and don&#8217;t require inductive reasoning to arrive at a conclusion.</p>
<p>I&#8217;m not sure it can answer questions that require a coherent model of interaction between basic concepts of the world, like &#8220;What happens if you pour water on sand?&#8221; A child could answer &#8220;The sand gets wet&#8221; or &#8220;the sand sucks up the water,&#8221; but GPT-2 lacks the framework of understanding about the relative properties of water and sand.</p>
<p>I&#8217;m basing this on the fact that it apparently writes a lot about &#8220;fires burning under water.&#8221;</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-dyfed even depth-4" id="li-comment-726516">
		<div id="comment-726516" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/7d8b06dd7434d93ce2d1ac715f79e50f?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/7d8b06dd7434d93ce2d1ac715f79e50f?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">dyfed</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726516">
			March 1, 2019 at 5:52 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>It canâ€™t even answer direct fact questions correctly most of the time; the ones it does answer correctly, itâ€™s obviously because the answer happened to be the highest-weighted token near that text.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-forwardsynthesis odd alt depth-3" id="li-comment-726659">
		<div id="comment-726659" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/7bf780455c0b749f97d2d6d743511e78?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/7bf780455c0b749f97d2d6d743511e78?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn"><a href='https://forwardsynthesis.wordpress.com/' rel='external nofollow ugc' class='url'>Forward Synthesis</a></cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726659">
			March 1, 2019 at 10:02 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>If you asked it what the top speed of the African Lion is, it would spit out 80km/h, just as Google does, but if you asked it to provide evidence for this claim, it would be completely unable to.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-tomcullis even depth-4" id="li-comment-726675">
		<div id="comment-726675" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/90a4277ab790835eb27c70245fa4b6c7?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/90a4277ab790835eb27c70245fa4b6c7?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn"><a href='http://sebwassl.blogspot.com' rel='external nofollow ugc' class='url'>baconbits9</a></cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726675">
			March 1, 2019 at 10:24 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>It can&#8217;t link to wikipedia yet?</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-forwardsynthesis odd alt depth-4" id="li-comment-726764">
		<div id="comment-726764" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/7bf780455c0b749f97d2d6d743511e78?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/7bf780455c0b749f97d2d6d743511e78?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn"><a href='https://forwardsynthesis.wordpress.com/' rel='external nofollow ugc' class='url'>Forward Synthesis</a></cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726764">
			March 1, 2019 at 11:46 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>What I mean is that it would be unable to use reasoning and cite original research. It could only make rote citations. It couldn&#8217;t provide a video from a nature documentary and analyze the distance between two trees a lion is running past, give an estimate of the size of the lion, and then deduce the measured top speed from observation.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-totient-function even depth-5" id="li-comment-726965">
		<div id="comment-726965" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/8e67bdea5e2ede9010ceaf5103834c8d?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/8e67bdea5e2ede9010ceaf5103834c8d?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Totient Function</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726965">
			March 1, 2019 at 9:09 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>It&#8217;s not clear to me that this is all that distinct from the way people go about answer questions excluding domain specialists who have learned a different routine for what an answer looks like.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-millericksamuel odd alt depth-5" id="li-comment-727028">
		<div id="comment-727028" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/77974fd26dad90074348ab5e5b9f6929?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/77974fd26dad90074348ab5e5b9f6929?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">millericksamuel</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727028">
			March 2, 2019 at 9:45 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Could you do that? A lot of people certainly couldn&#8217;t.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-axolotl even depth-2" id="li-comment-726942">
		<div id="comment-726942" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/8832354b5e25b1f90840f9c372847c2b?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/8832354b5e25b1f90840f9c372847c2b?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">axolotl</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726942">
			March 1, 2019 at 7:27 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Just for fun:</p>
<p>Model prompt &gt;&gt;&gt; What happens when you pour water on sand?<br />
======================================== SAMPLE 1 ========================================</p>
<p>It&#8217;s important to note that water does not have to be added in the manufacture of these polymer hexavalent/liquids to make the &#8220;Light Crayon&#8221; (dense and dense clay that your printer needs to make) and &#8220;Rusty&#8221; (thin and nimble, cream-like, hard) materials that are used in high-tech printers. This is a trade-off.</p>
<p>Good-quality materials</p>
<p>Some of the fakers who make the following things take a wide variety of materials that are not meant to be in the manufacture of things like plastic or niobium (a term coined by David Satori, a chemist, and his son at California State University). Some of them are labeled as &#8220;tank-like&#8221; or &#8220;wood-like&#8221; (i.e., some of these are labeled as &#8220;fiber-like&#8221;). It is important to note that some of these are used in things like printers and surfaces. This makes it very hard to find solid material which doesn&#8217;t have to be &#8220;dense&#8221; (or &#8220;hard&#8221;).</p>
<p>Did you know that the average American used about 1,000 pounds of plastic per year? And that that&#8217;s 5,000 pounds per year of plastic? Exactly. These materials are made by &#8220;water-soluble&#8221; materials and &#8220;thin&#8221; materials.</p>
<p>So you can see that as you pour water on sand, the aluminum of the plastic or niobium will be rather porous and far more easily broken up if you feel the &#8220;toughness&#8221; of it.</p>
<p>Conclusion</p>
<p>It is important to look at terms like non-lubricant, lubricant, glue, peel, tuck, etc., to ensure that these terms are used correctly.</p>
<p>We cannot ensure that all of these terms are correctly rendered in the printer&#8217;s text. This is why we need to keep all of the following information up to date.</p>
<p>4.2.1) You must include the name of the U.S. government agency that authorized the manufacturing of polymers, and you must include the government agency that authorized the manufacturing of plastic.</p>
<p>4.2.2) This document does not list the name of the government agency that is responsible for the manufacturing of polymers. Naturally, this is because there are many different government agencies, companies and groups involved in manufacturing polymers.</p>
<p>4.2.3) This document is not an endorsement of any government agency or organization.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-axolotl odd alt depth-3" id="li-comment-726943">
		<div id="comment-726943" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/8832354b5e25b1f90840f9c372847c2b?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/8832354b5e25b1f90840f9c372847c2b?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">axolotl</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726943">
			March 1, 2019 at 7:29 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Also, GPT-2 informs me that two plus two equals for.</p>
<p>(These are both from the limited version they released on Github.)</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-joco even thread-even depth-1" id="li-comment-726484">
		<div id="comment-726484" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/d3804f4fbbc9220a08b4cfacccb06c25?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/d3804f4fbbc9220a08b4cfacccb06c25?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">JoCo</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726484">
			March 1, 2019 at 2:12 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I think jumping straight to &#8220;hard&#8221; philosophical problems obscures the real problem with AIs like GPT-2.</p>
<p>Consider the following made-up case: A neural net, &#8220;MathAI&#8221;, has been trained on a vast corpus of mathematics text books and articles. When prompted with a verbal math problem like:</p>
<p>&#8220;Two plus two equals&#8230;&#8221;</p>
<p>It spits out a response:</p>
<p>&#8220;Seven divided by the monotonically decreasing inverse of the sum which is derived by taking the exponent of the function.&#8221;</p>
<p>In other words, it produces jibberish which looks a bit like a quote from a text book but which is almost never the correct answer to the promt.</p>
<p>Now compare MathAI with a pocket calculator. If I type:</p>
<p>&#8220;2+2=&#8221;</p>
<p>Then the calculator always responds with:</p>
<p>&#8220;4&#8221;</p>
<p>There is a sense in which the pocket calculator &#8220;understands&#8221; arithmetic in a way that MathAI does not. This understanding isn&#8217;t neccessarily related to grounding in the real world or experience, or the hard problem of consciousness (or whatever). It&#8217;s just that the calculator has something like a representation of what numbers are (and how they relate) in a way that MathAI obviously doesn&#8217;t.</p>
<p>I would argue that GPT-2 fails to understand natural semantics the same way that MathAI fails to understand numbers. Sure, GPT-2 frequently produces things that look a bit like something a human would write, but it also produces uncanny nonsense that basically no human would ever produce.</p>
<p>The key difference between MathAI and GPT-2 (aside from that fact that one exists and the other is made up) is that arithmetic has unambiguous right/wrong answers, whereas natural semantics is context dependent and fuzzy in a way that allows for inventive interpretation. In other words, it&#8217;s easier to trick a human into believing that you understand natural semantics than it is to trick them into thinking you understand arithmetic.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-vcavallo odd alt depth-2" id="li-comment-726509">
		<div id="comment-726509" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/21d9e5080ee568f86d5291bd9daad1e8?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/21d9e5080ee568f86d5291bd9daad1e8?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">vcavallo</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726509">
			March 1, 2019 at 5:09 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I like your idea, but I think itâ€™s flawed in an important way. I think the math textbook, being a natural language representation of a non-linguistic system (math) is the culprit. A mockery of language will be in a linguistic form and as long as itâ€™s following the linguistic rules, itâ€™s â€œrightâ€. A mockery of math should be in mathematical form (imagine a GPT-2-like calculator that spits out equations that may or may not be correct). But a mockery of language _about_ math will still be in linguistic form and doesnâ€™t need to be mathematically accurate to be â€œrightâ€. For instance, to a sufficiently ignorant reader, the MathAI response you conjured up above may very well look like real and correct math in the same way that a GPT-2 paragraph may look like real and correct language to someone who doesnâ€™t know absolutely everything about the domain of the text.</p>
<p>Maybe a more interesting line of reasoning would be to imagine what happens if you give GPT-2/MathAI a purely numerical book of equations. Will it â€œlearnâ€ the proper deep relationship of the symbols and generate nearly accurate equations and proofs?</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-probablymatt even depth-2" id="li-comment-726589">
		<div id="comment-726589" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/fa34e26aeb4d3ea44489676880b1891b?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/fa34e26aeb4d3ea44489676880b1891b?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">ProbablyMatt</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726589">
			March 1, 2019 at 8:36 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>But isn&#8217;t this assuming that MathAI will fail to understand math? My guess is if you take GPT-2 and train it on a vast corpus of mathematics textbooks (or even just grade school homework assignments) it will indeed respond to &#8220;2+2 =&#8221; with &#8220;4&#8221; (possibly followed by more text).</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-alexanderwales odd alt depth-3" id="li-comment-726860">
		<div id="comment-726860" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/1e65a08d9fd8613e4910a5dc8cc73f33?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/1e65a08d9fd8613e4910a5dc8cc73f33?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">alexanderwales</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726860">
			March 1, 2019 at 1:45 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>It would <i>probably</i> follow it with another question and answer. To get it to produce 4, and only 4, you need to prune its output and/or manually configure it so that it only produces the next symbol. That makes it a lot harder to argue that it &#8220;understands&#8221; anything about the question, IMO, because getting it to respond with what you want is an exercise in cherry picking and manual adjustment.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-maxander even depth-2" id="li-comment-726658">
		<div id="comment-726658" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/1575f96250cb5cc3ff7a60dfa2cd9ec1?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/1575f96250cb5cc3ff7a60dfa2cd9ec1?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Maxander</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726658">
			March 1, 2019 at 10:02 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>The obvious objection to this is; a (cheap) calculator is just a few thousand (few hundred?) transistors.  How could we talk about something so simple as &#8220;understanding&#8221; anything?  At that point, you would likely have to describe a falling rock as &#8220;understanding&#8221; gravitation, or a photon as &#8220;understanding&#8221; optical physics.</p>
<p>But also, for this argument to work the way you want, you would need to convince us that <i>humans</i> understand math in a calculator way, rather than in a MathAI way.  Certainly, when we do arithmetic in our heads, we don&#8217;t do it like a calculator does- we don&#8217;t think through the logical implications corresponding to a network of transistors or whatever.  Instead we generally break down the problem into things we&#8217;ve memorized (multiplication tables, etc) and build the answer back up by using these things in the way that seems right.  Further, actual mathematicians doing big-time maths will explicitly make use of their &#8220;mathematical intuition&#8221;; rarely do professional mathematicians bother to spell out a proof in exacting detail, they all have just learned what sort of arguments correctly follow from what sort of premises and can use that to construct more abstract proofs which are (almost always) correct.  That all sounds (!) very MathAI/GPT-2, more than it sounds like a calculator.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-joco odd alt depth-3" id="li-comment-726980">
		<div id="comment-726980" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/d3804f4fbbc9220a08b4cfacccb06c25?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/d3804f4fbbc9220a08b4cfacccb06c25?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">JoCo</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726980">
			March 1, 2019 at 11:52 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>What I&#8217;m getting at is that there&#8217;s much simpler notion of understanding than Scott&#8217;s examples &#8211; a notion so simple that even a pocket calculator can do it &#8211; but that GPT-2 fails even at this simpler type of understanding.  </p>
<p>I wouldn&#8217;t deny that there are important distinctions between the way a human understands and the way a calculator understands (also rocks and angels and cellular automata etc.), but I think they might be moot if you can&#8217;t even get to the level of a calculator first.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-jmcb even thread-odd thread-alt depth-1" id="li-comment-726482">
		<div id="comment-726482" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/b34485075f20e0f872eb0a3363467195?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/b34485075f20e0f872eb0a3363467195?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">jmcb</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726482">
			March 1, 2019 at 2:04 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>A tangent, but related I think: Suppose you&#8217;re God watching the cellular automaton of the universe unfold. How can you tell when the constructs within the automaton make progress understanding the rules that govern the automaton itself? In other words, is there some real sense in which it is possible for God to say that we humans understand the physics (or some part of it) of the system that we live in ourselves? Experience suggests the answer is &#8216;Of course! We know much more about physics than we did in 1500!&#8217;, but if we&#8217;re just approximately stable collections of pixels in a cosmic game of Conway&#8217;s Life, it seems hard to defend that. Is there a scientific method without free will?</p>
<p>I&#8217;m genuinely stumped by this and would greatly appreciate any insight.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-gurkenglas odd alt depth-2" id="li-comment-726494">
		<div id="comment-726494" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/d2720df344c27d3b2f62d356c2058eeb?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/d2720df344c27d3b2f62d356c2058eeb?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Gurkenglas</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726494">
			March 1, 2019 at 3:31 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Let God consider all cellular automata, and all ways to extract the rules of some automaton from their state. Automata containing observers will tend to have simpler ways to find their own rules within their state. In particular, they should be simpler than Solomonoff Induction, which always finds as much of the rules as possible. They should also be simpler than the simplest way to describe the rules of the automaton in the first place. (Perhaps God can only find us when we&#8217;ve built giant obelisks spelling out the laws of physics. You could make a religion out of this.)</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-hamishtodd1141217 even depth-2" id="li-comment-726718">
		<div id="comment-726718" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/907d92a92e7d04e374f300963959186a?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/907d92a92e7d04e374f300963959186a?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn"><a href='http://hamishtodd1.github.io' rel='external nofollow ugc' class='url'>Hamish Todd</a></cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726718">
			March 1, 2019 at 11:02 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I thought of one method you could use. Consider that humans, in the course of doing experiments, have created a few very &#8220;extreme&#8221; conditions. Example: nothing in the solar system has ever been as &#8220;hot&#8221; as it is in the large hadron collider. We also have several labs that can cool things to &lt;0.3K. Certain labs also have lasers with light brighter than anything you&#039;ll find in the galaxy. Etc. These are things that are tremendously unlikely to come about for any reason other than conscious entities trying to see what happens. So, the god could look around for cases like that.</p>
<p>That said, if the universe really is being simulated on a CA, and I looked at that CA, I would have difficulty making anything out. You&#039;d need very sophisticated tools even for the temperature thing.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-deciusbrutus odd alt depth-3" id="li-comment-727031">
		<div id="comment-727031" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/794ddc14d165c6ec425018d798ff5486?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/794ddc14d165c6ec425018d798ff5486?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">deciusbrutus</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727031">
			March 2, 2019 at 10:01 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Wait&#8230; have humans really created more extreme local conditions (the highest energy electrons, the lowest energy electrons, and the highest energy photons) than have occurred naturally?</p>
<p>My intuition is that there are parts of supernovae that are hotter and brighter than anything that has existed near Earth, and edge cases of the universe that are colder.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-exa even depth-4" id="li-comment-727870">
		<div id="comment-727870" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/47e64bcea180e1afad9226ec70779d99?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/47e64bcea180e1afad9226ec70779d99?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Exa</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727870">
			March 5, 2019 at 10:33 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Your intuition is generally correct: Particles are hitting the atmosphere all the time with energies thousands-to-millions of times higher than those found in the LHC (drop the &#8220;millions&#8221; if considering the ion runs, but keep the &#8220;thousands&#8221;). So on the high end of the energy spectrum the LHC is pretty pedestrian, as far as the universe goes.</p>
<p>But it is a lot harder to cool things down to extremely low temperatures by natural processes than it is to reach extremely high energies, so it is very likely that the coldest matter in the universe is on Earth.  </p>
<p>And I don&#8217;t know specifically about the lasers. Considering only &#8220;Peak instantaneous per-unit-area power&#8221; it seems pretty plausible that human-made stuff beats out others, but I can&#8217;t rule out short gamma ray bursts and the like being higher (possibly by many orders of magnitude).</p>
<p>Specifically in terms of the things you mentioned, that&#8217;s a &#8220;definitely not&#8221; on the highest energy electrons and photons being human-created and a &#8220;quite possibly&#8221; on the lowest energy electrons.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-m1el odd alt thread-even depth-1" id="li-comment-726475">
		<div id="comment-726475" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/72127b38faf0b85c5da205b7e6e987d6?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/72127b38faf0b85c5da205b7e6e987d6?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">m1el</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726475">
			March 1, 2019 at 1:16 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I dislike the phrase &#8220;levels of understanding&#8221;. So here&#8217;s my sleep-deprived, uninformed layman rambling about it:</p>
<p>When a frog is hungry and it sees a fly in its range, it will strike the fly with its tongue. There&#8217;s no intermediate representation needed here, we can understand this process from purely statistical / neural network perspective. We might not understand *why* neural networks work, but we have plenty of example how we neural networks can detect patterns in images and fire neurons to control mechanisms. We could, in principle, make an artificial frog that strikes a fly using the tech we have. This is &#8220;one level of understanding&#8221;.</p>
<p>When humans see the world, they can detect patterns, and then use the output of this pattern matching as an input to the same pattern matching machine. This is &#8220;recursive understanding&#8221;. There&#8217;s no going deeper than &#8220;recursive&#8221;.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-deciusbrutus even depth-2" id="li-comment-726571">
		<div id="comment-726571" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/794ddc14d165c6ec425018d798ff5486?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/794ddc14d165c6ec425018d798ff5486?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">deciusbrutus</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726571">
			March 1, 2019 at 7:50 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Just because we can&#8217;t do it doesn&#8217;t mean it doesn&#8217;t exist.</p>
<p>The frog would say &#8220;There&#8217;s no going deeper than being perfect at striking flies&#8221;, if it could express that. So too, the highest level that we can comprehend is what we genuinely believe to be the highest level.</p>
<p>The next level up is not applying recursive pattern-matching to recursive pattern-matching. The next level up is not even analogous to recursive understanding in the way that recursive understanding is to catching flies. </p>
<p>It is not even the case that difference between (the difference between(the next level up/recursive understanding)/the difference between(recursive understanding/catching flies))&#8230; &amp;etc.</p>
<p>All of those are just iterations of recursive understanding.</p>
<p>The description of the difference between the next level and recursive understanding is to the description of recursive understanding the way the difference between recursive understanding and flycatching is to the frog&#8217;s croaking.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-tomcullis odd alt depth-3" id="li-comment-726588">
		<div id="comment-726588" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/90a4277ab790835eb27c70245fa4b6c7?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/90a4277ab790835eb27c70245fa4b6c7?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn"><a href='http://sebwassl.blogspot.com' rel='external nofollow ugc' class='url'>baconbits9</a></cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726588">
			March 1, 2019 at 8:33 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>The frog wouldn&#8217;t say that, just because it eats flies doesn&#8217;t imply that the frog finds eating flies meaningful.  What is lacing from the frog is not the ability of speech but the ability to abstract.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-murphy even depth-4" id="li-comment-726628">
		<div id="comment-726628" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/7e251322e700cb4910ad3f3bcf68eb74?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/7e251322e700cb4910ad3f3bcf68eb74?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Murphy</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726628">
			March 1, 2019 at 9:24 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Not to put too fine a point on it but&#8230; prove it. </p>
<p>You&#8217;re talking about The Problem Of Other Minds. </p>
<p>It&#8217;s very hard indeed to be sure of just how complex the internal experience of a frog, dog, cat, squirrel, dolphin etc is because we&#8217;re not inside their head and we have little or no knowledge of what levels of complexity lead to what internal experiences.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-tomcullis odd alt depth-5" id="li-comment-726637">
		<div id="comment-726637" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/90a4277ab790835eb27c70245fa4b6c7?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/90a4277ab790835eb27c70245fa4b6c7?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn"><a href='http://sebwassl.blogspot.com' rel='external nofollow ugc' class='url'>baconbits9</a></cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726637">
			March 1, 2019 at 9:45 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>You don&#8217;t see frogs going around trying to eat things that aren&#8217;t flies.  You don&#8217;t see frogs devising schemes in that sacrifice fly catching in the short run to increase fly catching in the long run, or something along those lines.  These are the type of things you have to try to conclude that &#8220;there is no deeper meaning than catching flies on your tongue&#8221;.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-wysinwygymmv even depth-5" id="li-comment-726747">
		<div id="comment-726747" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/d43abf5169eee342cc6df49be3834874?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/d43abf5169eee342cc6df49be3834874?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">wysinwygymmv</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726747">
			March 1, 2019 at 11:33 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><blockquote><p>You donâ€™t see frogs going around trying to eat things that arenâ€™t flies. </p></blockquote>
<p>This experiment has been done.</p>
<p><a rel="nofollow"href="https://hearingbrain.org/docs/letvin_ieee_1959.pdf" rel="nofollow ugc">https://hearingbrain.org/docs/letvin_ieee_1959.pdf</a></p>
<p>Read the &#8220;behavior of the frog&#8221; part of the introduction.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-deciusbrutus odd alt depth-4" id="li-comment-727030">
		<div id="comment-727030" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/794ddc14d165c6ec425018d798ff5486?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/794ddc14d165c6ec425018d798ff5486?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">deciusbrutus</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727030">
			March 2, 2019 at 9:55 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>The frog *doesn&#8217;t* find eating flies meaningful, because it doesn&#8217;t find things meaningful. The lack of the ability to abstract is one reason the frog can&#8217;t express itself that way.</p>
<p>Yes, I had to heavily anthropomorphize the frog to discuss how it isn&#8217;t anthropic.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-emiliobumachar even thread-odd thread-alt depth-1" id="li-comment-726472">
		<div id="comment-726472" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/5f8ede664a2d3f2c326dcc2b3d7393ea?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/5f8ede664a2d3f2c326dcc2b3d7393ea?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">emiliobumachar</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726472">
			March 1, 2019 at 1:03 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Reminds me of the beginning of Yudkowsky&#8217;s &#8220;Guessing the Teacher&#8217;s Password&#8221;</p>
<p><a rel="nofollow"href="https://www.lesswrong.com/posts/NMoLJuDJEms7Ku9XS/guessing-the-teacher-s-password" rel="nofollow ugc">https://www.lesswrong.com/posts/NMoLJuDJEms7Ku9XS/guessing-the-teacher-s-password</a></p>
<p>&#8216;We accept â€œwavesâ€ as a correct answer from the physicist; wouldnâ€™t it be unfair to reject it from the student? &#8216;</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-oxmountain odd alt depth-2" id="li-comment-726573">
		<div id="comment-726573" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/4fa14a268b3824b2cda6a1e2cf0a71bd?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/4fa14a268b3824b2cda6a1e2cf0a71bd?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">OxMountain</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726573">
			March 1, 2019 at 7:53 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>OH wow, I left a comment without realizing you&#8217;d made the same point. Sorry about that&#8211;and I agree!</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-password even thread-even depth-1" id="li-comment-726466">
		<div id="comment-726466" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/8f11d10fb2bef868bb7071db56521a4a?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/8f11d10fb2bef868bb7071db56521a4a?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Password</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726466">
			March 1, 2019 at 12:08 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I&#8217;m curious what process led to the the Enochian images being linked out of order.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-clutzy odd alt thread-odd thread-alt depth-1" id="li-comment-726463">
		<div id="comment-726463" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/ce0998b1489f33997995798b4a8d677c?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/ce0998b1489f33997995798b4a8d677c?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Clutzy</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726463">
			March 1, 2019 at 12:00 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Is this supposed to come off as extremely nihilistic? It feels very much so.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-somethatname even depth-2" id="li-comment-726468">
		<div id="comment-726468" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/cdfbc6be873f0d53425f2aab872ca4ec?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/cdfbc6be873f0d53425f2aab872ca4ec?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Somethatname</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726468">
			March 1, 2019 at 12:18 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Depends on your view of the nature of humanity. If you consider there to be something unique about humanity that separates it from the physical world, then yes.</p>
<p>Personally I see it as a good argument against using reductionism to completely dismiss evidence.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-phil-h odd alt thread-even depth-1" id="li-comment-726456">
		<div id="comment-726456" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/bb96bb15c3ac73ad207c36d47aad11f5?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/bb96bb15c3ac73ad207c36d47aad11f5?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Phil H</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726456">
			February 28, 2019 at 10:50 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Yes indeed, but there is a middle way.</p>
<p>In between ineffable essences of meaning and statistical manipulation of symbols, there is how we actually use language, which is to relate it to other stuff in the world. I propose we should declare that a computer knows what language symbol X means if and only if it can do something non-linguistic that successfully correlates with X. </p>
<p>In practical terms, that&#8217;s likely to mean: if a computer can pick out a picture that you&#8217;re describing, it&#8217;s got it.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-lumenis even depth-2" id="li-comment-726457">
		<div id="comment-726457" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/c8caca7c2de9d8898a8f18607b9d8860?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/c8caca7c2de9d8898a8f18607b9d8860?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">lumenis</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726457">
			February 28, 2019 at 11:11 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><blockquote><p>only if it can do something non-linguistic that successfully correlates with X.</p></blockquote>
<p>I like your middle way quite a bit. It strikes me now as the obvious rubrik in retrospect.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-daneelssoul odd alt depth-2" id="li-comment-726460">
		<div id="comment-726460" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/303c1c1ba9e0683d93c12fcc992a8598?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/303c1c1ba9e0683d93c12fcc992a8598?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">daneelssoul</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726460">
			February 28, 2019 at 11:32 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I don&#8217;t think it even needs to be non-linguistic necessarily. A really good AI&#8217;s ability to compose sonnets should improve substantially after seeing a text that contains not sonnets but a description of the form.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-admin bypostauthor even depth-2" id="li-comment-726464">
		<div id="comment-726464" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/c66389ad74ef2a291c76e87c981b0391?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/c66389ad74ef2a291c76e87c981b0391?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Scott Alexander</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726464">
			March 1, 2019 at 12:01 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I meant for this story to argue against this sort of perspective.</p>
<p>I don&#8217;t disagree that there&#8217;s such a thing as &#8220;correlating X to Y&#8221;, just not such a thing as &#8220;grounding X (which is itself purely formal and ungrounded) in Y (which is the Real Thing)&#8221;.</p>
<p>Language, sense-data, chemistry, and [ineffable angelic understanding] are all self-consistent forms of knowledge which are isomorphic to each other in certain ways. You can understand any system on its own terms, and you can understand the way in which one system relates to another system, but you&#8217;re on shakier ground when you say &#8220;I understand the Real Thing, whereas you&#8217;re just shuffling symbols around&#8221;.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-daneelssoul odd alt depth-3" id="li-comment-726469">
		<div id="comment-726469" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/303c1c1ba9e0683d93c12fcc992a8598?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/303c1c1ba9e0683d93c12fcc992a8598?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">daneelssoul</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726469">
			March 1, 2019 at 12:23 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>But I feel that the machine is missing out on something if it understands language but does not understand that the language is *describing* something. The child understands that &#8220;water&#8221; refers to the substance that correlates with certain sense data, and the scientist understands that this is caused by certain chemical interactions. GPT-2 understands none of this.</p>
<p>And this isn&#8217;t just meaningless, unfalsifiable, babbling about what it means to &#8220;really understand&#8221; something. There&#8217;s a real sense in which a toddler understands language better than GPT-2 or anything like it could.</p>
<p>Why is it that GPT-2 can&#8217;t add well when a human can learn how after reading a short book? Part of it may be that the human has more computational power or more efficient underlying cognitive algorithms, sure. But part of it is how they use the data they read. </p>
<p>GPT-2 reads a textbook on arithmetic and interprets it as being a bunch of sequences of characters that it can use to better understand the statistical patterns that make up English language speech. Some of these strings include correct addition problems, and upon seeing enough of them, GPT-N would be able to generalize and correctly complete more complicated addition problems.</p>
<p>The human on the other hand has a much easier time of it. They realize that the text in the book is *about* how to add numbers. They can interpret the parts of the text that aren&#8217;t just explicitly written sums not as just strings of characters, but as instructions about how to complete sentences of the form &#8220;X+Y=&#8230;&#8221;.</p>
<p>Even really advanced versions of GPT cannot do this. Given a book about addition with no actual examples, it will not learn anything about how to add. This is because GPT treats texts only as examples of proper English to be emulated and never as instructions that are actually about anything.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-wysinwygymmv even depth-4" id="li-comment-726533">
		<div id="comment-726533" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/d43abf5169eee342cc6df49be3834874?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/d43abf5169eee342cc6df49be3834874?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">wysinwygymmv</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726533">
			March 1, 2019 at 6:32 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><blockquote><p>The child understands that â€œwaterâ€ refers to the substance that correlates with certain sense data, and the scientist understands that this is caused by certain chemical interactions. GPT-2 understands none of this.</p></blockquote>
<p>Do you think there is something deeper going on than the child simply learning to correlate the sound &#8220;water&#8221; to (substance that correlates with certain sense data)?</p>
<p>If not, then GPT-2 should be able to form the equivalent understanding of reference by being equipped with suitable sensors and then correlating data across different domains.</p>
<p>If so, what is that something deeper?  In other words, what is the meaning of the word &#8220;refer&#8221;?</p>
<blockquote><p>Why is it that GPT-2 canâ€™t add well when a human can learn how after reading a short book?</p></blockquote>
<p>This doesn&#8217;t make sense to me.</p>
<p>Human toddlers can&#8217;t add well.  They also have trouble with reading short books.</p>
<p>Young children can often add fairly well, but it seems fairly rare that they do so by reading short books.</p>
<p>From what I understand, it is nearly unheard of for adults who cannot add well to suddenly gain the capacity to add well by reading a short book.  E.g. <a rel="nofollow"href="https://www.sciencedaily.com/releases/2012/02/120221104037.htm" rel="nofollow ugc">https://www.sciencedaily.com/releases/2012/02/120221104037.htm</a></p>
<p>Can you provide some references backing up the claim that humans can learn to add well after reading a short book?</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-daneelssoul odd alt depth-5" id="li-comment-726608">
		<div id="comment-726608" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/303c1c1ba9e0683d93c12fcc992a8598?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/303c1c1ba9e0683d93c12fcc992a8598?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">daneelssoul</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726608">
			March 1, 2019 at 8:58 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>OK. Maybe addition specifically was a bad example. A lot of learning to add is memorizing the one digit tables, and people usually learn to add before they learn how to understand and execute algorithms, so the process is often more involved.</p>
<p>Fine. Maybe replace addition with computing gcd&#8217;s. I&#8217;m pretty sure that people *do* learn the Euclidean algorithm from books.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-quixote even depth-5" id="li-comment-726671">
		<div id="comment-726671" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/2ff7a85b1ea207684d04472b5451e1f9?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/2ff7a85b1ea207684d04472b5451e1f9?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Quixote</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726671">
			March 1, 2019 at 10:18 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>A 19 year old human can go from not knowing linear algebra to knowing the basics by reading a short book.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-wysinwygymmv odd alt depth-5" id="li-comment-726734">
		<div id="comment-726734" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/d43abf5169eee342cc6df49be3834874?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/d43abf5169eee342cc6df49be3834874?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">wysinwygymmv</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726734">
			March 1, 2019 at 11:13 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>@daneelssoul, Quixote:</p>
<p>Sure, but do they learn those things <em>without examples</em>?  My intuition is that only a very few people learn a very few things successfully without seeing many examples and then working out some example problems themselves first.  Most &#8211;if not all! &#8212; people are much better at reasoning from examples to abstract principles rather than vice versa.  Starting with the abstract principles seems much rarer and more difficult &#8212; even those who <em>can</em> do so would usually do better given examples (again, according to my intuition).</p>
<p>@daneelsoul:</p>
<p>Any ideas on my question about whether learning language is just correlation along another dimension or not?  In support of the notion that it <em>is</em>, consider the fact that learning a new language by immersion is much more effective than trying to learn through memorization.  IOW, it&#8217;s harder to learn the right words to use by memorizing what various sounds <em>refer</em> to than to be placed in a context where you&#8217;re repeatedly shown how words are <em>used</em>.  The statistical/functional approach to language learning is more effective than the logical/semantic approach.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-daneelssoul even depth-5" id="li-comment-726773">
		<div id="comment-726773" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/303c1c1ba9e0683d93c12fcc992a8598?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/303c1c1ba9e0683d93c12fcc992a8598?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">daneelssoul</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726773">
			March 1, 2019 at 11:56 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>@wysinwygymmv:</p>
<p>I agree that humans are much better at learning things with examples. Also they are much better at learning things interactively, which is a major advantage we have over GPT.</p>
<p>But we can also learn without examples or with minimal examples. And I&#8217;m focusing on this not because it is the best way to learn something, but because from what I can tell, it is something that GPT is completely incapable of doing. GPT can learn (some) things eventually with enough examples, and it is not set up to be able to learn things interactively, so I am looking at learning without examples as the clearest example of what humans are doing differently.</p>
<p>As to whether learning is correlation along another dimension&#8230; I don&#8217;t know. I think it does capture most of what I&#8217;ve been talking about. The weakness in GPT that I have been trying to point out is that it fails to correlate things it reads in one text to things outside of that text (even if those other things are texts in its corpus). However, I am hesitant to state with any confidence that this is all that is the entirety of the difference.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-dedicating-ruckus odd alt depth-5" id="li-comment-726790">
		<div id="comment-726790" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/0d8ed7cf5e99d124f23c29308fc82d65?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/0d8ed7cf5e99d124f23c29308fc82d65?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Dedicating Ruckus</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726790">
			March 1, 2019 at 12:16 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>@wysinwygymmv:</p>
<p>Humans learn better with examples, but 1. at least in theory, they don&#8217;t need them, and 2. they can generalize examples to learn a rule, then apply the rule in ways that don&#8217;t look like the examples.</p>
<p>You could read a book about addition with examples that never went above two digits, then successfully add 295468 + 958687. GPT-2 couldn&#8217;t.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-mugasofer even depth-5" id="li-comment-726864">
		<div id="comment-726864" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/6e07249aee9d1dbe4d7bba82a7460a2a?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/6e07249aee9d1dbe4d7bba82a7460a2a?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn"><a href='http://pseudonymwrites.wordpress.com' rel='external nofollow ugc' class='url'>MugaSofer</a></cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726864">
			March 1, 2019 at 1:53 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>GPT is clearly capable of generalizing from examples to some extent, or it would be unable to predict what follows an input it hasn&#8217;t seen word-for-word previously.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-peffern odd alt depth-5" id="li-comment-726886">
		<div id="comment-726886" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/13360a9e3426878850574a08ffd90b84?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/13360a9e3426878850574a08ffd90b84?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Peffern</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726886">
			March 1, 2019 at 2:56 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Mostly unrelated, but I propose we start referring to the mass confusion of whether the AI knows what a word really means as &#8220;refer madness.&#8221;</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-realitychemist even depth-4" id="li-comment-726542">
		<div id="comment-726542" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/52e6a539f8223ffe4f2bf731c8d4c551?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/52e6a539f8223ffe4f2bf731c8d4c551?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">realitychemist</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726542">
			March 1, 2019 at 6:55 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I do share the general sense that there is something GPT-2 is missing, however I would argue that even a human would not be able to learn how to add numbers given a textbook with no actual examples.  At least, I think they would not be able to do it without <i>great difficulty</i>.  Imagine trying to teach category theory to someone without ever giving any examples of a category or an arrow!  This particular argument seems like the same sort of double-standard that people hold AIs to when they demand that AI drivers be perfectly safe.  Again, I feel like there is <i>something</i> different between GPT-2 and humans beyond just computational power, but I&#8217;m not sure this is it.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-daneelssoul odd alt depth-5" id="li-comment-726610">
		<div id="comment-726610" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/303c1c1ba9e0683d93c12fcc992a8598?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/303c1c1ba9e0683d93c12fcc992a8598?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">daneelssoul</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726610">
			March 1, 2019 at 8:59 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>No actual examples, maybe not. But you could probably do it without over writing a string of the form X+Y=Z explicitly. Or at very least, not writing it enough times that it would be possible to generalize just from those examples.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-valerylatyshev even depth-5" id="li-comment-727922">
		<div id="comment-727922" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/1ca4bf906119e7f41e5fc35fa742565b?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/1ca4bf906119e7f41e5fc35fa742565b?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Val</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727922">
			March 6, 2019 at 6:20 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I think that the situation with examples actually shows what is different between humans and AI.</p>
<p>You could give a human an explanatory text with one or two examples. Adding additional examples has quick diminishing returns. </p>
<p>With modern deep learning systems it is quite the opposite. Data is everything. A system trained on a set of 10 examples will not perform properly at all. You need thousands of data points to get even mediocre performance.</p>
<p>I believe this points to the difference between human understanding (the workings of which we do not yet understand, however imperfect and limited it may be) and relatively simple, data hungry symbol shuffling.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-mr-doolittle odd alt depth-5" id="li-comment-727936">
		<div id="comment-727936" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/dad73b28cac322ee520ebd0d91a6a7d4?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/dad73b28cac322ee520ebd0d91a6a7d4?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Mr. Doolittle</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727936">
			March 6, 2019 at 7:15 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Val, arguably the reason that a human can come up with reasonable explanations with few examples is because there are no &#8220;blank slate&#8221; humans to test with.  Every human capable of communication will have had millions of stimuli to help set parameters.  </p>
<p>I somewhat agree that humans are better at taking in fewer examples and coming to conclusions, and I very much agree that computers are nothing without significant numbers of examples.  I am quite hesitant to agree that humans really do have fewer examples, which may make the comparison shallower than you indicate.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-valerylatyshev even depth-5" id="li-comment-727968">
		<div id="comment-727968" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/1ca4bf906119e7f41e5fc35fa742565b?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/1ca4bf906119e7f41e5fc35fa742565b?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Val</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727968">
			March 6, 2019 at 9:16 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Fair point.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-dedicating-ruckus odd alt depth-5" id="li-comment-728131">
		<div id="comment-728131" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/0d8ed7cf5e99d124f23c29308fc82d65?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/0d8ed7cf5e99d124f23c29308fc82d65?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Dedicating Ruckus</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-728131">
			March 6, 2019 at 2:00 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>@Mr. Doolittle:</p>
<p>But you can give a human an explanatory text and &lt;10 examples for a system he&#039;s never seen before and that isn&#039;t at all related to the millions of (sensory) stimuli that any given human has seen over his lifetime, e.g. some esoteric mathematical system, and he&#039;ll still usually understand it and do correct operations on it. Not nearly so with any computer we&#039;ve ever had.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-sivaas even depth-5" id="li-comment-728872">
		<div id="comment-728872" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/1d2b7d9f7624f95ac028c4411cc940c8?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/1d2b7d9f7624f95ac028c4411cc940c8?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">moonfirestorm</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-728872">
			March 8, 2019 at 8:25 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>@Dedicating Ruckus</p>
<p>But he can do that because any given human has had a lot of experience understanding unfamiliar systems unrelated to things they&#8217;ve seen before.</p>
<p>It just goes up a level: lacking specific handling heuristics for that system, the human switches over to their developing-heuristics-for-new-systems heuristics.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-dedicating-ruckus odd alt depth-5" id="li-comment-728973">
		<div id="comment-728973" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/0d8ed7cf5e99d124f23c29308fc82d65?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/0d8ed7cf5e99d124f23c29308fc82d65?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Dedicating Ruckus</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-728973">
			March 8, 2019 at 12:34 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>@moonfirestorm:</p>
<p>Fair.</p>
<p>I claim that no computer we&#8217;ve built, or are close to building, has such &#8220;developing-new-heuristics heuristics&#8221;, nor can have them even given infinite input data.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-shion-arita even depth-4" id="li-comment-727264">
		<div id="comment-727264" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/0aa8e6e159a8121afe91b93d316fb00e?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/0aa8e6e159a8121afe91b93d316fb00e?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Shion Arita</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727264">
			March 3, 2019 at 11:17 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>One important difference that I haven&#8217;t seen pointed out in this discussion yet is the difference of domain space: a human can understand what water is because based on the inputs it gets, it has the ability to directly perceive some properties of water, like seeing it, feeling it, etc. GPT can only perceive text. I don&#8217;t think this is the only meaningful distinction that produces the difference, but it is a little hard to really determine how much these kinds of things know about the world when their inputs are so much more limited than our own.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-bugmaster odd alt depth-3" id="li-comment-726470">
		<div id="comment-726470" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/bd3feb70a6f6e5f20f761671d4ef386a?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/bd3feb70a6f6e5f20f761671d4ef386a?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Bugmaster</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726470">
			March 1, 2019 at 12:39 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>As I said on the other thread, AI-generated text is kind one of the easiest tasks (*), because our brains are so good at recognizing patterns in this kind of input, and at glossing over any any inaccuracies. </p>
<p>By analogy, true story: one day, I pointed my Sony camera at a campfire, because I wanted to take a picture of the flames. The camera&#8217;s face recognition module immediately recognized a face in the flames. I took the photo at the same time, and yes, I agree &#8212; the momentary pattern of flames did kind of look like a face. But this doesn&#8217;t mean that there are really jinn living in the fire, or that my camera has a human-grade imagination; only that my human brain was able to meet it halfway (and more than halfway, probably).</p>
<p>(*) Which doesn&#8217;t mean that it&#8217;s trivial, don&#8217;t get me wrong.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-phil-h even depth-3" id="li-comment-726471">
		<div id="comment-726471" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/bb96bb15c3ac73ad207c36d47aad11f5?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/bb96bb15c3ac73ad207c36d47aad11f5?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Phil H</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726471">
			March 1, 2019 at 12:44 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Haha, I&#8217;m confused! I&#8217;m not sure if I understand you, then, so I&#8217;m not sure if we disagree or not.</p>
<p>I think we&#8217;re in agreement in your para 2 here. We agree that there is such a thing as correlating X to Y. (I add the word successful, by which I mean to the satisfaction of the intentions of the partner(s) with intentions. For the moment, I&#8217;m assuming that AIs don&#8217;t have real intentions, so this cashes out as: did the computer understand what the human meant? was the computer able to reply in a way that the human could use?) I agree that there is no such thing as a Real Thing, and that we shouldn&#8217;t try to ground language in a Real Thing. (Here I&#8217;d add that I don&#8217;t think language is purely formal, and it always needs a bit of grounding, and that grounding is done in lots of different and messy ways.)</p>
<p>In your para 3 there are some things I wouldn&#8217;t agree with: &#8220;Language [is a] self-consistent form of knowledge.&#8221; I don&#8217;t think it&#8217;s necessarily self-consistent, but it&#8217;s consistent enough for day-to-day use. Same goes for chemistry.<br />
I don&#8217;t agree that you can understand language on its own terms. I learned Chomskyan linguistics at university, which is the attempt to do exactly that. Chomskyan linguistics is, so far as I can tell, the only synchronic linguistics that&#8217;s any good, and it&#8217;s so good that it&#8217;s proved itself to be untrue. I don&#8217;t think any purely formal approach to natural language is ultimately successful &#8211; and indeed the general failure of machines to speak English is a bit of indirect evidence for this.<br />
I don&#8217;t agree that language is isomorphic to any other form of knowledge. The difficulty of writing science is my go-to example. There isn&#8217;t in fact any way to write science directly using natural language &#8211; we either drop into maths, or start talking in a hodgepodge of metaphors. </p>
<p>Your last sentence is exactly what I agree with, and the whole point of my middle way! I don&#8217;t want to claim that one particular thing is a &#8220;Real Thing&#8221;, rather that any successful (satisfies intentions) mapping of language to any other system will do as a bit of grounding.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-xuenay odd alt depth-3" id="li-comment-726493">
		<div id="comment-726493" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/7155803233680c87724accc5c8169d77?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/7155803233680c87724accc5c8169d77?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Kaj Sotala</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726493">
			March 1, 2019 at 3:18 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>This reminds me of <a HREF="https://www.lesswrong.com/posts/MRqnYuCFHW46JPJag/understanding-is-translation" rel="nofollow">Understanding is Translation</a>.</p>
<p><i>A middle-schooler can understand numbers, in the sense of translating them to amounts of apples and such, but doesn&#8217;t immediately translate the expression &#8220;x &gt; 5&#8221; to a half-open ray on the number line. A self-taught singer can translate from heard notes to sung notes, but can&#8217;t translate either to notes on a staff; a self-taught guitarist is missing a different subset of those skills. A bilingual person can translate a Japanese sentence with the word &#8220;integral&#8221; to English, without knowing what integral means. You can be good at translating other people&#8217;s facial expressions to emotional states, but lousy at translating them to pencil sketches; your friend is the opposite; which of you &#8220;understands&#8221; human faces better? There&#8217;s no answer, or many answers. Don&#8217;t ask whether someone understands X. Instead, ask if they can translate X  Y.</i></p>
<p><i>That has implications for teaching. If you walk into a classroom intending to make students &#8220;understand&#8221; X, you&#8217;ll fail at teaching. (I&#8217;m speaking from experience here.) But if you find some Y, already understood by the students, that can be translated to X &#8211; and drill them repeatedly on both directions of translation &#8211; then they will begin to &#8220;understand&#8221; X.</i></p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-waltonmath even depth-4" id="li-comment-726730">
		<div id="comment-726730" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/884e82113c2f5a69a97490b80b5f9926?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/884e82113c2f5a69a97490b80b5f9926?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">waltonmath</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726730">
			March 1, 2019 at 11:10 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>That is what I first thought of when reading the OP! A worthwhile post.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-virgilkurkjian odd alt depth-3" id="li-comment-726502">
		<div id="comment-726502" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/6ba44fc8d35a1aec60bfdd498c0e8a47?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/6ba44fc8d35a1aec60bfdd498c0e8a47?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">VirgilKurkjian</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726502">
			March 1, 2019 at 4:36 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Whether or not someone understands the &#8220;real thing&#8221; is a red herring here, since obviously no group actually understands the &#8220;real thing&#8221;. The issue is that, confusingly, you don&#8217;t seem to distinguish between statistical and other forms of knowledge. </p>
<p>The kind of knowledge that children, chemists, and angels have that GPT-2 doesn&#8217;t have allows them answer questions about situations they&#8217;ve never seen before and hypotheticals about what the world would be like if the rules were slightly different. This is <i>very different</i> from pure statistical knowledge, which we know is a very convincing hack, but doesn&#8217;t provide the flexibility found in human thought.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-entirelyuseless even depth-3" id="li-comment-726522">
		<div id="comment-726522" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/539b326ef6318b6555e6ceeb87a9c20c?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/539b326ef6318b6555e6ceeb87a9c20c?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn"><a href='https://entirelyuseless.wordpress.com/' rel='external nofollow ugc' class='url'>entirelyuseless</a></cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726522">
			March 1, 2019 at 6:07 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I  think what you are missing here is the AI&#8217;s lack of concept of self.</p>
<p>Because people are contained in their ideas of the world, they know their ideas have some relationship to the things that caused those ideas. Since the AI has no idea of itself, it has no idea that the statistical relationships among words are caused by statistical relationships in the world. So the AI is just &#8220;shuffling symbols&#8221; where humans do know their symbols refer to something.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-detroitdan odd alt depth-4" id="li-comment-726681">
		<div id="comment-726681" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/6237f56734c0aae31c24b4c7763e30d8?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/6237f56734c0aae31c24b4c7763e30d8?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">detroitdan</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726681">
			March 1, 2019 at 10:29 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Good point about the concept of &#8220;self&#8221;.  I was thinking that &#8220;intelligence&#8221; as we know it is being good at surviving and reproducing.  Verbal intelligence is just the tip of the iceberg.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-dedicating-ruckus even depth-3" id="li-comment-726676">
		<div id="comment-726676" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/0d8ed7cf5e99d124f23c29308fc82d65?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/0d8ed7cf5e99d124f23c29308fc82d65?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Dedicating Ruckus</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726676">
			March 1, 2019 at 10:24 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Children, chemists and angels are all working on a model with (at least) two layers: they have a concept of The Real Thing (which notably does not mean that they &#8220;understand&#8221; it, merely that they have an entity in their model for it), and they relate symbols to The Real Thing on a separate basis. &#8220;Learning&#8221;, in this model, means &#8220;read symbols -&gt; decode them to the propositions they express about The Real Thing(s) -&gt; apply those propositions to the internal model of The Real Thing(s)&#8221;.</p>
<p>GPT-2 doesn&#8217;t have the second layer, or the notion that the symbols are &#8220;about&#8221; anything. &#8220;Learning&#8221; as GPT-2 does it means &#8220;read symbols -&gt; remember all statistical relationships between symbols&#8221;. When a human emits sentences in language, it&#8217;s because they encode claims about the human&#8217;s internal model; when GPT-2 does, it&#8217;s because they are statistically similar to sentences it&#8217;s seen before. Yes, there really is a difference between these.</p>
<p>One obvious consequence of this is that humans require far less input data. No human ever read 45GB of text, but they can still English far better than GPT-2 can. Another is the output that looks incoherent when read expecting semantic content. There&#8217;s nothing <i>statistically</i> wrong with a character speaking five words right before the narrator says &#8220;after only two words&#8221;, or with Gimli killing orcs in one paragraph and then not having taken part in the battle two paragraphs later. The level on which they don&#8217;t make any sense is one that GPT-2 doesn&#8217;t have.</p>
<p>Semantics is really the key. Obviously, words refer to things, and statements are about things; every claim that what GPT-2 is doing is really only a matter of degree from general intelligence has relied on skirting this fact, or else outright denying it just to preserve the a posteriori claim.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-detroitdan odd alt depth-4" id="li-comment-726688">
		<div id="comment-726688" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/6237f56734c0aae31c24b4c7763e30d8?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/6237f56734c0aae31c24b4c7763e30d8?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">detroitdan</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726688">
			March 1, 2019 at 10:37 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>@Dedicating Ruckus</p>
<p>Yes, that was very well said.  Recently Scott was all about the mental models, but now he seems to forgetting about that crucial concept.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-gerry-quinn even depth-4" id="li-comment-726976">
		<div id="comment-726976" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/06568c6437d4ca244756038ea5d60e36?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/06568c6437d4ca244756038ea5d60e36?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Gerry Quinn</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726976">
			March 1, 2019 at 11:10 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>The question is: what if the discrepancy doesn&#8217;t happen after two paragraphs, but a thousand?  How do you know we don&#8217;t have discrepancies just as bad?</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-dedicating-ruckus odd alt depth-5" id="li-comment-727375">
		<div id="comment-727375" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/0d8ed7cf5e99d124f23c29308fc82d65?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/0d8ed7cf5e99d124f23c29308fc82d65?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Dedicating Ruckus</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727375">
			March 4, 2019 at 9:04 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I mean, clearly humans are not <i>immune</i> to making semantic mistakes.</p>
<p>But there&#8217;s a big difference between doing a task imperfectly, and completely punting on doing it at all. (And also, bringing in complete hypotheticals of the form &#8220;humans might conceivably also make this mistake&#8221; without grounding it in specific and easily-observable common tendencies of humans is a little poor as an argument practice.)</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-don_flamingo even depth-3" id="li-comment-726692">
		<div id="comment-726692" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/f18f3dc4ceb79bfde1595a12f6c55aeb?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/f18f3dc4ceb79bfde1595a12f6c55aeb?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Don_Flamingo</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726692">
			March 1, 2019 at 10:40 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>@ScottAlexander<br />
Understanding deeper levels gives you the tools to more widen your understanding at the upper levels, though. So basing X in Y-substrate is useful, isn&#8217;t it?<br />
Also, how do you know, there might not be a ground or at some discoverable depth,<br />
or at least things, that actually appear to be so?<br />
Why should reality be fractal like that?<br />
I mean perhaps it is, but it&#8217;s not a priori obvious, that it would be so.<br />
Are you arguing for this to be the default assumption?</p>
<p>Nice read anyway, even if I don&#8217;t quite get the message.<br />
I was very pleased that the first point the chemists raised, that those children are way too weird. That was on my mind, too!</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-mbkarner odd alt depth-3" id="li-comment-726991">
		<div id="comment-726991" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/3f1a1447889df0699ec4c6fc700add6a?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/3f1a1447889df0699ec4c6fc700add6a?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Markus Karner</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726991">
			March 2, 2019 at 4:27 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>What Phil H says. This is also what Zen is all about.<br />
Also, Wittgenstein 2 of the &#8220;Investigations&#8221; (as opposed to Wittgenstein 1 of the Tractatus) &#8211; &#8220;If a lion could talk, we could not understand him.&#8221;<br />
Also, Tarski&#8217;s undefinability theorem &#8211; Semantic truth cannot be expressed through syntax.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-sebastiangarren even depth-3" id="li-comment-727168">
		<div id="comment-727168" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/ea8da9c604f43a6f2bff6c40ee222a5e?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/ea8da9c604f43a6f2bff6c40ee222a5e?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">JohnBuridan</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727168">
			March 3, 2019 at 3:30 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>What was bothering me about this post was that it was so Feyerabendian.<br />
However, previously you reviewed <a rel="nofollow"href="https://slatestarcodex.com/2019/01/08/book-review-the-structure-of-scientific-revolutions/">The Structure of Scientific Revolutions,</a> and you concluded that Kuhn is all about that &#8220;predictive processing,&#8221; and paradigms are just that!</p>
<p>Here you just pushed the claim a little more (too far, IMO) and said all the different paradigms are self-consistent and none of them are grounded in anything. I think this is a relativism which entails a lot of unpleasant consequences. Such as not being able to fight back against creeping superstitions or conspiracy theories, and inching toward a theory of knowledge in which there are no methods for rational inquiry, there are only &#8220;isomorphic forms of knowledge.&#8221;</p>
<p>What is shakier: claiming there is no ground, or claiming that systems can be self-consistent?</p>
<p>While I am wary of anyone claiming to have found The Real Thing, I also don&#8217;t want to leave the door open to the claim that &#8220;any grounding whatsoever is impossible.&#8221;</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-galle odd alt depth-3" id="li-comment-727502">
		<div id="comment-727502" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/6f96e08fb9d0d51c8f66cfbbc7e353f7?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/6f96e08fb9d0d51c8f66cfbbc7e353f7?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Galle</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727502">
			March 4, 2019 at 5:30 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I still have the very strong impression that the children know what water is and the AI does not. I don&#8217;t think that impression just came out of nowhere.</p>
<p>Yes, in a certain sense, the AI, children, and chemists are all merely manipulating symbols, but sense data has a unique importance that words and chemical compositions do not. If water is, in fact, a Real Thing, then sense data is our only form of access to information about it. Even the special knowledge possessed by the chemists ultimately derives from sense data. If there&#8217;s a Real Thing, then sense data has a direct causal connection to it, whereas the word &#8220;water&#8221; or mental models of little hydrogen and oxygen atoms do not.</p>
<p>So the concept of &#8220;grounding&#8221; something in the Real Thing, I suspect, consists of giving it some kind of causal connection to the Real Thing.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-googolplexbyte even depth-2" id="li-comment-726501">
		<div id="comment-726501" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/c4d17736f39742e0287148653b7bf61d?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/c4d17736f39742e0287148653b7bf61d?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">googolplexbyte</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726501">
			March 1, 2019 at 4:33 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>What of words that don&#8217;t have non-linguistic meaning?</p>
<p>Does no one know what punctutation means, because it doesn&#8217;t mean anything outside of language?</p>
<p>There&#8217;s also plenty of abstract terms that people know the meaning that doesn&#8217;t map to any stuff in the world.</p>
<p>Technical terms in mathematics have meaning, even if they refer to things with no physical equivalent.</p>
<p>What of fictional terms? Does no one know what magic missile means because it&#8217;s purely fictional?</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-deciusbrutus odd alt depth-3" id="li-comment-726566">
		<div id="comment-726566" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/794ddc14d165c6ec425018d798ff5486?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/794ddc14d165c6ec425018d798ff5486?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">deciusbrutus</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726566">
			March 1, 2019 at 7:40 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>&gt;Does no one know what punctutation means, because it doesnâ€™t mean anything outside of language?</p>
<p>Correct. No one knows what the %$@# punctuation means.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-zzzzort even depth-3" id="li-comment-726935">
		<div id="comment-726935" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/ab88c17fb9f14a469efdfd189a93453f?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/ab88c17fb9f14a469efdfd189a93453f?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">zzzzort</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726935">
			March 1, 2019 at 6:54 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Somewhat tangential, but I was really surprised to learn how many mathematicians subscribe to platonism, or believe in the independent reality mathematical concepts.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-shion-arita odd alt depth-4" id="li-comment-727268">
		<div id="comment-727268" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/0aa8e6e159a8121afe91b93d316fb00e?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/0aa8e6e159a8121afe91b93d316fb00e?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Shion Arita</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727268">
			March 3, 2019 at 11:24 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I agree that this is strange. What I would say is it&#8217;s unnecessary; the structures are necessarily manifested physically in the brain states of people who correctly understand them.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-dedicating-ruckus even depth-5" id="li-comment-727377">
		<div id="comment-727377" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/0d8ed7cf5e99d124f23c29308fc82d65?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/0d8ed7cf5e99d124f23c29308fc82d65?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Dedicating Ruckus</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727377">
			March 4, 2019 at 9:07 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>If you write a textbook about elliptic curves, and then someone reads it and comes to understand elliptic curves, the concept is &#8220;manifested physically&#8221; both in the textbook and the guy&#8217;s brain; but if you deny the independent reality of the concept, you still can&#8217;t talk coherently about how the textbook and the guy&#8217;s brain are referring to the same thing. Plus, it&#8217;s also pretty obvious that elliptic curves still &#8220;exist&#8221; in the same sense they currently do even if there aren&#8217;t any textbooks written about them or people who understand them, so the form of their existence can&#8217;t be wholly dependent on this physical manifestation, the way that of plain physical objects is.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-murphy odd alt depth-2" id="li-comment-726518">
		<div id="comment-726518" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/7e251322e700cb4910ad3f3bcf68eb74?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/7e251322e700cb4910ad3f3bcf68eb74?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Murphy</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726518">
			March 1, 2019 at 5:59 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Would a human qualify by this measure? </p>
<p>I suspect not. In real world examples, a human who is very much intelligent and self aware and fully capable of understanding the concept of a coffee cup, experiencing what it&#8217;s like to hold a coffee cup etc&#8230; if granted vision later in life through implants or surgery&#8230;.. may be entirely incapable of identifying a coffee cup in a visual scene until unless they can feel it first. </p>
<p>Similarly you could glue together a language system and something like this system for identifying objects in an image&#8230; </p>
<p><a rel="nofollow"href="https://www.imageidentify.com/" rel="nofollow ugc">https://www.imageidentify.com/</a></p>
<p>but then people would just declare that the system was just matching one pattern of bits to another pattern of bits.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-phil-h even depth-3" id="li-comment-726548">
		<div id="comment-726548" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/bb96bb15c3ac73ad207c36d47aad11f5?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/bb96bb15c3ac73ad207c36d47aad11f5?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Phil H</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726548">
			March 1, 2019 at 7:12 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Hi, Murphy. I think that I can answer your questions with confidence!<br />
(1) Would a human qualify &#8211; yes. My test is correlating linguistic symbols to *any* non-linguistic stuff. A blind human can correlate the words &#8220;coffee cup&#8221; to a set of physical sensations and sounds. I only chose the visual example of pictures because it&#8217;s the most common way, and computers are already good at processing images. </p>
<p>(2) [Y]ou could glue together a language system and something like this system &#8211; no, you couldn&#8217;t at the moment, or at least, not very successfully. That&#8217;s precisely the thing that doesn&#8217;t exist yet, because computers aren&#8217;t good enough at extracting meaning from natural language, or putting meaning into natural language. The way things are developing, this may become possible quite soon, and when it does, I will happily admit that the combined system understands language. (And we&#8217;ll all be out of a job, because that&#8217;s the day computers get smarter than us.)</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-sohakes odd alt depth-4" id="li-comment-726604">
		<div id="comment-726604" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/6b6c8a22d548d7735969d6c187154053?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/6b6c8a22d548d7735969d6c187154053?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">sohakes</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726604">
			March 1, 2019 at 8:52 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Maybe I understood you wrong, but isn&#8217;t content-based image retrieval exactly what you want? I think it&#8217;s not far from being solved. Yesterday I&#8217;ve seen this article <a rel="nofollow"href="https://towardsdatascience.com/one-neural-network-many-uses-image-captioning-image-search-similar-image-and-words-in-one-model-1e22080ce73d" rel="nofollow ugc">https://towardsdatascience.com/one-neural-network-many-uses-image-captioning-image-search-similar-image-and-words-in-one-model-1e22080ce73d</a> and this one <a rel="nofollow"href="https://gombru.github.io/MMSemanticRetrievalDemo/" rel="nofollow ugc">https://gombru.github.io/MMSemanticRetrievalDemo/</a> (there is a demo).</p>
<p>There are also generative adversarial networks that can generate images based on your caption such as this one <a rel="nofollow"href="https://github.com/hanzhanggit/StackGAN" rel="nofollow ugc">https://github.com/hanzhanggit/StackGAN</a>, which I think is more impressive. Yeah, it&#8217;s only for a few classes, but I think it could generalize with bigger networks and more training time.</p>
<p>I don&#8217;t have much knowledge in computer vision, so there are probably better examples.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-phil-h even depth-5" id="li-comment-726966">
		<div id="comment-726966" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/bb96bb15c3ac73ad207c36d47aad11f5?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/bb96bb15c3ac73ad207c36d47aad11f5?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Phil H</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726966">
			March 1, 2019 at 9:28 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Yep, I completely agree with this.<br />
(1) I think those models are a bit more limited than you think they are &#8211; they&#8217;re not actually as good at language as they look, because they use a limited subset of natural language (and use it well); and they&#8217;re comically easy to fool with edge-case or deceptive images. But yes, they are definitely progressing fast and&#8230;<br />
(2) When those models become as good as people at understanding images, I think the implications will be much more drastic than most people are imagining. They won&#8217;t just be captioning. For example, put an AI that can understand images onto a cctv system, and you have potentially a real-time unmanned crime detection system (&#8220;would your caption for this image be &#8216;man stealing from shop&#8217;?&#8221;). And mood monitor (Is this a happy face?). And engineer (Is this ground suitable for building a concrete structure?). And childminder (Is this child misbehaving?). It&#8217;s really close to the idea of &#8216;general AI&#8217;.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-galle odd alt depth-3" id="li-comment-727503">
		<div id="comment-727503" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/6f96e08fb9d0d51c8f66cfbbc7e353f7?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/6f96e08fb9d0d51c8f66cfbbc7e353f7?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Galle</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727503">
			March 4, 2019 at 5:39 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><blockquote><p>Similarly you could glue together a language system and something like this system for identifying objects in an imageâ€¦</p>
<p><a rel="nofollow"href="https://www.imageidentify.com/" rel="nofollow ugc">https://www.imageidentify.com/</a></p>
<p>but then people would just declare that the system was just matching one pattern of bits to another pattern of bits.</p></blockquote>
<p>What exactly does &#8220;just matching one pattern of bits to another pattern of bits&#8221; mean in this context? What we <i>want</i> is for the AI to look at the image, construct a model based on it, and then make statements about that model. People might, perhaps, justifiably complain that the AI is just matching patterns directly to other patterns without going through the intervening process of actually constructing a model, but that runs into diminishing returns fairly quickly.</p>
<p>It&#8217;s sort of like what I&#8217;ve always thought was the fatal flaw in Searle&#8217;s Chinese Room &#8211; by the time you can get the poor guy in the room to genuinely convince China&#8217;s greatest poet that he&#8217;s a native Chinese speaker even though he&#8217;s really just shuffling symbols around and has no idea what he&#8217;s saying, the department up the hall has already beaten you by just teaching their kidnapped American philosopher how to speak Chinese. Past a certain point, it&#8217;s easier to make a real AI than it is to make a fake one.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-phil-h even depth-4" id="li-comment-727850">
		<div id="comment-727850" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/bb96bb15c3ac73ad207c36d47aad11f5?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/bb96bb15c3ac73ad207c36d47aad11f5?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Phil H</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727850">
			March 5, 2019 at 7:18 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I&#8217;ve vowed to try to stamp out all mentions of Searle&#8217;s racist room, so here goes&#8230; the Chinese room thought experiment is sheer idiocy, disguised only by the exotic feel of &#8220;Chinese characters&#8221;. It posits a guy in a room with a computer that feeds him the correct responses. Replace the language Chinese with English in the thought experiment, and see if it makes any sense at all. It doesn&#8217;t, because it assumes that the computer in there with the guy is able to respond properly to natural language stimuli. It doesn&#8217;t matter whether it&#8217;s Chinese or English, the whole set up assumes a computer that can pass the Turing test, and then Searle acts shocked at the end that the result is a computer that can pass the Turing test.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-probablymatt odd alt depth-2" id="li-comment-726586">
		<div id="comment-726586" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/fa34e26aeb4d3ea44489676880b1891b?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/fa34e26aeb4d3ea44489676880b1891b?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">ProbablyMatt</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726586">
			March 1, 2019 at 8:30 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I must be missing something, but we do have neural nets that can write simple captions for images. This goes the other way to what you were proposing (from images to text rather than text to images), but I do wonder if it would be at least a step towards satisfying the requirement you suggest.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-phil-h even depth-3" id="li-comment-726967">
		<div id="comment-726967" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/bb96bb15c3ac73ad207c36d47aad11f5?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/bb96bb15c3ac73ad207c36d47aad11f5?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Phil H</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726967">
			March 1, 2019 at 9:50 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>As per my reply to Sohakes above, yes, I agree. Those image search and captioning nets are exactly what I think will lead to something with the power of &#8216;general AI&#8217;. And yes, I think it&#8217;s coming sooner than most people imagine, and the implications will be bigger than most people imagine.<br />
Ultimately, how many jobs are there that don&#8217;t involve (1) looking at something and (2) deciding what to do about it? And once a computer has the power to look at things as accurately as a human, and the power to read all the books or use all the software that tell us what to do about things&#8230; then it will put a lot of people out of work.<br />
The difficulty that they&#8217;re having with self-driving cars is an illustration of how it&#8217;s more difficult than it sounds. Driving is just looking at the road around you and deciding what to do, right? But it turns out that both parts are more complex than we thought. The looking part is still being solved, but it&#8217;s getting pretty close now. The deciding what to do part is clearly complex, and I think there&#8217;s a complicated feedback between the two. For example, if you ask a self driving system to look through images and pick the ones that show &#8220;a safe place to turn left,&#8221; evidence to date suggests that it won&#8217;t be entirely successful in doing so. Is that an &#8220;understanding the image&#8221; problem or a &#8220;deciding what to do&#8221; problem? Clearly, these two issues aren&#8217;t entirely distinct, and it will take incremental improvements in both to reach a full solution&#8230;<br />
The flipside of that is that success, when it comes, will be complete. There is no such thing as an AI that can recognise criminal activity in an image, but not know what to do about it. All of a sudden, our crime-fighting cctv AI will be able to (a) spot criminal activity happening (b) suggest the correct arrest strategy and (c) monitor and advise in real-time. There isn&#8217;t an intermediate kind of system that sees what&#8217;s happening, but needs a human to intervene and decide what to do. If it&#8217;s really good enough to see what&#8217;s happening, it&#8217;s already better at decision-making. So a lot of police are suddenly out of a job, or reduced to execution of orders rather than decision-making roles.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-fazowiec odd alt thread-odd thread-alt depth-1" id="li-comment-726451">
		<div id="comment-726451" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/30bb674ac6135b130a3c594b8fad0f06?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/30bb674ac6135b130a3c594b8fad0f06?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Faza (TCM)</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726451">
			February 28, 2019 at 10:26 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Faza leans over God&#8217;s shoulder and says:</p>
<p>&#8220;Well, we know what patterns it won&#8217;t produce at the next step, &#8216;coz they&#8217;re precluded by the rules of the game.&#8221;</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-dan-l even depth-2" id="li-comment-726914">
		<div id="comment-726914" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/3b1e2815f0bde2d8f5f1ee3b6b8a33ad?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/3b1e2815f0bde2d8f5f1ee3b6b8a33ad?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Dan L</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726914">
			March 1, 2019 at 4:54 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Maybe, but it gets interesting when the automata are Turing complete and successfully iterate a level down.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-nornagest odd alt thread-even depth-1" id="li-comment-726445">
		<div id="comment-726445" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/0d46df48d1f1b697ba13d56024a2ac23?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/0d46df48d1f1b697ba13d56024a2ac23?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Nornagest</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726445">
			February 28, 2019 at 10:13 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Huh, haven&#8217;t seen Enochian for a while.  I&#8217;m surprised there&#8217;s Unicode for&#8230; wait, no, those are just tiny images.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-admin bypostauthor even depth-2" id="li-comment-726449">
		<div id="comment-726449" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/c66389ad74ef2a291c76e87c981b0391?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/c66389ad74ef2a291c76e87c981b0391?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Scott Alexander</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726449">
			February 28, 2019 at 10:17 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I was surprised there <i>wasn&#8217;t</i> Unicode for it. John Dee reveals the language of angels, and we&#8217;re not even grateful enough to fit it in between an obscure symbol used in dentistry records and a rare Burmese punctuation mark?</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-ashleyyakeley odd alt depth-3" id="li-comment-726454">
		<div id="comment-726454" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/e72d591f55f1ac188fdfe09af0b02299?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/e72d591f55f1ac188fdfe09af0b02299?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Ashley Yakeley</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726454">
			February 28, 2019 at 10:43 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Well, <a href="https://unicode.org/mail-arch/unicode-ml/Archives-Old/UML019/0619.html" rel="nofollow">I tried nineteen years ago</a>, only to be <a href="https://unicode.org/mail-arch/unicode-ml/Archives-Old/UML019/0747.html" rel="nofollow">roundly told </a>by <a href="https://unicode.org/mail-arch/unicode-ml/Archives-Old/UML019/0760.html" rel="nofollow">the regulars</a> that it was no more than a font.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-admin bypostauthor even depth-4" id="li-comment-726455">
		<div id="comment-726455" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/c66389ad74ef2a291c76e87c981b0391?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/c66389ad74ef2a291c76e87c981b0391?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Scott Alexander</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726455">
			February 28, 2019 at 10:47 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>You are a true American hero.</p>
<p>(Also, for some reason the dialogue &#8220;It&#8217;s the natural language of angels&#8221; &#8220;So how big is the user community?&#8221; cracks me up.)</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-kuiperdolin odd alt depth-5" id="li-comment-726473">
		<div id="comment-726473" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/a53bf0f8e27d2aa9947eb0ac39773c44?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/a53bf0f8e27d2aa9947eb0ac39773c44?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Kuiperdolin</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726473">
			March 1, 2019 at 1:05 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>It&#8217;s so small it could fit on the head of a pin.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-nickcorrado95 even depth-5" id="li-comment-726528">
		<div id="comment-726528" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/9b01a3c75ca1817065349253e9811024?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/9b01a3c75ca1817065349253e9811024?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Nick</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726528">
			March 1, 2019 at 6:20 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>But if demons speak it too, <a rel="nofollow"href="https://slatestarcodex.com/2019/02/20/open-thread-121-75/#comment-723757">the userbase is huge</a>!</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-spencerbliven odd alt depth-5" id="li-comment-727299">
		<div id="comment-727299" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/3deb8c2dbc69013931e5c560e6bbe1b2?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/3deb8c2dbc69013931e5c560e6bbe1b2?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">spencer</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-727299">
			March 4, 2019 at 5:22 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>I have a feeling the user community grew considerably if you include Unsong readers.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-danielh even depth-4" id="li-comment-726478">
		<div id="comment-726478" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/034119953c654e35e99d139941dfc868?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/034119953c654e35e99d139941dfc868?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">DanielH</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726478">
			March 1, 2019 at 1:27 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Surprisingly, I can&#8217;t even find it in the <a href="http://www.evertype.com/standards/csur/" title="ConScript Unicode Registry" rel="nofollow">CSUR</a> or any other Unicode private-use area standardization project. I&#8217;d bet those would be more willing to accept it.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-muskwalker odd alt depth-5" id="li-comment-726537">
		<div id="comment-726537" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/81c553bef5b6fcaaf53f363c69dc6f8d?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/81c553bef5b6fcaaf53f363c69dc6f8d?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">muskwalker</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726537">
			March 1, 2019 at 6:44 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Having read the thread Ashley linked to, the discussion covered both Unicode and CSUR.  </p>
<p>The big objection for Unicode is that evidence wasn&#8217;t given as to a community of users of the script (saying &#8220;here is our script&#8221; is one thing, saying &#8220;we are trying to write things in our script and can&#8217;t&#8221; is another; it looked like users of the Enochian language preferred to use Latin script).   </p>
<p>The objection for CSUR was the impression that people who <i>did</i> use Enochian script only used it as a cipher for English, and not, say, to write Enochian languageâ€”hence &#8220;no more than a font&#8221;.  (This impression may or may not be correct; one of the participants admitted confusion with another mystic alphabet.)</p>
<p>They were invited to submit a proposal to CSUR though.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-virgilkurkjian even depth-4" id="li-comment-726500">
		<div id="comment-726500" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/6ba44fc8d35a1aec60bfdd498c0e8a47?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/6ba44fc8d35a1aec60bfdd498c0e8a47?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">VirgilKurkjian</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726500">
			March 1, 2019 at 4:28 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>This is the best thing I have ever heard.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-tsutsifrutsi odd alt depth-4" id="li-comment-726572">
		<div id="comment-726572" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/ff0d6c934fba08135f95947533584eb9?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/ff0d6c934fba08135f95947533584eb9?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">tsutsifrutsi</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726572">
			March 1, 2019 at 7:52 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>These days, maybe you could get it added as a Unicode variation-selector rendering rule, like the hints for which script to render a CJK-unified ideograph in. (In other words, an <i>abstract</i> font!)</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-danielh even depth-3" id="li-comment-726477">
		<div id="comment-726477" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/034119953c654e35e99d139941dfc868?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/034119953c654e35e99d139941dfc868?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">DanielH</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726477">
			March 1, 2019 at 1:23 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Because those are images and I&#8217;m unfamiliar with the script (not being myself an angel), I&#8217;m having trouble parsing it. It seems like several of the characters you use (in particular, the one that looks like a lowercase lambda/upside-down y and the one that I&#8217;m reading as a dagger with something on the end) aren&#8217;t in the description as mentioned on Wikipedia.</p>
<p>Can somebody help me out, please?</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	<ul class="children">
	<li class="comment byuser comment-author-well odd alt depth-4" id="li-comment-726924">
		<div id="comment-726924" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/b91ba7d4a14048591631076671f19c5e?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/b91ba7d4a14048591631076671f19c5e?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn"><a href='http://welldotdotdot.wordpress.com' rel='external nofollow ugc' class='url'>Well...</a></cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726924">
			March 1, 2019 at 5:42 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>They&#8217;re mostly Hebrew letters, written in a brushy stylized way.</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
	<li class="comment byuser comment-author-virtualyric even depth-4" id="li-comment-730752">
		<div id="comment-730752" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/57cced2fbb3eabdc3ae3da47ff61aaad?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/57cced2fbb3eabdc3ae3da47ff61aaad?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">Virtua Lyric</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-730752">
			March 14, 2019 at 4:10 pm</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>The words are:<br />
budapest<br />
sarajevo<br />
paris rome lisbon<br />
berlin<br />
warsaw</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	<li class="comment byuser comment-author-toastengineer odd alt depth-3" id="li-comment-726504">
		<div id="comment-726504" class="commentholder">
		<div class="comment-author vcard">
			<img alt='' src='https://secure.gravatar.com/avatar/00a34191d4cd325019288e7ba41c2595?s=40&#038;d=identicon&#038;r=g' srcset='https://secure.gravatar.com/avatar/00a34191d4cd325019288e7ba41c2595?s=80&#038;d=identicon&#038;r=g 2x' class='avatar avatar-40 photo' height='40' width='40' />			<cite class="fn">toastengineer</cite> <span class="says">says:</span>		</div><!-- .comment-author .vcard -->
		
		<div class="comment-meta commentmetadata"><a href="https://slatestarcodex.com/2019/02/28/meaningful/#comment-726504">
			March 1, 2019 at 4:47 am</a>		</div><!-- .comment-meta .commentmetadata -->

		<div class="comment-body"><p>Sorry, too busy adding</p>
<p>    * 66 emoji characters, including 4 new emoji components for hair color. </p>
<p>    * Copyleft symbol</p>
<p>    * Half stars for rating systems</p>
<p>    *Additional astrological symbols</p>
<p>     *Xiangqi Chinese chess symbols</p>
<p>(and also support for several dead languages of practical scholarly interest.)</p>
</div>

		<div class="reply">
					</div><!-- .reply -->
	</div><!-- #comment-##  -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
			</ol>


	</div><!-- #comments -->




			</div><!-- #pjgm-content -->
		</div><!-- #pjgm-box -->

<div id="primary" class="widget-area" role="complementary">
	<a class="sidebar-toggle" title="Expand Sidebar"><i class="fa fa-angle-double-left"></i><i class="fa fa-angle-double-right"></i></a>
	<ul class="xoxo">
		<li id="meta-2" class="widget-container widget_meta"><h3 class="widget-title">Meta</h3>			<ul>
			<li><a href="https://slatestarcodex.com/wp-login.php?action=register">Register</a></li>			<li><a href="https://slatestarcodex.com/wp-login.php">Log in</a></li>
			<li><a href="https://slatestarcodex.com/feed/">Entries feed</a></li>
			<li><a href="https://slatestarcodex.com/comments/feed/">Comments feed</a></li>
			<li><a href="https://wordpress.org/">WordPress.org</a></li>			</ul>
			</li><li id="blog_subscription-2" class="widget-container widget_blog_subscription jetpack_subscription_widget"><h3 class="widget-title">Subscribe via Email</h3>
            <form action="#" method="post" accept-charset="utf-8" id="subscribe-blog-blog_subscription-2">
				                    <p id="subscribe-email">
                        <label id="jetpack-subscribe-label"
                               class="screen-reader-text"
                               for="subscribe-field-blog_subscription-2">
							Email Address                        </label>
                        <input type="email" name="email" required="required" class="required"
                               value=""
                               id="subscribe-field-blog_subscription-2"
                               placeholder="Email Address"/>
                    </p>

                    <p id="subscribe-submit">
                        <input type="hidden" name="action" value="subscribe"/>
                        <input type="hidden" name="source" value="https://slatestarcodex.com/2019/02/28/meaningful/?reverseComments="/>
                        <input type="hidden" name="sub-type" value="widget"/>
                        <input type="hidden" name="redirect_fragment" value="blog_subscription-2"/>
						                        <button type="submit"
	                        		                    	                        name="jetpack_subscriptions_widget"
	                    >
	                        Subscribe                        </button>
                    </p>
				            </form>
		
</li><li id="text-4" class="widget-container widget_text"><h3 class="widget-title">Advertisements</h3>			<div class="textwidget"></div>
		</li><li id="arvins_ad_randomizer-2" class="widget-container widget_arvins_ad_randomizer"><div class="widget-text arvins_ad_randomizer_box"><div class="aar_div"><p><center><A HREF="https://www.metaculus.com/questions/?show-welcome=true"><IMG SRC="https://slatestarcodex.com/blog_images/metaculus_vert.jpg"></A></center></p>

<p>Metaculus is a platform for generating crowd-sourced predictions about the future, especially science and technology. If you're interested in testing yourself and contributing to their project, check out their <A HREF="https://www.metaculus.com/questions/">questions page</A></p>
</div><div class="aar_div"><p><center><A HREF="https://www.givingwhatwecan.org/?refer=ssc"><IMG SRC="https://slatestarcodex.com/blog_images/eatowr2.jpg"></A></center></p>

<p><A HREF="https://www.givingwhatwecan.org/">Giving What We Can</A> is a charitable movement promoting giving some of your money to the developing world or other worthy causes. If you're interested in this, consider taking their Pledge as a formal and public declaration of intent.</p></div><div class="aar_div"><p><center><A HREF="http://www.mealsquares.com/"><IMG SRC="https://slatestarcodex.com/blog_images/mealsquares_ad.png"></A></center></p>

<p><A HREF="http://www.mealsquares.com/">MealSquares</A> is a "nutritionally complete" food that contains a balanced diet worth of nutrients in a few tasty easily measurable units. Think Soylent, except zero preparation, made with natural ingredients, and looks/tastes a lot like an ordinary scone. </p></div><div class="aar_div"><center><p><a href="https://www.patreon.com/user?u=926060"><IMG SRC="https://slatestarcodex.com/blog_images/vert_patreon3.png" border="0"></A></p></center>

<p>Support Slate Star Codex on <A HREF="https://www.patreon.com/user?u=926060">Patreon</A>. I have a day job and SSC gets free hosting, so don't feel pressured to contribute. But extra cash helps pay for contest prizes, meetup expenses, and me spending extra time blogging instead of working.</p></div><div class="aar_div"><p><center><A HREF="http://epidemicforecasting.org/"><IMG SRC="https://slatestarcodex.com/blog_images/vert_covid.png"></A></center></p>

<p>The <A HREF="http://epidemicforecasting.org/">COVID-19 Forecasting Project</A> at the University of Oxford is making advanced pandemic simulations of 150+ countries available to the public, and also offer pro-bono forecasting services to decision-makers.</p>
</div><div class="aar_div"><p><center><A HREF="https://www.beeminder.com/"><IMG SRC="https://slatestarcodex.com/blog_images/beeminder_ad.png"></A></center></p>

<p>Beeminder's an evidence-based willpower augmention tool that collects quantifiable data about your life, then helps you organize it into commitment mechanisms so you can keep resolutions. They've also got a blog about what they're doing <A HREF="http://blog.beeminder.com/tag/rationality/">here</A></p></div><div class="aar_div"><p><center><A HREF="https://www.janestreet.com/join-jane-street/open-positions/?utm_source=ssc&utm_medium=banner&utm_campaign=trading&utm_term=trading&utm_content=sierpinski"><img src="https://slatestarcodex.com/blog_images/sierpinski-low_res.png" srcset="https://slatestarcodex.com/blog_images/sierpinski-low_res.png 1x, https://slatestarcodex.com/blog_images/sierpinski_med_res.png 2x"></A></center></p>

<p><A HREF="https://www.janestreet.com/join-jane-street/open-positions/?utm_source=ssc&utm_medium=banner&utm_campaign=trading&utm_term=trading&utm_content=sierpinski">Jane Street</A> is a quantitative trading firm with a focus on technology and collaborative problem solving. We're always hiring talented programmers, traders, and researchers and have internships and fulltime positions in New York, London, and Hong Kong. No background in finance required.</p></div><div class="aar_div"><p><center><A HREF="https://b4x.com/"><IMG SRC="https://slatestarcodex.com/blog_images/vert_b4x.png"></A></center></p>

<p><A HREF="https://b4x.com/">B4X</A> is a free and open source developer tool that allows users to write apps for Android, iOS, and more.</p></div><div class="aar_div"><p><center><A HREF="http://seattleanxiety.com/"><IMG SRC="https://slatestarcodex.com/blog_images/vert_seattle.png"></A></center></p>

<p><A HREF="https://seattleanxiety.com/">Seattle Anxiety Specialists</A> are a therapy practice helping people overcome anxiety and related mental health issues (eg GAD, OCD, PTSD) through evidence based interventions and self-exploration. Check out their free anti-anxiety guide <A HREF="https://seattleanxiety.com/#free-guide-section">here</A></p>.</div><div class="aar_div"><p><center><A HREF="https://substack.com/?utm_source=ssc&utm_campaign=ssc1"><IMG SRC="https://slatestarcodex.com/blog_images/vert_substack.png"></A></center></p>

<p><A HREF="https://substack.com/?utm_source=ssc&utm_campaign=ssc1">Substack</A> is a blogging site that helps writers earn money and readers discover articles they'll like.</p></div><div class="aar_div"><p><center><A HREF="http://effectivealtruism.org/?refer=ssc"><IMG SRC="https://slatestarcodex.com/blog_images/eatowr.jpg"></A></center></p>

<p>The <A HREF="http://effectivealtruism.org/?refer=ssc">Effective Altruism newsletter</A> provides monthly updates on the highest-impact ways to do good and help others.</p></div><div class="aar_div"><p><center><A HREF="https://80000hours.org/key-ideas/?utm_source=ssc&utm_campaign=2017-04+Sidebar+Ad&utm_medium=blog"><IMG SRC="https://slatestarcodex.com/blog_images/80k_vertise.png"></A></center></p>

<p>80,000 Hours researches different problems and professions to help you figure out how to do as much good as possible. Their <A HREF="https://80000hours.org/career-guide/?utm_source=ssc&utm_campaign=2017-04+Sidebar+Ad&utm_medium=blog">free career guide</A> show you how to choose a career that's fulfilling and maximises your contribution to solving the world's most pressing problems.</p></div><div class="aar_div"><p><center><A HREF="http://LauraBaurMD.com"><IMG SRC="https://slatestarcodex.com/blog_images/vert_baur.jpg"></A></center></p>

<p>Dr. Laura Baur is a psychiatrist with interests in literature review, reproductive psychiatry, and relational psychotherapy; see <A HREF="http://LauraBaurMD.com">her website</A> for more.  Note that due to conflict of interest she doesn't treat people in the NYC rationalist social scene.</p></div><div class="aar_div"><p><center><A HREF="https://safetywing.com/"><IMG SRC="https://slatestarcodex.com/blog_images/vert_safety.png"></A></center></p>

<p>Norwegian founders with an international team on a mission to offer the equivalent of a Norwegian social safety net globally available as a membership. Currently offering <A HREF="https://safetywing.com/nomad-insurance">travel medical insurance for nomads</A>, and <A HREF="https://safetywing.com/remote-health">global health insurance for remote teams</A>.</p></div><div class="aar_div"><p><center><A HREF="https://aisafety.com/reading-group/"><IMG SRC="https://slatestarcodex.com/blog_images/vert_aisafety.jpg"></A></center></p>

<p><A HREF="https://aisafety.com/reading-group/">AISafety.com</A> hosts a Skype reading group Wednesdays at 19:45 UTC, reading new and old articles on different aspects of AI Safety. We start with a presentation of a summary of the article, and then discuss in a friendly atmosphere.</A></p></div><div class="aar_div"><p><center><A HREF="https://altruisto.com/?ref=ssc"><IMG SRC="https://slatestarcodex.com/blog_images/vertise_altruisto.png"></A></center></p>

<p><A HREF="https://altruisto.com/?ref=ssc">Altruisto</A> is a browser extension so that when you shop online, a portion of the money you pay goes to effective charities (no extra cost to you). Just install an extension and when you buy something, people in poverty will get medicines, bed nets, or financial aid.</p></div><div class="aar_div"><p><center><A HREF="https://www.chartercitiesinstitute.org"><IMG SRC="https://slatestarcodex.com/blog_images/vert_charter.png"></A></center></p>

<p>The <A HREF="https://www.chartercitiesinstitute.org/">Charter Cities Institute</A> is working on ways governments can set up special zones with unique legal institutions. <A HREF="https://www.chartercitiesinstitute.org/intro">Learn more</A> about how this could help tackle problems from global poverty to climate change.</p></div></div></li>	</ul>
</div><!-- #primary .widget-area -->
			</div><!-- #pjgm-main -->
			<div id="pjgm-footer">
				<div id="pjgm-ender">
					<a href="http://www.hulozila.com/" title="Hulozila" rel="designer"> </a>
				</div><!-- #pjgm-ender -->
			</div><!-- #pjgm-footer -->
		</div><!-- #pjgm-wrap -->
		<script>
    jQuery(document).ready(function () {
		jQuery.post('https://slatestarcodex.com?ga_action=googleanalytics_get_script', {action: 'googleanalytics_get_script'}, function(response) {
			var s = document.createElement("script");
			s.type = "text/javascript";
			s.innerHTML = response;
			jQuery("head").append(s);
		});
    });
</script><script type='text/javascript'>
/* <![CDATA[ */
var ReportCommentsJs = {"ajaxurl":"https:\/\/slatestarcodex.com\/wp-admin\/admin-ajax.php","confirm":"Are you sure you want to report this comment"};
/* ]]> */
</script>
<script type='text/javascript' src='https://slatestarcodex.com/wp-content/plugins/old.reportcomments/reportcomments.js?ver=1.2'></script>
<script type='text/javascript' src='https://c0.wp.com/c/5.4.1/wp-includes/js/comment-reply.min.js'></script>
<script type='text/javascript' src='https://slatestarcodex.com/wp-content/plugins/page-links-to/dist/new-tab.js?ver=3.3.3'></script>
<script type='text/javascript'>
/* <![CDATA[ */
var pmcc_ajax = {"ajaxurl":"https:\/\/slatestarcodex.com\/wp-admin\/admin-ajax.php","errors":{"thank_you_message":"Reported.","invalid_nonce_message":"It seems you already reported this comment.","invalid_values_message":"Cheating huh?","already_flagged_message":"It seems you already reported this comment.","already_flagged_note":"Comment has been flagged already."}};
/* ]]> */
</script>
<script type='text/javascript' src='https://slatestarcodex.com/wp-content/plugins/crowd-control/js/ajax.js?ver=20150929'></script>
<script type='text/javascript' src='https://c0.wp.com/c/5.4.1/wp-includes/js/wp-embed.min.js'></script>
<script type='text/javascript' src='https://stats.wp.com/e-202021.js' async='async' defer='defer'></script>
<script type='text/javascript'>
	_stq = window._stq || [];
	_stq.push([ 'view', {v:'ext',j:'1:8.5',blog:'46701818',post:'5412',tz:'-7',srv:'slatestarcodex.com'} ]);
	_stq.push([ 'clickTrackerInit', '46701818', '5412' ]);
</script>
		<script src="https://bakkot.github.io/SlateStarComments/ssc.js"></script>

		<script>
			jQuery(function() {

				/*  Sidebar collapse
				/* ------------------------------------ */

				jQuery('#primary .sidebar-toggle').click(function(){
					jQuery('body').toggleClass('s1-collapse').toggleClass('s1-expand');
					if (jQuery('body').is('.s2-expand')) {
						jQuery('body').toggleClass('s2-expand').toggleClass('s2-collapse');
					}
				});
				jQuery('#left-sidebar .sidebar-toggle').click(function(){
					jQuery('body').toggleClass('s2-collapse').toggleClass('s2-expand');
					if (jQuery('body').is('.s1-expand')) {
						jQuery('body').toggleClass('s1-expand').toggleClass('s1-collapse');
					}
				});
			});
		</script>
	</body>
</html>
