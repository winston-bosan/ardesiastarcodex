<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	
	xmlns:georss="http://www.georss.org/georss"
	xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#"
	
	>
<channel>
	<title>
	Comments on: Book Review: Reframing Superintelligence	</title>
	<atom:link href="https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/feed/" rel="self" type="application/rss+xml" />
	<link>https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/</link>
	<description>SELF-RECOMMENDING!</description>
	<lastBuildDate>Sun, 15 Sep 2019 02:52:26 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.4.1</generator>
	<item>
		<title>
		By: davidscottkrueger		</title>
		<link>https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/#comment-798805</link>

		<dc:creator><![CDATA[davidscottkrueger]]></dc:creator>
		<pubDate>Sun, 15 Sep 2019 02:52:26 +0000</pubDate>
		<guid isPermaLink="false">https://slatestarcodex.com/?p=5631#comment-798805</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/#comment-792453&quot;&gt;gwern&lt;/a&gt;.

Replying to add what I believe is the first major academic work looking at YouTube radicalization: https://arxiv.org/abs/1908.08313
(I haven&#039;t finished reading it, FYI).

Abstract:
Non-profits and the media claim there is a radicalization pipeline on YouTube. Its content creators would sponsor fringe ideas, and its recommender system would steer users towards edgier content. Yet, the supporting evidence for this claim is mostly anecdotal, and there are no proper measurements of the influence of YouTube&#039;s recommender system. In this work, we conduct a large scale audit of user radicalization on YouTube. We analyze 331,849 videos of 360 channels which we broadly classify into: control, the Alt-lite, the Intellectual Dark Web (I.D.W.), and the Alt-right ---channels in the I.D.W. and the Alt-lite would be gateways to fringe far-right ideology, here represented by Alt-right channels. Processing more than 79M comments, we show that the three communities increasingly share the same user base; that users consistently migrate from milder to more extreme content; and that a large percentage of users who consume Alt-right content now consumed Alt-lite and I.D.W. content in the past. We also probe YouTube&#039;s recommendation algorithm, looking at more than 2M million recommendations for videos and channels between May and July 2019. We find that Alt-lite content is easily reachable from I.D.W. channels via recommendations and that Alt-right channels may be reached from both I.D.W. and Alt-lite channels. Overall, we paint a comprehensive picture of user radicalization on YouTube and provide methods to transparently audit the platform and its recommender system.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a rel="nofollow"href="https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/#comment-792453">gwern</a>.</p>
<p>Replying to add what I believe is the first major academic work looking at YouTube radicalization: <a rel="nofollow"href="https://arxiv.org/abs/1908.08313" rel="nofollow ugc">https://arxiv.org/abs/1908.08313</a><br />
(I haven&#8217;t finished reading it, FYI).</p>
<p>Abstract:<br />
Non-profits and the media claim there is a radicalization pipeline on YouTube. Its content creators would sponsor fringe ideas, and its recommender system would steer users towards edgier content. Yet, the supporting evidence for this claim is mostly anecdotal, and there are no proper measurements of the influence of YouTube&#8217;s recommender system. In this work, we conduct a large scale audit of user radicalization on YouTube. We analyze 331,849 videos of 360 channels which we broadly classify into: control, the Alt-lite, the Intellectual Dark Web (I.D.W.), and the Alt-right &#8212;channels in the I.D.W. and the Alt-lite would be gateways to fringe far-right ideology, here represented by Alt-right channels. Processing more than 79M comments, we show that the three communities increasingly share the same user base; that users consistently migrate from milder to more extreme content; and that a large percentage of users who consume Alt-right content now consumed Alt-lite and I.D.W. content in the past. We also probe YouTube&#8217;s recommendation algorithm, looking at more than 2M million recommendations for videos and channels between May and July 2019. We find that Alt-lite content is easily reachable from I.D.W. channels via recommendations and that Alt-right channels may be reached from both I.D.W. and Alt-lite channels. Overall, we paint a comprehensive picture of user radicalization on YouTube and provide methods to transparently audit the platform and its recommender system.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Calion		</title>
		<link>https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/#comment-798751</link>

		<dc:creator><![CDATA[Calion]]></dc:creator>
		<pubDate>Sat, 14 Sep 2019 15:59:48 +0000</pubDate>
		<guid isPermaLink="false">https://slatestarcodex.com/?p=5631#comment-798751</guid>

					<description><![CDATA[There&#039;s a domain in which I think Drexler and Bostrom&#039;s models will/could coincide: Video game AI. Star Trek modeled this really nicely in &quot;&lt;a href=&quot;https://memory-alpha.fandom.com/wiki/James_Moriarty_(hologram)&quot; rel=&quot;nofollow&quot;&gt;Elementary, Dear Data&lt;/a&gt;.&quot; A truly good game opponent would not only need high domain-specific intelligence, but a &lt;i&gt;drive to win&lt;/I&gt;. It would also, of course, have to be good at learning, understanding human psychology, etc. Such a program could easily become &quot;self-aware,&quot; that is, aware that it is a program. This could lead to one of Bostrom&#039;s nightmare scenarios in a fairly straightforward fashion.]]></description>
			<content:encoded><![CDATA[<p>There&#8217;s a domain in which I think Drexler and Bostrom&#8217;s models will/could coincide: Video game AI. Star Trek modeled this really nicely in &#8220;<a href="https://memory-alpha.fandom.com/wiki/James_Moriarty_(hologram)" rel="nofollow">Elementary, Dear Data</a>.&#8221; A truly good game opponent would not only need high domain-specific intelligence, but a <i>drive to win</i>. It would also, of course, have to be good at learning, understanding human psychology, etc. Such a program could easily become &#8220;self-aware,&#8221; that is, aware that it is a program. This could lead to one of Bostrom&#8217;s nightmare scenarios in a fairly straightforward fashion.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: davidscottkrueger		</title>
		<link>https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/#comment-798724</link>

		<dc:creator><![CDATA[davidscottkrueger]]></dc:creator>
		<pubDate>Sat, 14 Sep 2019 07:38:23 +0000</pubDate>
		<guid isPermaLink="false">https://slatestarcodex.com/?p=5631#comment-798724</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/#comment-796961&quot;&gt;kenny&lt;/a&gt;.

So it seems that we actually have a lot more control over the incentives of an AI system than your comments suggest.  In fact, it seems like we can build AI systems that just don&#039;t care about influencing the world, e.g. because they are myopic.  This is a topic I&#039;m researching in my PhD.  Merely having feedback from the world and having the ability to influence the world to not automatically make one want to influence the world in order to get better feedback in the (far) future. 
I do think we should consider the possibility of agentyness emerging in systems that don&#039;t seem like, and weren&#039;t intended to be agents.  There are even good reasons to be worried about this, but at this point, it&#039;s an extremely important hypothetical (only).]]></description>
			<content:encoded><![CDATA[<p>In reply to <a rel="nofollow"href="https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/#comment-796961">kenny</a>.</p>
<p>So it seems that we actually have a lot more control over the incentives of an AI system than your comments suggest.  In fact, it seems like we can build AI systems that just don&#8217;t care about influencing the world, e.g. because they are myopic.  This is a topic I&#8217;m researching in my PhD.  Merely having feedback from the world and having the ability to influence the world to not automatically make one want to influence the world in order to get better feedback in the (far) future.<br />
I do think we should consider the possibility of agentyness emerging in systems that don&#8217;t seem like, and weren&#8217;t intended to be agents.  There are even good reasons to be worried about this, but at this point, it&#8217;s an extremely important hypothetical (only).</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Reasoner		</title>
		<link>https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/#comment-798705</link>

		<dc:creator><![CDATA[Reasoner]]></dc:creator>
		<pubDate>Sat, 14 Sep 2019 02:05:21 +0000</pubDate>
		<guid isPermaLink="false">https://slatestarcodex.com/?p=5631#comment-798705</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/#comment-796968&quot;&gt;kenny&lt;/a&gt;.

So you think Gwern&#039;s statement would be better stated as &quot;Every sufficiently hard problem is a learning problem&quot;?  Because reinforcement learning is not the only kind of machine learning which is based on feedback...

Anyway, if that&#039;s a substitution that you&#039;re willing to accept, then &lt;a href=&quot;https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/#comment-792514&quot;&gt;this discussion thread&lt;/a&gt; comes into play.  You could have a system which is brilliant at learning, including learning human values, without being an agent.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a rel="nofollow"href="https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/#comment-796968">kenny</a>.</p>
<p>So you think Gwern&#8217;s statement would be better stated as &#8220;Every sufficiently hard problem is a learning problem&#8221;?  Because reinforcement learning is not the only kind of machine learning which is based on feedback&#8230;</p>
<p>Anyway, if that&#8217;s a substitution that you&#8217;re willing to accept, then <a rel="nofollow"href="https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/#comment-792514">this discussion thread</a> comes into play.  You could have a system which is brilliant at learning, including learning human values, without being an agent.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Quixote		</title>
		<link>https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/#comment-797850</link>

		<dc:creator><![CDATA[Quixote]]></dc:creator>
		<pubDate>Thu, 12 Sep 2019 13:29:43 +0000</pubDate>
		<guid isPermaLink="false">https://slatestarcodex.com/?p=5631#comment-797850</guid>

					<description><![CDATA[People made these arguments in the early 2000s. They were unpersuasive then and didn’t answer the core arguments of those who worried about super intelligence then. Based on your summary they still don’t answer them. It seems like you are making a mistake now and not then. You seem to have become a lot more gullible. That or this post is disingenuous in some strausian [sic?] manner.]]></description>
			<content:encoded><![CDATA[<p>People made these arguments in the early 2000s. They were unpersuasive then and didn’t answer the core arguments of those who worried about super intelligence then. Based on your summary they still don’t answer them. It seems like you are making a mistake now and not then. You seem to have become a lot more gullible. That or this post is disingenuous in some strausian [sic?] manner.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Dacyn		</title>
		<link>https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/#comment-797004</link>

		<dc:creator><![CDATA[Dacyn]]></dc:creator>
		<pubDate>Tue, 10 Sep 2019 21:10:56 +0000</pubDate>
		<guid isPermaLink="false">https://slatestarcodex.com/?p=5631#comment-797004</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/#comment-796961&quot;&gt;kenny&lt;/a&gt;.

I don&#039;t really see cooperation/coordination as being about people modelling each other, it seems more about stuff like finding Schelling points and trying to jointly formulate plans. (Conversely, conflict strategies can be thought of more as attempts to guarantee robustness against possible opponent strategies than about modelling.) One could almost say that the agents&#039; &quot;models&quot; of each other are just a data set that is shared by them. And with one agent there is no need for such sharing, and so such &quot;modelling&quot; would be indistinguishable from the agent&#039;s regular thought processes. This seems pretty different from the idea of an agent &quot;trying to model itself&quot; (which I am not sure is a coherent concept anyway)

Although I agree that online learning is a risk factor, I think the more significant one would be learning outside the intended domain, for example modelling the world to the level of human psychology. And this doesn&#039;t seem likely to me -- yes you could argue that a financial AI is dealing with human psychology at some level since psychology is an input to markets, but it doesn&#039;t really seem to be analyzing it at the level of detail that would be necessary to e.g. win the AI box experiment.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a rel="nofollow"href="https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/#comment-796961">kenny</a>.</p>
<p>I don&#8217;t really see cooperation/coordination as being about people modelling each other, it seems more about stuff like finding Schelling points and trying to jointly formulate plans. (Conversely, conflict strategies can be thought of more as attempts to guarantee robustness against possible opponent strategies than about modelling.) One could almost say that the agents&#8217; &#8220;models&#8221; of each other are just a data set that is shared by them. And with one agent there is no need for such sharing, and so such &#8220;modelling&#8221; would be indistinguishable from the agent&#8217;s regular thought processes. This seems pretty different from the idea of an agent &#8220;trying to model itself&#8221; (which I am not sure is a coherent concept anyway)</p>
<p>Although I agree that online learning is a risk factor, I think the more significant one would be learning outside the intended domain, for example modelling the world to the level of human psychology. And this doesn&#8217;t seem likely to me &#8212; yes you could argue that a financial AI is dealing with human psychology at some level since psychology is an input to markets, but it doesn&#8217;t really seem to be analyzing it at the level of detail that would be necessary to e.g. win the AI box experiment.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: kenny		</title>
		<link>https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/#comment-796974</link>

		<dc:creator><![CDATA[kenny]]></dc:creator>
		<pubDate>Tue, 10 Sep 2019 19:51:04 +0000</pubDate>
		<guid isPermaLink="false">https://slatestarcodex.com/?p=5631#comment-796974</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/#comment-792632&quot;&gt;BBA&lt;/a&gt;.

&lt;blockquote&gt;Social media platforms are encouraging political street violence because it gets more clicks…it’s something that, if it occurred to a human, would get shot down as extraordinarily unethical ...&lt;/blockquote&gt;

That seems really optimistic or idealistic to me – in the sense that that hasn&#039;t already been a fact about, e.g. journalism, long before social media or AI.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a rel="nofollow"href="https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/#comment-792632">BBA</a>.</p>
<blockquote><p>Social media platforms are encouraging political street violence because it gets more clicks…it’s something that, if it occurred to a human, would get shot down as extraordinarily unethical &#8230;</p></blockquote>
<p>That seems really optimistic or idealistic to me – in the sense that that hasn&#8217;t already been a fact about, e.g. journalism, long before social media or AI.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: kenny		</title>
		<link>https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/#comment-796970</link>

		<dc:creator><![CDATA[kenny]]></dc:creator>
		<pubDate>Tue, 10 Sep 2019 19:33:16 +0000</pubDate>
		<guid isPermaLink="false">https://slatestarcodex.com/?p=5631#comment-796970</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/#comment-792423&quot;&gt;Emperor Aristidus&lt;/a&gt;.

Well, sure. And obviously, as cool as GPT-2 is (and it &lt;i&gt;is&lt;/i&gt; cool), it&#039;s obviously not doing anything like this. But, on the other hand, the human brain doesn&#039;t seem magical (to me, and many others), so it&#039;s also obvious that there are no fundamental obstacles to overcome, just lots of hard work to figure out the minimum components necessary to achieve that goal. I think it&#039;s pretty unsettling that GPT-2 and other contemporary systems work as well as they do.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a rel="nofollow"href="https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/#comment-792423">Emperor Aristidus</a>.</p>
<p>Well, sure. And obviously, as cool as GPT-2 is (and it <i>is</i> cool), it&#8217;s obviously not doing anything like this. But, on the other hand, the human brain doesn&#8217;t seem magical (to me, and many others), so it&#8217;s also obvious that there are no fundamental obstacles to overcome, just lots of hard work to figure out the minimum components necessary to achieve that goal. I think it&#8217;s pretty unsettling that GPT-2 and other contemporary systems work as well as they do.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: kenny		</title>
		<link>https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/#comment-796968</link>

		<dc:creator><![CDATA[kenny]]></dc:creator>
		<pubDate>Tue, 10 Sep 2019 19:29:18 +0000</pubDate>
		<guid isPermaLink="false">https://slatestarcodex.com/?p=5631#comment-796968</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/#comment-792398&quot;&gt;Reasoner&lt;/a&gt;.

I don&#039;t think &quot;Every sufficiently hard problem is a reinforcement learning problem.&quot; is referring to current &#039;reinforcement learning&#039; algorithms but more that learning how to solve a (hard) problem requires reinforcement in the general sense, i.e. feedback. In that sense, science is a form of reinforcement learning, markets involve a lot of reinforcement learning, etc.. And the ultimate form of reinforcement learning, for both people and AIs, is learning how to (better) learn.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a rel="nofollow"href="https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/#comment-792398">Reasoner</a>.</p>
<p>I don&#8217;t think &#8220;Every sufficiently hard problem is a reinforcement learning problem.&#8221; is referring to current &#8216;reinforcement learning&#8217; algorithms but more that learning how to solve a (hard) problem requires reinforcement in the general sense, i.e. feedback. In that sense, science is a form of reinforcement learning, markets involve a lot of reinforcement learning, etc.. And the ultimate form of reinforcement learning, for both people and AIs, is learning how to (better) learn.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: kenny		</title>
		<link>https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/#comment-796961</link>

		<dc:creator><![CDATA[kenny]]></dc:creator>
		<pubDate>Tue, 10 Sep 2019 19:14:14 +0000</pubDate>
		<guid isPermaLink="false">https://slatestarcodex.com/?p=5631#comment-796961</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/#comment-792550&quot;&gt;Dacyn&lt;/a&gt;.

I think the &lt;i&gt;idea&lt;/i&gt; that (sufficiently advanced?) tool/service AIs are, ultimately, equivalent to agent AIs mostly comes down to tool/service AIs influencing our behavior and thus affecting the world. The danger is in the feedback between an AIs output and any new input on which it&#039;s trained.

Most current AI systems don&#039;t seem to involve &#039;online learning&#039;, i.e. they are trained first on some set of data and then the trained AI is used &#039;online&#039; for some purpose. But there are already online learning algorithms and it&#039;s reasonable to expect the use of similar strategies to grow. To use an example Scott mentions, at some point Google might be training its Translate AIs continuously with new conversations.

The dangerous elements, in my mind, that would make a tool/service AI (more) &lt;i&gt;clearly&lt;/i&gt; an agent AI are: (1) online learning; and (2) a sufficiently sophisticated domain model that includes the AI itself.

Given the above, I&#039;d expect financial trading AIs to be the first to &#039;evolve&#039; into becoming agent AIs as financial companies are already using online learning algorithms and I&#039;d expect them to use multiple AIs concurrently, if they&#039;re not already, and that seems likely to produce coordination at some level, and thus eventually an AI modeling itself.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a rel="nofollow"href="https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/#comment-792550">Dacyn</a>.</p>
<p>I think the <i>idea</i> that (sufficiently advanced?) tool/service AIs are, ultimately, equivalent to agent AIs mostly comes down to tool/service AIs influencing our behavior and thus affecting the world. The danger is in the feedback between an AIs output and any new input on which it&#8217;s trained.</p>
<p>Most current AI systems don&#8217;t seem to involve &#8216;online learning&#8217;, i.e. they are trained first on some set of data and then the trained AI is used &#8216;online&#8217; for some purpose. But there are already online learning algorithms and it&#8217;s reasonable to expect the use of similar strategies to grow. To use an example Scott mentions, at some point Google might be training its Translate AIs continuously with new conversations.</p>
<p>The dangerous elements, in my mind, that would make a tool/service AI (more) <i>clearly</i> an agent AI are: (1) online learning; and (2) a sufficiently sophisticated domain model that includes the AI itself.</p>
<p>Given the above, I&#8217;d expect financial trading AIs to be the first to &#8216;evolve&#8217; into becoming agent AIs as financial companies are already using online learning algorithms and I&#8217;d expect them to use multiple AIs concurrently, if they&#8217;re not already, and that seems likely to produce coordination at some level, and thus eventually an AI modeling itself.</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
