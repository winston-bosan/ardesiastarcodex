<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	
	xmlns:georss="http://www.georss.org/georss"
	xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#"
	
	>
<channel>
	<title>
	Comments on: Map Of Effective Altruism	</title>
	<atom:link href="https://slatestarcodex.com/2020/02/02/map-of-effective-altruism/feed/" rel="self" type="application/rss+xml" />
	<link>https://slatestarcodex.com/2020/02/02/map-of-effective-altruism/</link>
	<description>SELF-RECOMMENDING!</description>
	<lastBuildDate>Thu, 27 Feb 2020 13:06:50 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.4.1</generator>
	<item>
		<title>
		By: Pablo		</title>
		<link>https://slatestarcodex.com/2020/02/02/map-of-effective-altruism/#comment-858269</link>

		<dc:creator><![CDATA[Pablo]]></dc:creator>
		<pubDate>Thu, 27 Feb 2020 13:06:50 +0000</pubDate>
		<guid isPermaLink="false">https://slatestarcodex.com/?p=5862#comment-858269</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://slatestarcodex.com/2020/02/02/map-of-effective-altruism/#comment-848287&quot;&gt;Taymon A. Beal&lt;/a&gt;.

&lt;blockquote&gt;My hypothesis is that the biggest reason for this is that, in order to prioritize anti-aging as an EA cause, you have to both not shy away from weird speculative cause areas that the mainstream disdains, and not buy the arguments that X-risk prevention is more important than anything else.&lt;/blockquote&gt;

Yeah, I offered a very similar explanation &lt;a href=&quot;https://www.reddit.com/r/EffectiveAltruism/comments/dyyd1y/how_come_i_see_so_little_longevity_research_in/f84ht0o/?context=3&quot; rel=&quot;nofollow ugc&quot;&gt;here&lt;/a&gt;:

&lt;blockquote&gt;Longevity research occupies an unstable position in the space of possible EA cause areas: it is very &quot;hardcore&quot; and &quot;weird&quot; on some dimensions, but not at all on others. The EAs in principle most receptive to the case for longevity research tend also to be those most willing to question the &quot;common-sense&quot; views that only humans, and present humans, matter morally. But, as you note, one needs to exclude animals and take a person-affecting view to derive the &quot;obvious corollary that curing aging is our number one priority&quot;. As a consequence, such potential supporters of longevity research end up deprioritizing this cause area relative to less human-centric or more long-termist alternatives.&lt;/blockquote&gt;]]></description>
			<content:encoded><![CDATA[<p>In reply to <a rel="nofollow"href="https://slatestarcodex.com/2020/02/02/map-of-effective-altruism/#comment-848287">Taymon A. Beal</a>.</p>
<blockquote><p>My hypothesis is that the biggest reason for this is that, in order to prioritize anti-aging as an EA cause, you have to both not shy away from weird speculative cause areas that the mainstream disdains, and not buy the arguments that X-risk prevention is more important than anything else.</p></blockquote>
<p>Yeah, I offered a very similar explanation <a rel="nofollow"href="https://www.reddit.com/r/EffectiveAltruism/comments/dyyd1y/how_come_i_see_so_little_longevity_research_in/f84ht0o/?context=3" rel="nofollow ugc">here</a>:</p>
<blockquote><p>Longevity research occupies an unstable position in the space of possible EA cause areas: it is very &#8220;hardcore&#8221; and &#8220;weird&#8221; on some dimensions, but not at all on others. The EAs in principle most receptive to the case for longevity research tend also to be those most willing to question the &#8220;common-sense&#8221; views that only humans, and present humans, matter morally. But, as you note, one needs to exclude animals and take a person-affecting view to derive the &#8220;obvious corollary that curing aging is our number one priority&#8221;. As a consequence, such potential supporters of longevity research end up deprioritizing this cause area relative to less human-centric or more long-termist alternatives.</p></blockquote>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: profgerm		</title>
		<link>https://slatestarcodex.com/2020/02/02/map-of-effective-altruism/#comment-852366</link>

		<dc:creator><![CDATA[profgerm]]></dc:creator>
		<pubDate>Thu, 13 Feb 2020 20:08:01 +0000</pubDate>
		<guid isPermaLink="false">https://slatestarcodex.com/?p=5862#comment-852366</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://slatestarcodex.com/2020/02/02/map-of-effective-altruism/#comment-848356&quot;&gt;thisheavenlyconjugation&lt;/a&gt;.

I mean, isn&#039;t that the &lt;a href=&quot;https://slatestarcodex.com/2017/08/16/fear-and-loathing-at-effective-altruism-global-2017/&quot;&gt;party line for effective altruism&lt;/a&gt;?

&lt;blockquote&gt;(I had been avoiding the 80,000 Hours people out of embarassment after their career analyses discovered that being a doctor was low-impact, but by bad luck I ended up sharing a ride home with one of them. I sheepishly introduced myself as a doctor, and he said “Oh, so am I!” I felt relieved until he added that he had stopped practicing medicine after he learned how low-impact it was, and gone to work for 80,000 Hours instead.&lt;/blockquote&gt;

&quot;The most effective thing you can do is preach about effective altruism&quot; is pretty much my takeaway from that whole post. Though I&#039;d disagree: there&#039;s nothing ordinary about the people of EA, and this has many pros and many cons.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a rel="nofollow"href="https://slatestarcodex.com/2020/02/02/map-of-effective-altruism/#comment-848356">thisheavenlyconjugation</a>.</p>
<p>I mean, isn&#8217;t that the <a rel="nofollow"href="https://slatestarcodex.com/2017/08/16/fear-and-loathing-at-effective-altruism-global-2017/">party line for effective altruism</a>?</p>
<blockquote><p>(I had been avoiding the 80,000 Hours people out of embarassment after their career analyses discovered that being a doctor was low-impact, but by bad luck I ended up sharing a ride home with one of them. I sheepishly introduced myself as a doctor, and he said “Oh, so am I!” I felt relieved until he added that he had stopped practicing medicine after he learned how low-impact it was, and gone to work for 80,000 Hours instead.</p></blockquote>
<p>&#8220;The most effective thing you can do is preach about effective altruism&#8221; is pretty much my takeaway from that whole post. Though I&#8217;d disagree: there&#8217;s nothing ordinary about the people of EA, and this has many pros and many cons.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Schilling		</title>
		<link>https://slatestarcodex.com/2020/02/02/map-of-effective-altruism/#comment-850415</link>

		<dc:creator><![CDATA[John Schilling]]></dc:creator>
		<pubDate>Sat, 08 Feb 2020 20:45:15 +0000</pubDate>
		<guid isPermaLink="false">https://slatestarcodex.com/?p=5862#comment-850415</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://slatestarcodex.com/2020/02/02/map-of-effective-altruism/#comment-849255&quot;&gt;John Schilling&lt;/a&gt;.

&lt;blockquote&gt;The reason I brought it up is because you said “we’ve seen all the X-risk class asteroids before”, which is kind of an absurd statement,&lt;/blockquote&gt;

It is probably a literally true statement, so please go recalibrate your absurd-ometer.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a rel="nofollow"href="https://slatestarcodex.com/2020/02/02/map-of-effective-altruism/#comment-849255">John Schilling</a>.</p>
<blockquote><p>The reason I brought it up is because you said “we’ve seen all the X-risk class asteroids before”, which is kind of an absurd statement,</p></blockquote>
<p>It is probably a literally true statement, so please go recalibrate your absurd-ometer.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Bugmaster		</title>
		<link>https://slatestarcodex.com/2020/02/02/map-of-effective-altruism/#comment-850338</link>

		<dc:creator><![CDATA[Bugmaster]]></dc:creator>
		<pubDate>Sat, 08 Feb 2020 06:49:25 +0000</pubDate>
		<guid isPermaLink="false">https://slatestarcodex.com/?p=5862#comment-850338</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://slatestarcodex.com/2020/02/02/map-of-effective-altruism/#comment-849255&quot;&gt;John Schilling&lt;/a&gt;.

@John Schilling:
The pedantry here is actually important, because we&#039;re looking at very low probabilities across the board. If the probability of a killer asteroid impact is 1e-26, and the probability of a killer Singularity is 1e-50, then we should still allocate more money toward asteroids -- even if &quot;more money&quot; translates into &quot;not much at all&quot;. 

The reason I brought it up is because you said &quot;we’ve seen all the X-risk class asteroids before&quot;, which is kind of an absurd statement, which leads me to believe that you are wildly off on your probability calculations. Can you really guarantee with total certainty that there isn&#039;t some low-albedo asteroid hurtling towards us from the Oort cloud (or maybe even interstellar space) right now ? If not, then we&#039;re talking about low-probability events, and we need to show our math; and saying &quot;we can&#039;t calculate P(X) and therefore we should worry about X&quot; is not math.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a rel="nofollow"href="https://slatestarcodex.com/2020/02/02/map-of-effective-altruism/#comment-849255">John Schilling</a>.</p>
<p>@John Schilling:<br />
The pedantry here is actually important, because we&#8217;re looking at very low probabilities across the board. If the probability of a killer asteroid impact is 1e-26, and the probability of a killer Singularity is 1e-50, then we should still allocate more money toward asteroids &#8212; even if &#8220;more money&#8221; translates into &#8220;not much at all&#8221;. </p>
<p>The reason I brought it up is because you said &#8220;we’ve seen all the X-risk class asteroids before&#8221;, which is kind of an absurd statement, which leads me to believe that you are wildly off on your probability calculations. Can you really guarantee with total certainty that there isn&#8217;t some low-albedo asteroid hurtling towards us from the Oort cloud (or maybe even interstellar space) right now ? If not, then we&#8217;re talking about low-probability events, and we need to show our math; and saying &#8220;we can&#8217;t calculate P(X) and therefore we should worry about X&#8221; is not math.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: FeepingCreature		</title>
		<link>https://slatestarcodex.com/2020/02/02/map-of-effective-altruism/#comment-850031</link>

		<dc:creator><![CDATA[FeepingCreature]]></dc:creator>
		<pubDate>Fri, 07 Feb 2020 16:58:55 +0000</pubDate>
		<guid isPermaLink="false">https://slatestarcodex.com/?p=5862#comment-850031</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://slatestarcodex.com/2020/02/02/map-of-effective-altruism/#comment-848264&quot;&gt;Nancy Lebovitz&lt;/a&gt;.

Note that depending how early you can get to it, you only need to shift its orbit a very small amount. Haven&#039;t done the math but I suspect that given years of warning, you could probably divert it with a few ordinary launches. Elon Musk could probably do it on his own.

The farther it&#039;s from the earth when you get a rocket to it, the larger the effect of your maneuver by the time it gets close to Earth.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a rel="nofollow"href="https://slatestarcodex.com/2020/02/02/map-of-effective-altruism/#comment-848264">Nancy Lebovitz</a>.</p>
<p>Note that depending how early you can get to it, you only need to shift its orbit a very small amount. Haven&#8217;t done the math but I suspect that given years of warning, you could probably divert it with a few ordinary launches. Elon Musk could probably do it on his own.</p>
<p>The farther it&#8217;s from the earth when you get a rocket to it, the larger the effect of your maneuver by the time it gets close to Earth.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: John Schilling		</title>
		<link>https://slatestarcodex.com/2020/02/02/map-of-effective-altruism/#comment-849964</link>

		<dc:creator><![CDATA[John Schilling]]></dc:creator>
		<pubDate>Fri, 07 Feb 2020 14:39:18 +0000</pubDate>
		<guid isPermaLink="false">https://slatestarcodex.com/?p=5862#comment-849964</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://slatestarcodex.com/2020/02/02/map-of-effective-altruism/#comment-849255&quot;&gt;John Schilling&lt;/a&gt;.

&lt;blockquote&gt;What, really all of them ? Are the chances of us having missed just one such asteroid literally zero ?&lt;/blockquote&gt;

If you want to play the pedantic literalism game, scroll up &lt;a href=&quot;https://slatestarcodex.com/2020/02/02/map-of-effective-altruism/#comment-848255&quot;&gt;five posts&lt;/a&gt;.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a rel="nofollow"href="https://slatestarcodex.com/2020/02/02/map-of-effective-altruism/#comment-849255">John Schilling</a>.</p>
<blockquote><p>What, really all of them ? Are the chances of us having missed just one such asteroid literally zero ?</p></blockquote>
<p>If you want to play the pedantic literalism game, scroll up <a rel="nofollow"href="https://slatestarcodex.com/2020/02/02/map-of-effective-altruism/#comment-848255">five posts</a>.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Taymon A. Beal		</title>
		<link>https://slatestarcodex.com/2020/02/02/map-of-effective-altruism/#comment-849687</link>

		<dc:creator><![CDATA[Taymon A. Beal]]></dc:creator>
		<pubDate>Thu, 06 Feb 2020 18:17:41 +0000</pubDate>
		<guid isPermaLink="false">https://slatestarcodex.com/?p=5862#comment-849687</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://slatestarcodex.com/2020/02/02/map-of-effective-altruism/#comment-849501&quot;&gt;argentus&lt;/a&gt;.

I think you may have posted this comment in the wrong thread.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a rel="nofollow"href="https://slatestarcodex.com/2020/02/02/map-of-effective-altruism/#comment-849501">argentus</a>.</p>
<p>I think you may have posted this comment in the wrong thread.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: argentus		</title>
		<link>https://slatestarcodex.com/2020/02/02/map-of-effective-altruism/#comment-849501</link>

		<dc:creator><![CDATA[argentus]]></dc:creator>
		<pubDate>Thu, 06 Feb 2020 14:54:26 +0000</pubDate>
		<guid isPermaLink="false">https://slatestarcodex.com/?p=5862#comment-849501</guid>

					<description><![CDATA[One thought on native people and suicide rates.  I think isolation of place also has something to do with suicide rates of &quot;alienated&quot; rural people.  If you live in Podunk, East Texas, say, it&#039;s comparatively easy to get out of Podunk for the day and go to some town or city of decent size.  There&#039;s also dozens of little towns around you within reasonable distance that collectively have things in them: a restaurant here, a theater there, a nice park there.  That also means there&#039;s more of a chance of some place with decent employment prospects, though you might have to be willing to drive 30-40 miles to get it.  If you live in Podunk, West Texas, it&#039;s entirely possible there&#039;s nothing more exciting than a Wal-Mart for 300 miles in any direction and no job within 75 miles better than working at 7-11.  I use this example because I come from Podunk, East Texas and understand what living in a small, dying rural town with nothing in it is like.  I also know the difference in the sort of abiding melancholy that lingers in a place like my hometown and the raging demons of despair that howl in places like Ft. Stockton in West Texas where there is nothing but boulders and tumbleweeds for miles in all directions.     

Now think of how isolation is even more pronounced in some place like Greenland or the Far North.  This is compounded if you are poor and have no transportation to speak of.  I would be interested to see how suicide rates vary among native people when you consider both the size of the place they live and that place&#039;s relative isolation from other places.]]></description>
			<content:encoded><![CDATA[<p>One thought on native people and suicide rates.  I think isolation of place also has something to do with suicide rates of &#8220;alienated&#8221; rural people.  If you live in Podunk, East Texas, say, it&#8217;s comparatively easy to get out of Podunk for the day and go to some town or city of decent size.  There&#8217;s also dozens of little towns around you within reasonable distance that collectively have things in them: a restaurant here, a theater there, a nice park there.  That also means there&#8217;s more of a chance of some place with decent employment prospects, though you might have to be willing to drive 30-40 miles to get it.  If you live in Podunk, West Texas, it&#8217;s entirely possible there&#8217;s nothing more exciting than a Wal-Mart for 300 miles in any direction and no job within 75 miles better than working at 7-11.  I use this example because I come from Podunk, East Texas and understand what living in a small, dying rural town with nothing in it is like.  I also know the difference in the sort of abiding melancholy that lingers in a place like my hometown and the raging demons of despair that howl in places like Ft. Stockton in West Texas where there is nothing but boulders and tumbleweeds for miles in all directions.     </p>
<p>Now think of how isolation is even more pronounced in some place like Greenland or the Far North.  This is compounded if you are poor and have no transportation to speak of.  I would be interested to see how suicide rates vary among native people when you consider both the size of the place they live and that place&#8217;s relative isolation from other places.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Bugmaster		</title>
		<link>https://slatestarcodex.com/2020/02/02/map-of-effective-altruism/#comment-849440</link>

		<dc:creator><![CDATA[Bugmaster]]></dc:creator>
		<pubDate>Thu, 06 Feb 2020 12:12:12 +0000</pubDate>
		<guid isPermaLink="false">https://slatestarcodex.com/?p=5862#comment-849440</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://slatestarcodex.com/2020/02/02/map-of-effective-altruism/#comment-849403&quot;&gt;Itai Bar-Natan&lt;/a&gt;.

&lt;blockquote&gt;Of course the fact that we don’t have any past experiences of the long-term consequences of technological civilization makes speculation on this topic far more uncertain than asteroid collision&lt;/blockquote&gt;I agree that P(Singularity) is &#060;&#060; P(asteroid), though Scott seems to disagree. That said, P(long-term consequences of technological civilization) &#062; P(asteroid), given that global warming does already exist.

(Amusingly enough, the Singularity reached backwards in time and made my original comment say the opposite of what I intended... Well, either that, or the HTML parser misinterpreted my angle brackets.)]]></description>
			<content:encoded><![CDATA[<p>In reply to <a rel="nofollow"href="https://slatestarcodex.com/2020/02/02/map-of-effective-altruism/#comment-849403">Itai Bar-Natan</a>.</p>
<blockquote><p>Of course the fact that we don’t have any past experiences of the long-term consequences of technological civilization makes speculation on this topic far more uncertain than asteroid collision</p></blockquote>
<p>I agree that P(Singularity) is &lt;&lt; P(asteroid), though Scott seems to disagree. That said, P(long-term consequences of technological civilization) &gt; P(asteroid), given that global warming does already exist.</p>
<p>(Amusingly enough, the Singularity reached backwards in time and made my original comment say the opposite of what I intended&#8230; Well, either that, or the HTML parser misinterpreted my angle brackets.)</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Nornagest		</title>
		<link>https://slatestarcodex.com/2020/02/02/map-of-effective-altruism/#comment-849405</link>

		<dc:creator><![CDATA[Nornagest]]></dc:creator>
		<pubDate>Thu, 06 Feb 2020 06:49:15 +0000</pubDate>
		<guid isPermaLink="false">https://slatestarcodex.com/?p=5862#comment-849405</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://slatestarcodex.com/2020/02/02/map-of-effective-altruism/#comment-848972&quot;&gt;The Nybbler&lt;/a&gt;.

Also because it&#039;s fucking expensive. Effective nation-building is hard even when you haven&#039;t just bombed the whole region into the stone age (which is how we prefer to win wars); when you have, it&#039;s ruinous.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a rel="nofollow"href="https://slatestarcodex.com/2020/02/02/map-of-effective-altruism/#comment-848972">The Nybbler</a>.</p>
<p>Also because it&#8217;s fucking expensive. Effective nation-building is hard even when you haven&#8217;t just bombed the whole region into the stone age (which is how we prefer to win wars); when you have, it&#8217;s ruinous.</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
