<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	
	xmlns:georss="http://www.georss.org/georss"
	xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#"
	>

<channel>
	<title>statistics &#8211; Slate Star Codex</title>
	<atom:link href="https://slatestarcodex.com/tag/statistics/feed/" rel="self" type="application/rss+xml" />
	<link>https://slatestarcodex.com</link>
	<description>SELF-RECOMMENDING!</description>
	<lastBuildDate>Wed, 08 Apr 2020 06:08:04 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.4.1</generator>
	<item>
		<title>Never Tell Me The Odds (Ratio)</title>
		<link>https://slatestarcodex.com/2020/04/07/never-tell-me-the-odds-ratio/</link>
					<comments>https://slatestarcodex.com/2020/04/07/never-tell-me-the-odds-ratio/#comments</comments>
		
		<dc:creator><![CDATA[Scott Alexander]]></dc:creator>
		<pubDate>Wed, 08 Apr 2020 06:07:20 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[statistics]]></category>
		<guid isPermaLink="false">https://slatestarcodex.com/?p=5927</guid>

					<description><![CDATA[[Epistemic status: low confidence, someone tell me if the math is off. Title was stolen from an old Less Wrong post that seems to have disappeared &#8211; let me know if it&#8217;s yours and I&#8217;ll give you credit] I almost &#8230; <a href="https://slatestarcodex.com/2020/04/07/never-tell-me-the-odds-ratio/">Continue reading <span class="pjgm-metanav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p><font size="1"><i>[Epistemic status: low confidence, someone tell me if the math is off. Title was stolen from an old Less Wrong post that seems to have disappeared &#8211; let me know if it&#8217;s yours and I&#8217;ll give you credit]</i></font></p>
<p>I almost screwed up yesterday&#8217;s journal club. The study reported an odds ratio of 2.9 for antidepressants. Even though I <i>knew</i> <A HREF="http://itre.cis.upenn.edu/~myl/languagelog/archives/004767.html">odds ratios are terrible</A> and you should never trust your intuitive impression of them, I <i>still</i> mentally filed this away as &#8220;sounds like a really big effect&#8221;.</p>
<p>This time I was saved by Chen&#8217;s <A HREF="https://sci-hub.tw/10.1080/03610911003650383">How Big is a Big Odds Ratio? Interpreting the Magnitudes of Odds Ratios in Epidemiological Studies</A>, which explains how to convert ORs into effect sizes. Colored highlights are mine. I have followed the usual statistical practice of interpreting effect sizes of 0.2 as &#8220;small&#8221;, of 0.5 as &#8220;moderate&#8221;, and 0.8 as &#8220;large&#8221;, but feeling guilty about it.</p>
<p><center><IMG SRC="http://slatestarcodex.com/blog_images/chentable.png"></center></p>
<p>Based on <A HREF="https://stats.stackexchange.com/questions/352586/convert-odds-ratio-to-cohens-d-taking-rate-of-prevalence-into-account">this page</A>, I gather Chen has used some unusually precise formula to calculate this, but that a quick heuristic is to ignore the prevalence and just take [ln(odds ratio)]/1.81. </p>
<p>Suppose you run a drug trial. In your control group of 1000 patients, 300 get better on their own. In your experimental group of 1000 patients, 600 get better total (presumably 300 on their own, 300 because your drug worked). The <A HREF="https://www.medcalc.org/calc/relative_risk.php">relative risk calculator</A> says your relative risk of recovery on the drug is 2.0. <A HREF="https://www.theanalysisfactor.com/the-difference-between-relative-risk-and-odds-ratios/">Odds ratio</A> is 3.5, effect size is 0.7. So you&#8217;ve managed to double the recovery rate &#8211; in fact, to save an entire extra 30% of your population &#8211; and you <i>still</i> haven&#8217;t qualified for a &#8220;large&#8221; effect size.</p>
<p>The moral of the story is that (to me) odds ratios sound bigger than they really are, and effect sizes sound smaller, so you should be really careful comparing two studies that report their results differently.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://slatestarcodex.com/2020/04/07/never-tell-me-the-odds-ratio/feed/</wfw:commentRss>
			<slash:comments>19</slash:comments>
		
		
			</item>
		<item>
		<title>More Confounders</title>
		<link>https://slatestarcodex.com/2019/06/24/you-need-more-confounders/</link>
					<comments>https://slatestarcodex.com/2019/06/24/you-need-more-confounders/#comments</comments>
		
		<dc:creator><![CDATA[Scott Alexander]]></dc:creator>
		<pubDate>Tue, 25 Jun 2019 06:45:07 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[medicine]]></category>
		<category><![CDATA[science]]></category>
		<category><![CDATA[statistics]]></category>
		<guid isPermaLink="false">https://slatestarcodex.com/?p=5553</guid>

					<description><![CDATA[[Epistemic status: Somewhat confident in the medical analysis, a little out of my depth discussing the statistics] For years, we&#8217;ve been warning patients that their sleeping pills could kill them. How? In every way possible. People taking sleeping pills not &#8230; <a href="https://slatestarcodex.com/2019/06/24/you-need-more-confounders/">Continue reading <span class="pjgm-metanav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p><font size="1"><i>[<b>Epistemic status:</b> Somewhat confident in the medical analysis, a little out of my depth discussing the statistics]</i></font></p>
<p>For years, we&#8217;ve been warning patients that their sleeping pills could kill them. How? <i>In every way possible.</i> People taking sleeping pills not only have higher all-cause mortality. They have higher mortality from every individual cause studied. Death from cancer? Higher. Death from heart disease? Higher. Death from lung disease? Higher. Death from car accidents? Higher. Death from suicide? Higher. Nobody&#8217;s ever proven that sleeping pill users are more likely to get hit by meteors, but nobody&#8217;s ever proven that they <i>aren&#8217;t</i>.</p>
<p>In case this isn&#8217;t scary enough, it only takes a few sleeping pills before your risk of death starts shooting up. Even if you take sleeping pills only a few nights per year, your chance of dying double or triple.</p>
<p>When these studies first came out, doctors were understandably skeptical. First, it seems suspicious that so few sleeping pills could have such a profound effect. Second, why would sleeping pills raise your risk of everything at once? Lung disease? Well, okay, sleeping pills can cause respiratory depression. Suicide? Well, okay, overdosing on sleeping pills is a popular suicide method. Car accidents? Well, sleeping pills can keep you groggy in the morning, and maybe you don&#8217;t drive very well on your way to work. But cancer? Nobody has a good theory for this. Heart disease? Seems kind of weird. Also, there are lots of different kinds of sleeping pills with different biological mechanisms; why should they <i>all</i> cause these effects?</p>
<p>The natural explanation was that the studies were confounded. People who have lots of problems in their lives are more stressed. Stress makes it harder to sleep at night. People who can&#8217;t sleep at night get sleeping pills. Therefore, sleeping pill users have more problems, for every kind of problem you can think of. When problems get bad enough, they kill you. This is why sleeping pill users are more likely to die of everything.</p>
<p>This is a reasonable and reassuring explanation. But people tried to do studies to test it, and the studies kept finding that sleeping pills increased mortality even when adjusted for confounders. Let&#8217;s look at a few of the big ones:</p>
<p><A HREF="https://bmjopen.bmj.com/content/2/1/e000850">Kripke et al 2012</A> followed 10,529 patients and 23,676 controls for an average of 2.5 years. They used a sophisticated de-confounding method which &#8220;controlled for risk factors and [used] up to 116 strata, which exactly matched cases and controls by 12 classes of comorbidity&#8221;. Sleeping pill users still had 3-5x the risk of death, regardless of which of various diverse sedatives they took. Even users in their lowest-exposure category, fewer than 18 pills per year, had 3.6x the mortality rate. Cancer rate in particular increased by 1.35x.</p>
<p><A HREF="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3498427/">Kao et al 2012</A> followed 14,950 patients and 60,000+ matched controls for three years. They tried to match cases and controls by age, sex, and eight common medical and psychiatric comorbidities. They still found that Ambien approximately doubled rates of oral, kidney, esophageal, breast, lung, liver, and bladder cancer, and slightly increased rates of various other types types of cancer as well.</p>
<p><A HREF="https://www.bmj.com/content/348/bmj.g1996">Welch et al 2017</A> took 34,727 patients on sleeping pillsand 69,418 controls and followed them for eight years. They controlled for sex, age, sleep disorders, anxiety disorders, other psychiatric disorders, a measure of general medical morbidity, smoking, alcohol use, medical clinic (as a proxy for socioeconomic status), and prescriptions for other drugs. They also excluded all deaths in the first year of their study to avoid patients who were prescribed sleeping pills for some kind of time-sensitive crisis &#8211; and check the paper for descriptions of some more complicated techniques they used for this. But even with all of these measures in place to prevent confounding, they still found that the patients on sedatives had three times the death rate.</p>
<p>This became one of the rare topics to make it out of the medical journals and into popular consciousness. Time Magazine: <A HREF="http://healthland.time.com/2012/02/28/study-sleeping-pills-linked-with-early-death/">Sleeping Pills Linked With Early Death</A>. AARP: Rest Uneasy: <A HREF="https://blog.aarp.org/healthy-living/rest-uneasy-sleeping-pills-linked-to-early-death-cancer">Sleeping Pills Linked To Early Death, Cancer</A>. The Guardian: <A HREF="https://www.theguardian.com/science/2012/feb/27/sleeping-pills-increase-risk-death-study">Sleeping Pills Increase Risk Of Death, Study Suggests</A>. Most doctors I know are aware of these results, and have at least considered changing their sedative prescribing habits. I&#8217;ve gone back and forth: such high risks are inherently hard-to-believe, but the studies sure do seem pretty good.</p>
<p>This is the context you need to understand Patorno et al 2017: <A HREF="https://www.bmj.com/content/358/bmj.j2941">Benzodiazepines And Risk Of All Cause Mortality In Adults: Cohort Study</A>.</p>
<p>P&#038;a focus on benzodiazepines, a class of sedatives commonly used as sleeping pills, and one of the types of drugs analyzed in the studies above. They do the same kind of analysis as the other studies, using a New Jersey Medicare database to follow 4,182,305 benzodiazepine users and 35,626,849 non-users for nine years. But unlike the other studies, they find minimal to zero difference in mortality risk between users and non-users. Why the difference?</p>
<p>Daniel Kripke, one of the main proponents of the sleeping-pills-are-dangerous hypothesis, thinks it&#8217;s because of the switch from looking at all sleeping pills to looking at benzodiazepines in particular. In a review article, he writes:</p>
<blockquote><p>[Patorno et al] was not included [in this review] because it was not focused on hypnotics, specifically excluded nonbenzodiazepine “Z” drugs such as zolpidem, and failed to compare drug use of cases and controls during follow-ups.</p></blockquote>
<p>I&#8217;m not sure this matters that much. Most of the studies of sleeping pills, including Kripke&#8217;s own study, including benzodiazepines, specifically analyzed them as a separate subgroup, and found they greatly increased mortality risk. For example, <A HREF="https://bmjopen.bmj.com/content/2/1/e000850">Kripke 2012</A> finds that the benzodiazepine sleeping pill temazepam increased death hazard ratio by 3.7x, the same as Ambien and everything else. If Patorno&#8217;s study is right, Kripke&#8217;s study is wrong about benzodiazepines and so (one assumes) probably wrong in the same way about Ambien and everything else. I understand why Kripke might not want to include it in a systematic review with stringent inclusion criteria, but we still have to take it seriously.</p>
<p>He&#8217;s also concerned about the use of an intention-to-treat design. This is where your experimental group is &#8220;anyone who was prescribed medication to begin with&#8221; and your control group is &#8220;anyone who was not prescribed medication to begin with&#8221;. If people switch, they stay in the same group &#8211; for example, someone taking medication stops taking it, they&#8217;re still in the &#8220;taking medication&#8221; group. This is the gold standard for medical research because having people switch groups midstream can introduce extra biases. But if people in the &#8220;taking medication&#8221; group end up taking no more medication than people in the &#8220;not taking medication&#8221; group, obviously it&#8217;s impossible for your study to get a positive finding. So although P&#038;a were justified in using an intention-to-treat design, Kripke is also justified in worrying that it might get the wrong result.</p>
<p>But <A HREF="https://www.bmj.com/content/358/bmj.j2941/rr-6">the authors respond</A> by giving a list of theoretical reasons why they were right to use intention-to-treat, and (more relevantly) repeating their analysis doing the statistics the other way and showing it doesn&#8217;t change the results (see page 10 <A HREF="https://www.bmj.com/sites/default/files/attachments/bmj-article/pre-pub-history/Author%20response%2023.5.17.pdf">here</A>). Also, they point out that some of the studies that did show the large increases in mortality also used intention-to-treat, so this can&#8217;t explain the differences between their studies and previous ones. Overall I find their responses to Dr. Kripke&#8217;s concerns convincing. Also, my priors on a few sleeping pills per year tripling your risk of everything is so low that I&#8217;m biased towards believing P&#038;a.</p>
<p>So why <i>did</i> they get such different results from so many earlier studies? In their response to Kripke, they offer a clear answer:</p>
<p>They adjusted for three hundred confounders.</p>
<p>This is a totally unreasonable number of confounders to adjust for. I&#8217;ve never seen any other study do anything even close. Most other papers in this area have adjusted for ten or twenty confounders. Kripke&#8217;s study adjusted for age, sex, ethnicity, marital status, BMI, alcohol use, smoking, and twelve diseases. Adjusting for nineteen things is impressive. It&#8217;s the sort of thing you do when you really want to cover your bases. Adjusting for 300 different confounders is totally above and beyond what anyone would normally consider.</p>
<p>Reading between the lines, one of the P&#038;a co-authors was Robert Glynn, a Harvard professor of statistics who <A HREF="http://sci-hub.tw/10.1097/EDE.0b013e3181a663cc">helped develop an algorithm</A> that automatically identifies massive numbers of confounders to form a &#8220;propensity score&#8221;, then adjusts for it. The P&#038;a study was one of the first applications of the algorithm on a controversial medical question. It looks like this study was partly intended to test it out. And it got the opposite result from almost every past study in this field.</p>
<p>I don&#8217;t know enough to judge the statistics involved. I can imagine ways in which trying to adjust out so many things might cause some form of overfitting, though I have no evidence this is actually a concern. And I don&#8217;t want to throw out decades of studies linking sleeping pills and mortality just because one contrary study comes along with a fancy new statistical gadget.</p>
<p>But I think it&#8217;s important to notice: if they&#8217;re right, everyone else is wrong. If you&#8217;re using a study design that controls for things, you&#8217;re operating on an assumption that you have a pretty good idea what things are important to control for, and that if you control for the ten or twenty most important ones you can think of then that&#8217;s enough. If P&#038;a are right (and again, I don&#8217;t want to immediately jump to that conclusion, but it seems plausible) then this assumption is wrong. At least it&#8217;s wrong in the domain of benzodiazepine prescription and mortality. Who knows how many other domains it might be wrong in? Everyone who tries to &#8220;control for confounders&#8221; who isn&#8217;t using something at least as good as P&#038;a&#8217;s algorithm isn&#8217;t up to the task they&#8217;ve set themselves, and we should doubt their results (also, <A HREF="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0152719">measurement issues</A>!)</p>
<p>This reminds me of <A HREF="https://slatestarcodex.com/2019/05/07/5-httlpr-a-pointed-review/">how a lot of</A> the mysteries that troubled geneticists in samples of 1,000 or 5,000 people suddenly disappeared <A HREF="https://slatestarcodex.com/2018/09/13/the-omnigenic-model-as-metaphor-for-life/">once they got samples of</A> 100,000 or 500,000 people. Or how a lot of seasonal affective disorder patients who don&#8217;t respond to light boxes <A HREF="https://meaningness.com/metablog/sad-light-lumens">will anecdotally</A> <A HREF="https://twitter.com/meaningness/status/936626851090333697">respond to</A> gigantic really really unreasonably bright light boxes. Or of  <A HREF="https://www.lesswrong.com/posts/z8usYeKX7dtTWsEnk/more-dakka">lots of things</A>, really.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://slatestarcodex.com/2019/06/24/you-need-more-confounders/feed/</wfw:commentRss>
			<slash:comments>167</slash:comments>
		
		
			</item>
		<item>
		<title>Does Reality Drive Straight Lines On Graphs, Or Do Straight Lines On Graphs Drive Reality?</title>
		<link>https://slatestarcodex.com/2019/03/13/does-reality-drive-straight-lines-on-graphs-or-do-straight-lines-on-graphs-drive-reality/</link>
					<comments>https://slatestarcodex.com/2019/03/13/does-reality-drive-straight-lines-on-graphs-or-do-straight-lines-on-graphs-drive-reality/#comments</comments>
		
		<dc:creator><![CDATA[Scott Alexander]]></dc:creator>
		<pubDate>Thu, 14 Mar 2019 00:52:57 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[statistics]]></category>
		<guid isPermaLink="false">https://slatestarcodex.com/?p=5423</guid>

					<description><![CDATA[Here&#8217;s a graph of US air pollution over time: During the discussion of 90s environmentalism, some people pointed out that this showed the Clean Air Act didn&#8217;t matter. The trend is the same before the Act as after it. This &#8230; <a href="https://slatestarcodex.com/2019/03/13/does-reality-drive-straight-lines-on-graphs-or-do-straight-lines-on-graphs-drive-reality/">Continue reading <span class="pjgm-metanav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>Here&#8217;s a graph of US air pollution over time:</p>
<p><center><IMG SRC="http://slatestarcodex.com/blog_images/drive_cleanair1990.png"></center></p>
<p>During the discussion of <A HREF="https://slatestarcodex.com/2019/01/01/what-happened-to-90s-environmentalism/">90s environmentalism</A>, some people pointed out that this showed the Clean Air Act didn&#8217;t matter. The trend is the same before the Act as after it.</p>
<p>This kind of argument is common. For example, here&#8217;s the libertarian Mercatus Institute <A HREF="https://www.mercatus.org/publication/evaluating-oshas-effectiveness-and-suggestions-reform">arguing that OSHA didn&#8217;t help workplace safety</A>:</p>
<p><center><IMG SRC="http://slatestarcodex.com/blog_images/drive_osha.png"></center></p>
<p>I&#8217;ve always taken these arguments pretty seriously. But recently I&#8217;ve gotten more cautious. Here&#8217;s a graph of Moore&#8217;s Law, the &#8220;rule&#8221; that transistor counts will always increase by a certain amount per year:</p>
<p><center><IMG SRC="http://slatestarcodex.com/blog_images/drive_moore.png"></center></p>
<p>The Moore&#8217;s Law Wikipedia article lists factors that have helped transistors keep shrinking during that time, for example &#8220;the invention of deep UV excimer laser photolithography&#8221; in 1980. But if we wanted to be really harsh, we could make a graph like this:</p>
<p><center><IMG SRC="http://slatestarcodex.com/blog_images/drive_laser.png"></center></p>
<p>But the same argument that disproves the importance of photolithography disproves the importance of anything else. We&#8217;d have to retreat to a thousand-coin-flips model where each factor is so small that it happening or not happening at any given time doesn&#8217;t change the graph in a visible way.</p>
<p>The only satisfying counterargument I&#8217;ve heard to this is that Moore&#8217;s Law comes from a combination of physical law and human commitment. Physical law is consistent with transistors shrinking this quickly. But having noticed this, humans (like the leadership of Intel) commit to achieve it. That commitment functions kind of as a control system. If there&#8217;s a big advance in one area, they can relax a little bit in other areas. If there&#8217;s a problem in one area, they&#8217;ll pour more resources into it until there stops being a problem. One can imagine an event big enough to break the control system &#8211; a single unexpected discovery that cuts sizes by a factor of 1000 all on its own, or a quirk of physical law that makes it impossible to fit more transistors on a chip without inventing an entirely new scientific paradigm. But in fact there was no event big enough to break the control system during this period, so the system kept working.</p>
<p>But then we have to wonder whether other things like clean air are control systems too.</p>
<p>That is, suppose that as the economy improves and stuff, the American people demand cleaner air. They will only be happy if the air is at least 2% cleaner each year than the year before. If one year the air is 10% cleaner than the year before, environmentalist groups get bored and wander off, and there&#8217;s no more progress for the next five years. But if one year the air is only 1% cleaner, newly-energized environmentalist voters threaten to vote out all the incumbents who contributed to the problem, and politicians pass some emergency measure to make it go down another 1%. So absent some event strong enough to overwhelm the system, air pollution will always go down 2% per year. But that doesn&#8217;t mean the Clean Air Act didn&#8217;t change things! The Clean Air Act was part of the toolkit that the control system used to keep the decline at 2%. If the Clean Air Act had never happened, the control system would have figured out some other way to keep air pollution low, but that doesn&#8217;t mean the Clean Air Act didn&#8217;t matter. Just that it mattered exactly as much as whatever it would have been replaced with.</p>
<p>If this were true, you wouldn&#8217;t see the effects of pollution-busting technologies on pollution. You&#8217;d see them on everything else. For example, suppose that absent any other progress on air pollution, politicians would regulate cars harder, and that&#8217;s what would make air pollution go down by 2% that year. In that case, the effects of inventing an unexpected new pollution-busting technology wouldn&#8217;t appear in pollution levels, they would appear in car prices. Unless car prices are also governed by a control system &#8211; maybe car companies have a target of keeping costs below $20,000 per car, and so they would skimp on safety in order to bring prices back down, and then the effects of a new anti-pollution technology would appear in car accident fatality rates.</p>
<p>How do we tell the difference between this world, and the world where the Clean Air Act <i>really</i> doesn&#8217;t matter? I&#8217;m not sure (does anyone know of research on this?). Maybe this is one of those awful situations where you have use common sense instead of looking at statistics.</p>
<p>I&#8217;m worried this could be a fully general excuse to dismiss any evidence that a preferred policy didn&#8217;t work. But it does make me at least a <i>little</i> slower to believe arguments based on interventions not changing trends.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://slatestarcodex.com/2019/03/13/does-reality-drive-straight-lines-on-graphs-or-do-straight-lines-on-graphs-drive-reality/feed/</wfw:commentRss>
			<slash:comments>155</slash:comments>
		
		
			</item>
		<item>
		<title>In Search Of Missing US Suicides</title>
		<link>https://slatestarcodex.com/2018/05/31/in-search-of-missing-us-suicides/</link>
					<comments>https://slatestarcodex.com/2018/05/31/in-search-of-missing-us-suicides/#comments</comments>
		
		<dc:creator><![CDATA[Scott Alexander]]></dc:creator>
		<pubDate>Thu, 31 May 2018 21:13:28 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[psychiatry]]></category>
		<category><![CDATA[statistics]]></category>
		<category><![CDATA[too many graphs]]></category>
		<guid isPermaLink="false">http://slatestarcodex.com/?p=4941</guid>

					<description><![CDATA[[Content warning: suicide. Thanks to someone on Twitter I forget for alerting me to this question] Among US states, there&#8217;s a clear relationship between gun ownership rates and suicide rates, but not between gun ownership rates and homicide rates: You &#8230; <a href="https://slatestarcodex.com/2018/05/31/in-search-of-missing-us-suicides/">Continue reading <span class="pjgm-metanav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p><font size="1"><i>[Content warning: suicide. Thanks to someone on Twitter I forget for alerting me to this question]</i></font></p>
<p>Among US states, there&#8217;s a clear relationship between gun ownership rates and suicide rates, but not between gun ownership rates and homicide rates:</p>
<p><center><IMG SRC="http://slatestarcodex.com/blog_images/gunsuicide1.png"></center></p>
<p>You might conclude guns increase suicides but not homicides. Then you might predict that the gun-loving US would be an international outlier in suicides but not homicides. In fact, it&#8217;s the opposite:</p>
<p><center><IMG SRC="http://slatestarcodex.com/blog_images/gunsuicide2.png"></center></p>
<p>Why should this be?</p>
<p>We&#8217;ve <A HREF="http://slatestarcodex.com/2016/01/06/guns-and-states/">already discussed</A> why US homicide rates are so high. But why isn&#8217;t the suicide rate elevated?</p>
<p>One possibility: suicide methods are fungible. If guns are easily available, you might use a gun; if not, you might overdose, hang yourself, or jump off a bridge. So getting rid of one suicide method or another doesn&#8217;t do much.</p>
<p>This sounds plausible, but it&#8217;s the opposite of scientific consensus on the subject. See for example <A HREF="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3290984/">Controlling Access To Suicide Means</A>, which says that &#8220;restrictions of access to common means of suicide has lead to lower overall suicide rates, particularly regarding suicide by firearms in USA, detoxification of domestic and motor vehicle gas in England and other countries, toxic pesticides in rural areas, barriers at jumping sites and hanging&#8230;&#8221; This is particularly brought up in the context of US gun control &#8211; see eg <A HREF="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3518361/">Suicide, Guns, and Public Policy</A>, which describes &#8220;strong empirical evidence that restriction of access to firearms reduces suicides&#8221;. </p>
<p>The state-level data from above support this view &#8211; taking guns away from a state <i>does</i> decrease its suicide rate. And then there&#8217;s this graph, from <A HREF="https://www.armedwithreason.com/suicides-the-missing-movement/">Armed With Reason</A>:</p>
<p><center><IMG SRC="http://slatestarcodex.com/blog_images/gunsuicide3.png"></center></p>
<p>&#8230;which shows that adding more guns to a state does not decrease its nonfirearm suicide rate.</p>
<p>But if suicide methods aren&#8217;t fungible, then why doesn&#8217;t the US have higher suicide rates? Here&#8217;s another way of asking this question:</p>
<p><center><IMG SRC="http://slatestarcodex.com/blog_images/nongun_suicide.png"></center></p>
<p>The US has fewer nongun suicides than anywhere else. The seemingly obvious explanation is that guns are so common that everyone who wants to commit suicide is using guns, decreasing the non-gun rate. But that contradicts all the nonfungibility evidence above. So the other possibility is that the US ought to have an very low suicide rate, and it&#8217;s just all our guns that are bringing us back up to average. </p>
<p>Of all US states, Massachusetts, New Jersey, and Hawaii have the fewest guns. Unsurprisingly, suicides in these states are less likely than average to be committed with firearms. In <A HREF="http://efsgv.org/wp-content/uploads/2018/01/Firearm-Suicide-in-Massachusetts_February-2018_Final.pdf">MA</A>, the rate is 22%; in <A HREF="http://efsgv.org/wp-content/uploads/2017/12/FINAL-Firearm-Suicide-in-New-Jersey-November-2017-1.pdf">NJ</A> 24%; in HI, 20%. Their suicide rates are 8.8, 7.2, and 12.1, respectively. Hawaii has an unusual ethnic composition &#8211; 40% Asian and 20% Native Hawaiian, both groups with high suicide rates (see eg the suicide rate for Japan above). So it might be worth taking Massachusetts and New Jersey as examples to look at in more detail.</p>
<p>Either state, if it were independent, would be among the lowest-suicide-rate developed nations. And both <i>still</i> have more guns than our comparison countries. If we did a really simple linear extrapolation from New Jersey-level gun control to imagine a state where firearms were as restricted as in Britain, we would expect it to have a suicide rate of around 5 or 6 &#8211; which is around the current level of non-gun US suicides. This is much lower than any of the large comparison countries in the graph above, but there are two developed countries currently around this level &#8211; Italy and Israel. I think it makes sense to suppose that the US might have a low Italy/Israel-style base rate of suicides. </p>
<p>For one thing, it&#8217;s unusually religious for a developed country. Religion is one of the strongest <A HREF="https://academic.oup.com/aje/article/155/5/413/171404">protective factors</A> against suicide. This also seems like a good explanation for Italy and Israel.</p>
<p>For another, it&#8217;s culturally similar to Britain, which also has a low suicide rate somewhere in the 7s. Other British colonies don&#8217;t seem to have kept this effect &#8211; Australia and Canada are both higher &#8211; but maybe the US did. </p>
<p>And for another, it&#8217;s unusually ethnically diverse. Blacks and Hispanics have <A HREF="https://www.sprc.org/racial-ethnic-disparities">only about half</A> the suicide rate of whites; which means you would expect the US to be less suicidal than Europe. I previously believed this was because whites had more guns, but this doesn&#8217;t seem to be true: <A HREF="http://annals.org/aim/fullarticle/2679556/comparison-rates-firearm-nonfirearm-homicide-suicide-black-white-non-hispanic">Riddell et al</A> find that whites have higher non-firearm suicide rates too. So this could be an additional factor driving US rates down.</p>
<p>(another conclusion from the graphs above: US whites &#8211; who have most of the guns &#8211; <i>do</i> have an anomalously high suicide rate compared to other countries)</p>
<p>A confounding factor &#8211; the US has lots of different cultures, and Massachusetts and New Jersey represent only one of them. But if anything I would expect Southern and Midwestern culture, which are more religious, to have a lower base suicide rate; the South also has a lower percent white, another reason to expect their rate to be lower. And there is no evidence of these states having a higher non-firearm suicide rate, which we might expect if they were unusually suicidal.</p>
<p>So I think the simplest explanation is true. A gun-free US would have one of the lowest suicide rates in the developed world, maybe 5 or 6 people per hundred thousand. The US&#8217; average-seeming suicide rate is an artifact caused by combining the low base with the distorting effects of high gun availability. The lack of a relative suicide crisis in the US doesn&#8217;t indicate that easy firearm access isn&#8217;t causing thousands of preventable suicides per year.</p>
<p>This is maybe not the most pressing question we&#8217;re facing right now, but I take it as a warning against <A HREF="http://slatestarcodex.com/2018/05/08/varieties-of-argumentative-experience/">gotcha</A>-style debating. A simple bar graph comparing national suicide and homicide rates would be a compelling, elegant, and easily-digested argument that guns increased homicides but not suicides. It would also be totally wrong.</p>
<p>[<b>EDIT:</b> Commenters point out <A HREF="https://marginalrevolution.com/marginalrevolution/2013/11/firearms-and-suicides-in-us-states.html">this paper by Alex Tabarrok</A> on how there is some, but less-than-perfect, substitutability of suicide methods.]</p>
]]></content:encoded>
					
					<wfw:commentRss>https://slatestarcodex.com/2018/05/31/in-search-of-missing-us-suicides/feed/</wfw:commentRss>
			<slash:comments>224</slash:comments>
		
		
			</item>
		<item>
		<title>Why DC&#8217;s Low Graduation Rates?</title>
		<link>https://slatestarcodex.com/2018/04/10/why-dcs-low-graduation-rates/</link>
					<comments>https://slatestarcodex.com/2018/04/10/why-dcs-low-graduation-rates/#comments</comments>
		
		<dc:creator><![CDATA[Scott Alexander]]></dc:creator>
		<pubDate>Wed, 11 Apr 2018 03:38:09 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[education]]></category>
		<category><![CDATA[statistics]]></category>
		<guid isPermaLink="false">http://slatestarcodex.com/?p=4866</guid>

					<description><![CDATA[[Some changes to the conclusions in this post; see edit at the end and entry 21 on Mistakes page] US News: DC Schools Brace For Catastrophic Drop In Graduation Rates. &#8220;Catastrophic&#8221; isn&#8217;t hyperbole; the numbers are expected to drop from &#8230; <a href="https://slatestarcodex.com/2018/04/10/why-dcs-low-graduation-rates/">Continue reading <span class="pjgm-metanav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p><font size="1"><i>[Some changes to the conclusions in this post; see edit at the end and entry 21 on Mistakes page]</i></font></p>
<p>US News: <A HREF="https://www.usnews.com/news/education-news/articles/2018-03-02/dc-schools-brace-for-catastrophic-drop-in-graduation-rates">DC Schools Brace For Catastrophic Drop In Graduation Rates</A>. &#8220;Catastrophic&#8221; isn&#8217;t hyperbole; the numbers are expected to drop from 73% (close to the national average of 83%) all the way down to 42%.</p>
<p>There&#8217;s no debate about why this is happening &#8211; it&#8217;s because the previous graduation rate was basically fraudulent, inflated by pressure to show that recent &#8220;reforms&#8221; were working. Last year there was a big investigation, all the investigators agreed it was fraudulent, DC agreed to do a little less fraud this year, and this is the result. It&#8217;s pretty damning, given how everybody was praising the reforms and holding them up as a national model and saying this proved that Tough But Fair Education Policy could make a difference:</p>
<blockquote><p>As far as scandals in the education policy world go, D.C. schools so profoundly miscalculating graduation rates at a time when the high-profile school district had been so self-laudatory about its achievements may be difficult to top [&#8230;] Indeed, when Michelle Rhee took the reins of the flailing school system a decade ago, it galvanized the education reform movement, which had just begun blossoming around the country, and ushered in a host of controversial changes that included the shuttering of multiple schools, firing of hundreds of teachers and the institution of new teacher evaluation and compensation models.</p>
<p>The changes not only dramatically altered the local political landscape in Washington but also shined a national spotlight on D.C. schools that prompted other urban school districts and education policy researchers to consider the nation&#8217;s capital a bellwether for the entire education reform movement.</p></blockquote>
<p>Well, darn.</p>
<p>But the interesting bit isn&#8217;t just that DC schools are doing worse than we thought. It&#8217;s that DC schools are doing amazingly, uniquely, abysmally bad, below what should even be possible. We make fun of states like Mississippi and Alabama, but both have graduation rates around 80%. The lowest graduation rate in any of the fifty states is in Oregon, which still has 69%. And we are being told DC is 42%!</p>
<p>When we discussed this in the last links thread, people had a couple of explanations:</p>
<p><b>1</b>. Washington DC has a terrible school system, with uniquely incompetent administrators.</p>
<p><b>2</b>. Washington DC is poorer, blacker, and more segregated than any other state, and that leads to unique challenges other school systems don&#8217;t face. Even though everyone is doing their best, they face insurmountable structural difficulties.</p>
<p><b>3</b>. Maybe the fraud was so bad that DC over-corrected, and now has stricter standards than anywhere else.</p>
<p>Which of these is most important? </p>
<p>Let&#8217;s start by looking at test scores. Here&#8217;s a sample of DC&#8217;s NAEP scores compared to some other states (full list is <A HREF="https://www.nationsreportcard.gov/profiles/stateprofile/overview/DC?cti=PgTab_GapComparisons&#038;chort=2&#038;sub=MAT&#038;sj=DC&#038;fs=Grade&#038;st=MN&#038;year=2015R3&#038;sg=Race%2FEthnicity%3A+White+vs.+Black&#038;sgv=Difference&#038;ts=Single+Year&#038;tss=2015R3-2015R3&#038;sfj=NP">here</A>).</p>
<p><center><IMG SRC="http://slatestarcodex.com/blog_images/dc1.png"></center></p>
<p>In both reading and math, in all grades, DC does abysmally. But they don&#8217;t do <i>as</i> abysmally as a 42% graduation rate might predict. They are sometimes last, sometimes second- or third- to last, and in any case they&#8217;re rarely that different from other low-performers like Alabama and Mississippi. They might be best described as a member in good standing of the lowest-performing tier of US states.</p>
<p>But there are <A HREF="https://www.brookings.edu/blog/brown-center-chalkboard/2016/06/06/7-findings-that-illustrate-racial-disparities-in-education/">massive racial inequalities in education</A>, and DC is by far the blackest &#8220;state&#8221; in the nation. How does it do when we adjust for this?</p>
<p><center><IMG SRC="http://slatestarcodex.com/blog_images/dc2.png"></center></p>
<p>The full table is <A HREF="https://www.nationsreportcard.gov/profiles/stateprofile/overview/DC?cti=PgTab_GapComparisons&#038;chort=2&#038;sub=MAT&#038;sj=DC&#038;fs=Grade&#038;st=MN&#038;year=2015R3&#038;sg=Race%2FEthnicity%3A+White+vs.+Black&#038;sgv=%3F&#038;ts=Single+Year&#038;tss=2015R3&#038;sfj=NP">here</A>. DC has <i>by far</i> the highest white test scores in the country &#8211; probably because a lot of its white students are the kids of well-off bureaucrats and think-tank types. And its black test scores range from lowest-tier-member to mediocre.</p>
<p>Nor can this just be a Simpson&#8217;s Paradox, where both white and black students do fine but the difference is driven by a greater number of black students. The US average black graduation rate is 68%. No state has a black graduation rate lower than 56%. DC &#8211; again &#8211; is supposed to have a 42% total graduation rate.</p>
<p>This seems to refute hypotheses 1 and 2 &#8211; that DC just has a terrible school system, or just has an unusually disadvantaged population. Its white students do very well. Its black students do poorly but not too much worse than they would in other states. There is no gap &#8211; among either race or among both combined &#8211; that corresponds to the gap between Mississippi&#8217;s graduation rate (total 80%, black-only 77%), and DC&#8217;s graduation rate of 42%.</p>
<p>This leaves us with hypothesis 3 &#8211; that DC got burned so badly in the fraud investigations that its standards are now much higher than anywhere else&#8217;s. Maybe a graph will help:</p>
<p><center><IMG SRC="http://slatestarcodex.com/blog_images/dc7.png"></center></p>
<p>The horizontal axis is each state&#8217;s test scores &#8211; specifically the average of its 8th grade NAEP reading and math. The vertical axis is its graduation rate. The previous, supposedly fraudulent DC rate clusters together more or less with everybody else. The new, supposedly non-fraudulent rate is an extreme outlier, showing a graduation rate about 20 percentage points lower than test scores would predict.</p>
<p>So DC&#8217;s old graduation rate was normal relative to their test scores, and their new graduation rate is an outlier. But their old graduation rate is widely considered to have been maintained by fraud and really low standards, and their new graduation rate is widely considered correct. Does that mean that everywhere else with the same levels of poverty and segregation as DC also uses fraud and really low standards to keep their graduation rates up?</p>
<p>Maybe. Detroit is often used as a symbol of inner-city educational dysfunction, but <A HREF="https://www.freep.com/story/news/local/michigan/2015/03/05/michigan-graduation-rates/24443003/">even the district with the worst Detroit schools</A> has a 61.5% graduation rate. How do they do it? Given that <A HREF="https://www.freep.com/story/news/education/2016/08/30/detroit-mstep-sat-scores/89557200/">fewer than 5%</A> of their students pass exams, I assume they do it through fraud and really low standards. Los Angeles? <A HREF="http://www.capoliticalreview.com/capoliticalnewsandviews/lausd-fraud-on-students-ending-exit-exam-raises-graduation-rate/">Fraud</A> and <A HREF="http://www.latimes.com/local/education/la-me-lausd-20150610-story.html">really low standards</A>. Chicago? <A HREF="http://www.chicagotribune.com/news/ct-cps-inspector-general-report-met-20161214-story.html">Fraud</A> and <A HREF="https://www.theatlantic.com/education/archive/2015/07/chicago-graduation-rates/397736/">really low standards</A>. Baltimore? Given stories <A HREF="http://foxbaltimore.com/news/project-baltimore/teachers-stunned-by-project-baltimore-discovery-i-dont-think-its-possible">like the one</A> where one of the city&#8217;s highest-graduation-rate schools has zero percent of students score at &#8220;meets expectations&#8221; or even &#8220;approaches expectations&#8221; on statewide exams, it looks like fraud and really low standards.</p>
<p>I understand this is a really strong claim. But <A HREF="https://www.usatoday.com/story/opinion/2018/02/17/dont-assume-high-school-graduation-scandal-only-dc-nat-malkus-column/332727002/">others seem to agree</A>, and it&#8217;s the only way I can make sense of DC&#8217;s abysmally low projected graduation rates, in the context of their merely-awful exam scores.</p>
<p>Or maybe the key word is &#8220;projected&#8221;. From the US News article:</p>
<blockquote><p>The <A HREF="https://dcps.dc.gov/sites/default/files/dc/sites/dcps/page_content/attachments/2018-02-28_ACGRMemo_final.pdf">report</A> estimated that just 42 percent of seniors are on track to graduate at the end of the current school year, down from 73 percent who graduated in the 2016-17 school year. It noted that 19 percent of students were &#8220;moderately off-track&#8221; and could still earn enough credits to graduate.</p></blockquote>
<p>Plausibly, kindly school officials will find loopholes that allow all of those 19% of moderately-off-track children to graduate, DC&#8217;s actual graduation rate will be 62% (right on the trend line for the graph above), and this whole episode will be remembered as &#8220;that one time a scary report underestimating the graduation rate came out&#8221;. Plausibly, this is the main way that Detroit and Los Angeles and all those other cities keep their rates up, and the more dramatic scare stories are just that.</p>
<p>I hope this happens. Think how unfair it will be for DC students if it doesn&#8217;t. Somebody who would graduate comfortably from any other high school in the country will be held back because they happen to have been in DC the year it decided to enforce standards nobody else enforced. If <A HREF="https://reason.com/volokh/2018/03/24/bryan-caplans-case-against-education">the true value of education is signaling</A>, then the most important thing a school district can do is make sure it&#8217;s speaking the same signaling-language as everyone else. Probably somebody should fix the system in general, but that needs to happen on a national level if it&#8217;s not going to leave thousands of unfairly-failed children as collateral damage.</p>
<p><b>[EDIT: More discussion at <A HREF="http://slatestarcodex.com/2018/04/12/highlights-from-the-comments-on-dc-graduation-rates/">Highlights From The Comments On DC Graduation Rates</A>. Main update is that I underestimated the importance of absences, which are what&#8217;s causing a lot of the non-graduations, which there might be more of in DC, and which DC might be stricter about than other areas.]</b></p>
]]></content:encoded>
					
					<wfw:commentRss>https://slatestarcodex.com/2018/04/10/why-dcs-low-graduation-rates/feed/</wfw:commentRss>
			<slash:comments>296</slash:comments>
		
		
			</item>
		<item>
		<title>SSC Journal Club: Cipriani On Antidepressants</title>
		<link>https://slatestarcodex.com/2018/02/26/ssc-journal-club-cipriani-on-antidepressants/</link>
					<comments>https://slatestarcodex.com/2018/02/26/ssc-journal-club-cipriani-on-antidepressants/#comments</comments>
		
		<dc:creator><![CDATA[Scott Alexander]]></dc:creator>
		<pubDate>Tue, 27 Feb 2018 01:05:39 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[journal club]]></category>
		<category><![CDATA[psychiatry]]></category>
		<category><![CDATA[statistics]]></category>
		<guid isPermaLink="false">http://slatestarcodex.com/?p=4819</guid>

					<description><![CDATA[I. The big news in psychiatry this month is Cipriani et al&#8217;s Comparative efficacy and acceptability of 21 antidepressant drugs for the acute treatment of adults with major depressive disorder: a systematic review and network meta-analysis. It purports to be &#8230; <a href="https://slatestarcodex.com/2018/02/26/ssc-journal-club-cipriani-on-antidepressants/">Continue reading <span class="pjgm-metanav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p><b>I.</b></p>
<p>The big news in psychiatry this month is Cipriani et al&#8217;s <A HREF="http://www.thelancet.com/journals/lancet/article/PIIS0140-6736(17)32802-7/fulltext">Comparative efficacy and acceptability of 21 antidepressant drugs for the acute treatment of adults with major depressive disorder: a systematic review and network meta-analysis</A>. It purports to be the last word in the &#8220;do antidepressants work?&#8221; question, and a first (or at least early) word in the under-asked &#8220;which antidepressants are best?&#8221; question.</p>
<p>This study is very big, very sophisticated, and must have taken a very impressive amount of work. It meta-analyzes virtually every RCT of antidepressants ever done &#8211; 522 in all &#8211; then throws every statistical trick in the book at them to try to glob together into a coherent account of how antidepressants work. It includes Andrea Cipriani, one of the most famous research psychiatrists in the world &#8211; and John Ioannidis, one of the most famous statisticians. It&#8217;s been covered in news sources around the world: my favorite headline is Newsweek&#8217;s unsubtle <A HREF="http://www.newsweek.com/antidepressants-major-depressive-disorder-study-815415">Antidepressants Do Work And Many More People Should Take Them</A>, but honorable mention to Reuters&#8217; <A HREF="https://www.reuters.com/article/us-health-antidepressants/study-seeks-to-end-antidepressant-debate-the-drugs-do-work-idUSKCN1G52XX">Study Seeks To End Antidepressant Debate: The Drugs Do Work</A>.</p>
<p>Based on the whole &#8220;we&#8217;ve definitely proven antidepressants work&#8221; vibe in coverage, you would think that they&#8217;d directly contradicted Irving Kirsch&#8217;s claim that antidepressants aren&#8217;t very effective. I&#8217;ve mentioned <A HREF="http://slatestarcodex.com/2014/07/07/ssris-much-more-than-you-wanted-to-know/">my disagreements with Kirsch</A> before, but it would be nice to have a definitive refutation of his work. This study isn&#8217;t really it. Both Kirsch and Cipriani agree that antidepressants have statistical significance &#8211; they&#8217;re not literally doing nothing. The main debate was whether they were good enough to be worth it. Kirsch argues they aren&#8217;t, using a statistic called &#8220;effect size&#8221;. Cipriani uses a different statistic called &#8220;odds ratio&#8221; that is hard to immediately compare.</p>
<p><b>[EDIT: Commenters <A HREF="https://slatestarcodex.com/2018/02/26/ssc-journal-club-cipriani-on-antidepressants/#comment-605866">point out</A> that once you convert Cipriani&#8217;s odds ratios to effect sizes, the two studies are pretty much the same &#8211; in fact, Cipriani&#8217;s estimates are (slightly) <i>lower</i>. That is, &#8220;the study proving antidepressants work&#8221; presents a worse picture of antidepressants than &#8220;the study proving antidepressants don&#8217;t work&#8221;. If I had realized this earlier, this would have been the lede for this article. This makes all the media coverage of this study <i>completely insane</i> and means we&#8217;re doing science based entirely on how people choose to sum up their results. Strongly recommend this <A HREF="http://blogs.discovermagazine.com/neuroskeptic/2018/02/24/about-antidepressant-study/#.WpU57-eIbOg">Neuroskeptic article</A> on the topic. This is very important and makes the rest of this article somewhat trivial in comparison.]</b></p>
<p>Kirsch made a big deal of trying to get all the evidence, not just the for-public-consumption pharma-approved data. Cipriani also made such an effort, but I&#8217;m not sure how comparable the two are. Kirsch focused on FDA trials of six drugs. Cipriani took every trial ever published &#8211; FDA, academia, industry, whatever- of twenty-one drugs. Kirsch focused on using the Freedom Of Information Act to obtain non-public data from various failed trials. Cipriani says he looked pretty hard for unpublished data, but he might not have gone so far as to harass government agencies. Did he manage to find as many covered-up studies as Kirsch did? Unclear.</p>
<p>How confident should we be in the conclusion? These are very good researchers and their methodology is unimpeachable. But a lot of the 522 studies they cite are, well, kind of crap. The researchers acknowledge this and have constructed some kind of incredibly sophisticated model that inputs the chance of bias in each study and weights everything and simulates all sorts of assumptions to make sure they don&#8217;t change the conclusions too much. But we are basically being given a giant edifice of suspected-crap fed through super-powered statistical machinery meant to be able to certify whether or not it&#8217;s safe.</p>
<p>Of particular concern, 78% of the studies they cite are sponsored by pharmaceutical industries. The researchers run this through their super-powered statistical machinery and determine that this made no difference &#8211; in fact, if you look in the <A HREF="http://www.thelancet.com/cms/attachment/2119023008/2088154696/mmc1.pdf">supplement</A>, the size of the effect was literally zero:</p>
<blockquote><p>In our analyses, funding by industry was not associated with substantial differences in terms of response or dropout rates. However, non-industry funded trials were few and many trials did not report or disclose any funding.</p></blockquote>
<p>This is surprising, since <A HREF="https://www.ncbi.nlm.nih.gov/pubmed/16199844?dopt=Abstract">other papers</A> (which the researchers dutifully cite) find that pharma-sponsored trials are about five times more likely to get positive results than non-sponsored ones (though see <A HREF="https://slatestarcodex.com/2018/02/26/ssc-journal-club-cipriani-on-antidepressants/#comment-605843">this comment</A>). Cipriani&#8217;s excuse is that there weren&#8217;t enough non-industry trials to really get a good feel for the differences, and that a lot of the trials marked &#8220;non-industry&#8221; were probably secretly by industry anyway (more on this later). Fair enough, but if we can&#8217;t believe their &#8220;sponsorship makes zero difference to outcome&#8221; result, then the whole thing starts seeming kind of questionable. </p>
<p>I don&#8217;t want to come on too strong here. Science is never supposed to have to wait for some impossible perfectly-unbiased investigator. It&#8217;s supposed to accept that everyone will have an agenda, but strive through methodological rigor, transparency, and open debate to transcend those agendas and create studies everyone can believe. On the other hand, we&#8217;re really not very good at that yet, and nobody ever went broke overestimating the deceptiveness of pharmaceutical companies.</p>
<p>And there was one other kind of bias that did show up, hard. When a drug was new and exciting, it tended to do better in studies. When it was old and boring, it tended to do worse. You could argue this is a placebo effect on the patients, but I&#8217;m betting it&#8217;s a sign that people were able to bias the studies to fit their expected results (excited high-tech thing is better) in ways we&#8217;re otherwise not catching.</p>
<p>All of this will go double as we start looking at the next part, the ranking of different antidepressants.</p>
<p><b>II.</b></p>
<p><center><IMG SRC="http://slatestarcodex.com/blog_images/cipriani1.png"></center></p>
<p>All antidepressants, from best to worst! (though note the wide error bars)</p>
<p>If this were for real, it would be an amazing resource. Psychiatrists have longed to know if any antidepressant is truly better than any other. Now that we know this, should we just start everyone on amitriptyline (or mirtazapine if we&#8217;re worried about tricyclic side effects) and throw out the others?</p>
<p>(as a first line, of course. In reality, we try the best one first, but keep going down the list until we find one that works for you and your unique genetic makeup.)</p>
<p>This matches some parts of the psychiatric conventional wisdom and overturns other parts. How much should we trust this versus all of the rest of the lore and heuristics and smaller studies that have accreted over the years? </p>
<p>Some relevant points:</p>
<p>1. The study finds that all the SSRIs cluster together as basically the same, as they should. The drugs that stand out as especially good or especially bad are generally unique ones with weird pharmacology that <i>ought</i> to be especially different. Amitriptyline is a tricyclic, and very different from clomipramine which is the only other tricyclic tested. Mirtazapine does weird things to presynaptic norepinephrine. Duloxetine and venlafaxine are SNRIs. This passes the most obvious sanity check.</p>
<p>2. Amitriptyline, the most effective antidepressant in this study, is widely agreed to be very good. See eg <A HREF="http://bjp.rcpsych.org/content/178/2/129">Amitriptyline: Still The Leading Antidepressant After 40 Years Of Randomized Controlled Trials</A>. Amitriptyline does have many side effects that limit its use despite its impressive performance. I secretly still believe MAOIs, like phenelzine and tranylcypromine, to be even better than amitriptyline, but this study doesn&#8217;t include them so we can&#8217;t be sure.</p>
<p>3. Reboxetine, the least effective antidepressant in this study, is widely known to suck. It is not available in the United States becaues the FDA wouldn&#8217;t even approve it here.</p>
<p>4. On the other hand, agomelatine, another antidepressant widely known to suck, gains solid mid-tier status here, being about as good as anything else. The study even lists it as one of seven antidepressants that seems to do especially well (though it&#8217;s unclear what they mean and it&#8217;s obviously a different measure than this graph). But agomelatine was rejected by the FDA for not being good enough, <A HREF="http://www.ema.europa.eu/docs/en_GB/document_library/Summary_of_opinion_-_Initial_authorisation/human/000656/WC500089538.pdf">scathingly</A> rejected by the European regulators (although their decision was later reversed on appeal), and soundly mocked by various independent organizations and journals (<A HREF="https://www.ncbi.nlm.nih.gov/pubmed/20020562">1</A>, <A HREF="https://www.ncbi.nlm.nih.gov/pubmed/21830835">2</A>). It doesn&#8217;t look like Cipriani has access to any better data than anyone else, so how come his results are so much more positive?</p>
<p>5. Venlafaxine and desvenlafaxine are basically the same drug, minus a bunch of BS from the pharma companies trying to convince that desvenlafaxine is a super-new-advanced version that you should spend twenty times as much money on. But venlafaxine is the fourth most efficacious drug in the analysis; desvenlafaxine is the second <i>least</i> efficacious drug. Why should this be? I have similar complaints about citalopram and escitalopram. Should we privilege common sense over empiricism and say Cipriani has done something wrong? Or should we privilege empiricism over common sense and conclude that the super-trivial differences between these chemicals have some outsized metabolic significance that makes a big clinical difference? Or should we just notice that the 95% confidence intervals of almost everything in the study (including these two) overlap, so really Cipriani isn&#8217;t claiming to know anything about anything and it&#8217;s not surprising if the data are wrong?</p>
<p>6. I&#8217;m sad to see clomipramine doing so badly here, since I generally find it helpful and have even evangelized it <A HREF="https://apophany.wordpress.com/2016/12/29/drug-review-clomipramine-anafranil/">to my friends</A>. I accept that it has serious side effects, but I expected it to do at least a little better in terms of efficacy.</p>
<p>Hoping to rescue its reputation, I started looking through some of the clomipramine studies cited. First was <A HREF="https://link.springer.com/article/10.1007/BF00172884">Andersen 1986</A>, which compared clomipramine to Celexa and found some nice things about Celexa. This study doesn&#8217;t say a pharmaceutical company was involved in any way. But I notice the study was done in Denmark. And I also notice that Celexa is made by Lundbeck Pharmaceuticals, a Danish company. Am I accusing an entire European country of being in a conspiracy to promote Celexa? Would that be crazy?</p>
<p>The second clomipramine study listed is <A HREF="https://www.ncbi.nlm.nih.gov/pubmed/6215443">De Wilde 1982</A>, which compared clomipramine to Luvox and found some nice things about Luvox. This study also doesn&#8217;t say a pharmaceutical company was involved in any way. But I notice the study was done in Belgium. And I also notice that Luvox is made by Solvay Pharmaceuticals, a Belgian company. Again, I&#8217;m sure Belgium is a lovely country full of many people who are not pharma shills, but this is starting to get a little suspicious.</p>
<p>To Cipriani&#8217;s credit, his team did notice these sorts of things and mark these trials as having &#8220;unclear&#8221; sponsorship levels, which got fed into the analysis. But I&#8217;m actually a little concerned about the exact way he did this. If a pharma company sponsored a trial, he called the pharma company&#8217;s drug&#8217;s results biased, and the comparison drugs unbiased. That is, suppose that Lundbeck sponsors a study, comparing their new drug Celexa to old drug clomipramine. We assume that they&#8217;re trying to make it look like Celexa is better. In this study, Cipriani would mark the Celexa patients as biased, but the clomipramine patients as unbiased. </p>
<p>But surely if Lundbeck wants to make Celexa look good, they can either finagle the Celexa numbers upward, finagle the clomipramine numbers downward, or both. If you flag Celexa as high risk of being finagled upwards, but don&#8217;t flag clomipramine as at risk of being finagled downwards, I worry you&#8217;re likely to understate clomipramine&#8217;s case.</p>
<p>I make a big deal of this because about a dozen of the twenty clomipramine studies included in the analysis were very obviously pharma companies using clomipramine as the comparison for their own drug that they wanted to make look good; I suspect some of the non-obvious ones were too. If all of these are marked as &#8220;no risk of bias against clomipramine&#8221;, we&#8217;re going to have clomipramine come out looking pretty bad. </p>
<p>Clomipramine is old and canonical, so most of the times it gets studied are because some pharma company wants to prove their drug is at least as good as this well-known older drug. There are lots of things like this, where certain drugs tend to inspire a certain type of study. Cipriani says they adjusted for this. I hope they were able to do a good job, because this is a big deal and really hard to factor out entirely.</p>
<p>This is my excuse for why I&#8217;m not rushing to prescribe drugs in the exact order Cipriani found. It&#8217;s a good study and will definitely influence my decisions. But it&#8217;s got enough issues that I feel justified in taking my priors into account too.</p>
<p><b>III.</b></p>
<p>Speaking of which, here&#8217;s another set of antidepressant rankings:</p>
<p><center><IMG SRC="http://slatestarcodex.com/blog_images/cipriani2.png"></center></p>
<p>This is from Alexander et al 2017, which started life as <A HREF="http://slatestarcodex.com/2015/04/30/prescriptions-paradoxes-and-perversities/">this blog post</A> but which with help from some friends I managed to get published by a journal. We looked at some different antidepressants than Cipriani did, but there are enough of the same ones that we can compare results.</p>
<p><center><IMG SRC="http://slatestarcodex.com/blog_images/cipriani3.png"></center></p>
<p>Everything is totally different. I haven&#8217;t checked formally, but the correlation between those two lists looks like about zero. We find mirtazapine and venlafaxine to be unusually <i>bad</i>, and amitriptyline to be only somewhere around the middle.</p>
<p>I don&#8217;t claim anywhere near the sophistication or brilliance or level of work that Cipriani et al put in. But my list &#8211; I will argue &#8211; makes sense. Drugs with near-identical chemical structure &#8211; like venlafaxine and desvenlafaxine, or citalopram and escitalopram &#8211; are ranked similarly. Drugs with similar mechanisms of action are in the same place. We match pieces of psychiatric conventional wisdom like &#8220;Paroxetine is the worst SSRI&#8221;. </p>
<p>Part of the disagreement may be related to all the antidepressants being very close together on both lists. On Cipriani&#8217;s, the difference between the 25th vs. 75th percentile is OR 1.75 vs. OR 1.52. On mine, it&#8217;s a rating of 7.14 vs. 6.52. Aside from a few outliers, there&#8217;s not a lot of light between any of the antidepressants here, which makes it likely that different methodologies will come up with very different orders. And the few outliers that each of us did identify as truly distinct often didn&#8217;t make it into the other&#8217;s study &#8211; Cipriani doesn&#8217;t have MAOIs and I don&#8217;t have reboxetine. But this isn&#8217;t a good enough excuse. One of my top performers, clomipramine, is near the bottom for Cipriani. One of my bottom performers, mirtazapine, is near his top. I have to admit that these just don&#8217;t match.</p>
<p>And a big part of the disagreement has to be that we&#8217;re not doing the same things Cipriani did &#8211; we&#8217;re looking at a measure that combines efficacy and acceptability, whereas Cipriani looked at each separately. This could explain why my data penalizes some side-effect-heavy drugs like mirtazapine and amitriptyline. But again, this isn&#8217;t a good enough excuse. Why doesn&#8217;t my list penalize other side-effect-heavy meds like clomipramine?</p>
<p>In the end, these are two very different lists that can&#8217;t be easily reconciled. If you have any sense, trust a major international study before you trust me playing around with online drug ratings. But also be aware of the study&#8217;s flaws and why you might want to retain a bit of uncertainty.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://slatestarcodex.com/2018/02/26/ssc-journal-club-cipriani-on-antidepressants/feed/</wfw:commentRss>
			<slash:comments>45</slash:comments>
		
		
			</item>
		<item>
		<title>SSC Survey Results 2018</title>
		<link>https://slatestarcodex.com/2018/01/03/ssc-survey-results-2018/</link>
					<comments>https://slatestarcodex.com/2018/01/03/ssc-survey-results-2018/#comments</comments>
		
		<dc:creator><![CDATA[Scott Alexander]]></dc:creator>
		<pubDate>Thu, 04 Jan 2018 06:22:33 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[original research]]></category>
		<category><![CDATA[statistics]]></category>
		<category><![CDATA[survey]]></category>
		<guid isPermaLink="false">http://slatestarcodex.com/?p=4739</guid>

					<description><![CDATA[Thanks to the 8,077 people (!) who took this year&#8217;s SSC survey. I don&#8217;t have the energy to screenshot/copy/paste the graph for every single question the way I have in previous years, so let&#8217;s do it differently. The survey page &#8230; <a href="https://slatestarcodex.com/2018/01/03/ssc-survey-results-2018/">Continue reading <span class="pjgm-metanav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>Thanks to the 8,077 people (!) who took this year&#8217;s SSC survey.</p>
<p>I don&#8217;t have the energy to screenshot/copy/paste the graph for every single question the way I have in previous years, so let&#8217;s do it differently.</p>
<p>The survey page is changed so that you can just press &#8220;okay&#8221; and &#8220;submit&#8221;, and it will bring you to the results page and see all the results. I&#8217;m not sure you <i>can</i> take the whole survey anymore, but if you find a way to do so, please don&#8217;t. Just press &#8220;okay&#8221; and &#8220;submit&#8221; and you should be fine. Don&#8217;t worry, all identifying questions (including the identifier string and all long answers) have been hidden.</p>
<p>See <A HREF="https://archive.is/LQGCx">the exact questions for the SSC survey</A>.</p>
<p>See <A HREF="https://docs.google.com/forms/d/e/1FAIpQLSfzY5lTjMvzmkw2daeBsUCbz54gehU4gXHnJ4augSDJu9R2Sg/viewform?usp=sf_link
">results from the SSC survey</A>.</p>
<p>See <A HREF="https://docs.google.com/forms/d/e/1FAIpQLSdwsU8zHxW_VHapMVxEtbJ1SBC6R0Sd7rektMtjY0sQglKH-A/viewform?usp=sf_link">results from the Mechanical Turk comparison survey</A>.</p>
<p>(this might have a lot of lag if you try to do it at the same time as everyone else; if you tell your browser to stop scripts it might improve)</p>
<p>I plan to post longer analyses (including the ones in the <A HREF="http://slatestarcodex.com/2017/12/25/preregistration-of-hypotheses-for-the-ssc-survey/">pre-registered hypotheses</A>) later on, hopefully dragging them out into a bunch of Least Publishable Units.</p>
<p>If you want to scoop me, or investigate the data yourself, you can download the answers of the 7298 people who agreed to have their responses shared publicly:</p>
<p>Main survey: <A HREF="http://slatestarcodex.com/Stuff/ssc2018public.xlsx">.xlsx</A>, <A HREF="http://slatestarcodex.com/Stuff/ssc2018public.csv">.csv</A></p>
<p>Turk survey: <A HREF="http://slatestarcodex.com/Stuff/turk2018public.xlsx">.xlsx</A>, <A HREF="http://slatestarcodex.com/Stuff/turk2018public.csv">.csv</A></p>
]]></content:encoded>
					
					<wfw:commentRss>https://slatestarcodex.com/2018/01/03/ssc-survey-results-2018/feed/</wfw:commentRss>
			<slash:comments>440</slash:comments>
		
		
			</item>
		<item>
		<title>Preregistration Of Hypotheses For The SSC Survey</title>
		<link>https://slatestarcodex.com/2017/12/25/preregistration-of-hypotheses-for-the-ssc-survey/</link>
					<comments>https://slatestarcodex.com/2017/12/25/preregistration-of-hypotheses-for-the-ssc-survey/#comments</comments>
		
		<dc:creator><![CDATA[Scott Alexander]]></dc:creator>
		<pubDate>Tue, 26 Dec 2017 00:42:42 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[original research]]></category>
		<category><![CDATA[statistics]]></category>
		<category><![CDATA[survey]]></category>
		<guid isPermaLink="false">http://slatestarcodex.com/?p=4723</guid>

					<description><![CDATA[[This post is about the 2018 SSC Survey. If you&#8217;ve read at least one blog post here before, please take the survey if you haven&#8217;t already. Please don&#8217;t read on until you&#8217;ve taken it, since this could bias your results.] &#8230; <a href="https://slatestarcodex.com/2017/12/25/preregistration-of-hypotheses-for-the-ssc-survey/">Continue reading <span class="pjgm-metanav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>[<i>This post is about the 2018 SSC Survey. If you&#8217;ve read at least one blog post here before, please <A HREF="https://goo.gl/forms/8bmb7dwWyBtS5nDM2"><b>take the survey</b></A> if you haven&#8217;t already. Please don&#8217;t read on until you&#8217;ve taken it, since this could bias your results.</i>]</p>
<p>I&#8217;m preregistering my hypotheses for the survey this year. So far I&#8217;ve glanced at Google&#8217;s bar graphs for each individual question but haven&#8217;t started exploring relationships yet, so I&#8217;m not cheating <i>too</i> badly. I&#8217;ll still look for things I haven&#8217;t preregistered, but I&#8217;ll admit they&#8217;re preliminary results only. This is the stuff I&#8217;ve been thinking about beforehand and will be taking more seriously:</p>
<p><b>1.</b> I plan to replicate the general thrust of last year&#8217;s results reported in <A HREF="http://slatestarcodex.com/2017/07/14/can-we-link-perception-and-cognition/">Can We Link Perception And Cognition</A> on the sample of new people who didn&#8217;t take the survey last year. In particular, I&#8217;m expecting that weirder, more autistic, more liberal, more schizophrenic, and more transgender people will be more likely to display unusual patterns of perception (hollowness or ambiguity) in the Hollow Mask illusion. I expect this to become much more obvious since I&#8217;ve included three examples of the illusion this year including one that seems to give a wider diversity of results.</p>
<p>1a. I plan to replicate the results from last year that people who were better at noticing duplicate &#8220;the&#8217;s&#8221; are more likely to display unusual patterns of perception on the Hollow Mask illusion. </p>
<p><b>2.</b> I plan to conceptually replicate <A HREF="http://onlinelibrary.wiley.com/doi/10.1002/aur.130/abstract">Mitchell et al&#8217;s</A> study showing that autistic people are less susceptible to the Shepherd Table Illusion.</p>
<p><b>3.</b> I plan to conceptually replicate <A HREF="https://www.researchgate.net/publication/267870702_The_tree_to_the_left_the_forest_to_the_right_Political_attitude_and_perceptual_bias">Caparos et al&#8217;s</A> study showing that politically further-right people are more likely to use global processing on a Navon task (eg when there&#8217;s an H made of tiny Es, they see the H more than the Es).</p>
<p><b>4.</b> I plan to investigate a general construct of &#8220;first sight and second thoughts&#8221; that involves people being better able to see what&#8217;s actually there, and less susceptible to illusions, priors, stereotypes, and assumptions. This will involve correlations between the two Duplicate Thes illusions, the Hollow Mask illusion, the Shepherd Table illusion, the Cookies illusion, the Parentheses palindrome, the Map riddle, the Surgeon riddle, the Switched Answers task, the Cognitive Reflection test, and the Wason task. </p>
<p>4a. If I can figure out how to get a common factor out of all of these, I plan to see if it&#8217;s the same thing I&#8217;m looking at in 1, and how it relates to the same groups.</p>
<p>4b. Whether this relates to a general willingness to believe strange or unpopular things. Check vs. AI risk concern and HBD support.</p>
<p><b>5.</b> I plan to investigate a general construct of &#8220;ambiguity tolerance&#8221; that involves people being okay with a superposition of different conflicting ideas. This will involve correlations between ambiguous results on the Hollow Mask illusion, the Spinning Dancer illusion and the Squares-Circles illusion, and with answers to the questions from the Tolerance Of Ambiguity and Tolerance of Uncertainty scales.</p>
<p>5a. Whether perceptual ambiguity relates to cognitive ambiguity. I want to check whether people with high ambiguity tolerance on the optical illusions are more likely to say their political opponents have some good points, are less likely to say their political opponents are evil, and are less likely to say the existing political system is justifiable. Also if they&#8217;re more likely to enjoy puns.</p>
<p>5b. To what degree this is the same construct as (1), and is stronger among the same demographic groups.</p>
<p>5c. I also want to see if people with high ambiguity tolerance give less extreme answers on questions in general. I&#8217;ll probably use Ambition, Social Status, Romantic Life, and Morality for this, just because these seem like complicated questions there&#8217;s no obvious right answer to.</p>
<p>5d. I plan to confirm previous studies showing low ambiguity tolerance correlates with conservative philosophy; check vs. Political Spectrum 1-10. I predict that this will be stronger for populists than for &#8220;business conservatives&#8221;, so I expect the low ambiguity correlation will be weak for generic conservatives, stronger for Trump supporters, strongest for people who identify as alt-right.</p>
<p><b>6.</b> I plan to investigate whether autistic people are more likely to give process-centered rather than person-centered answers to the two political categorization questions (categorizing Nazis, categorizing civil disobedience on gay marriage). That is, neurotypical people will be more likely to categorize based on which side wins, and autistic people will be more likely to categorize based on what procedures were followed (eg violence, civil disobedience).</p>
<p>6a. I also want to investigate how these correlate with political views. I may end up controlling for this as a confounder in (6) above.</p>
<p>6b. This is a totally wild out-of-left field idea, but I suppose I should check how these relate to the Navon figures since they&#8217;re both about categorization.</p>
<p><b>7.</b> I plan to confirm or disprove, once and for all, whether our community has more older siblings. For lack of a fancier way to do this, I&#8217;ll take the set of all people who have exactly one sibling, and see what percent of them are older vs. younger. If it&#8217;s significantly above 50% older, I&#8217;m going to interpret this as a birth order effect. I&#8217;ll do the same with the set of people who have two siblings, three siblings, etc, and combine them all for a final determination. Half-siblings will be ignored. If you have any problems with this methodology, tell me <i>now</i>.</p>
<p>7a. If I find we&#8217;re disproportionately older, try to use subgroups to figure out where the effect is stronger or weaker, to try to find exactly what&#8217;s going on. For example, are Less Wrongers more older-skewed than SSC readers in general?</p>
<p>7b. Birth order by autism, Openness, and IQ/SAT.</p>
<p>7c. One traditional birth-order claim is that younger children are more rebellious, so check birth order vs. people who think system needs to be fine-tuned or destroyed.</p>
<p><b>8.</b> I plan to conceptually replicate studies showing that the more older brothers (but not younger brothers, or older or younger sisters) you have, the more likely you are to be gay.</p>
<p>8b. See if this predicts anything else: bisexuality, transgender, gender non-conformity, political leftism, autism, possibly &#8216;first sight and second thoughts&#8217;, possibly &#8216;ambiguity tolerance&#8217;.</p>
<p><b>9.</b> I plan to see whether people with ADHD are more likely to prefer the buzzing city aesthetic to the quiet village aesthetic, more likely to rate themselves as more risk-taking, and more likely to describe themselves as ambitious.</p>
<p><b>10.</b> I plan to investigate the hypothesis about sexual harassment mentioned <A HREF="http://slatestarcodex.com/2017/08/01/gender-imbalances-are-mostly-not-due-to-offensive-attitudes/">here</A>: that it&#8217;s higher in gender imbalanced industries only due to potential-perpetrator-to-victim ratio. I predict that in relatively gender imbalanced industries (in terms of survey categories, all three Computers fields, Finance, Physics, and Mathematics) compared to relatively gender-balanced industries (Health Care, Psychology, Art, Law, Biology), a higher percent of women will report being harassed at work, but the percent of men reporting harassing at work will remain the same. </p>
<p>10b. I predict that the more people identify with social justice, and the more positively they feel about feminism, the more likely they are to report both being harassed and harassing others, due to more awareness and lower threshold to report. I predict poor social skills and autism spectrum will predict more likely to say one is a harasser, due to causing unintentional offense. I predict people who are harassed more at work will also be harassed more outside of work.</p>
<p><b>11.</b> A long time ago, I randomized people into groups and made them read articles on AI risk to <A HREF="http://slatestarcodex.com/2016/10/24/ai-persuasion-experiment-results/">see how it changed their minds</A>. The effect mostly persisted after one month. Since those groups were randomized by birth date, and I asked respondents their birthdates, I plan to see if those effects continue to persist after a year.</p>
<p>These are mostly conceptual descriptions of what I&#8217;m going to do rather than algorithmic descriptions of exactly how I&#8217;m going to process the data. Part of that is that a lot of this involves statistical techniques at the limits of my abilities and I&#8217;m going to have to see if I can actually do them. Most important, I would like to learn enough about factor analysis to actually check for a General Factor Of First Sight/Second Thoughts, and a General Factor of Ambiguity Tolerance. If I have them, I&#8217;d like to use them to see if they correlate with the other things I&#8217;m wondering if they correlate with. If I can&#8217;t make this work or beg someone else to do it for me, I&#8217;ll just eyeball the correlations between individual questions, see which ones are highest, and maybe take an average of those questions or something.</p>
<p>Mostly I won&#8217;t be doing anything fancy or with too many branching paths to the data, but I plan to operationalize autism in two ways. First, a scale where professional diagnosis equals 3, self-diagnosis equals 2, family member equals 1, and no personal/family history equals 0. Second, the Autism Spectrum Quotient test I made people take at the bottom of the survey. I&#8217;m not at all confident these will correlate more than a weak amount, but I&#8217;ll try it and see. I might also try some kind of average of the two measures. Since there are a few things I expect to be correlated with autism &#8211; mathematical careers, bad response to clothing tags, poor social skills &#8211; I might check to see whether the first measure, the second measure, or the combination does a better job of predicting these, and stick with whichever one does. I&#8217;ll try not to base which measure I use on any of the variables I&#8217;m actually testing.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://slatestarcodex.com/2017/12/25/preregistration-of-hypotheses-for-the-ssc-survey/feed/</wfw:commentRss>
			<slash:comments>191</slash:comments>
		
		
			</item>
		<item>
		<title>Change Minds Or Drive Turnout?</title>
		<link>https://slatestarcodex.com/2017/07/10/change-minds-or-drive-turnout/</link>
					<comments>https://slatestarcodex.com/2017/07/10/change-minds-or-drive-turnout/#comments</comments>
		
		<dc:creator><![CDATA[Scott Alexander]]></dc:creator>
		<pubDate>Mon, 10 Jul 2017 06:21:13 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[politics]]></category>
		<category><![CDATA[reverse currentaffairsplaining]]></category>
		<category><![CDATA[statistics]]></category>
		<guid isPermaLink="false">http://slatestarcodex.com/?p=4459</guid>

					<description><![CDATA[I. Current Affairs: The Democratic Party Just Admitted It Doesn&#8217;t Stand For Anything. Overall it makes some good points, but one passage caught my eye: [Democrats believe that if you moderate your platform and swing toward the center] you might &#8230; <a href="https://slatestarcodex.com/2017/07/10/change-minds-or-drive-turnout/">Continue reading <span class="pjgm-metanav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p><b>I.</b></p>
<p>Current Affairs: <A HREF="https://www.currentaffairs.org/2017/07/the-democratic-party-just-admitted-it-doesnt-stand-for-anything">The Democratic Party Just Admitted It Doesn&#8217;t Stand For Anything</A>. Overall it makes some good points, but one passage caught my eye:</p>
<blockquote><p>[Democrats believe that if you moderate your platform and swing toward the center] you might lose a few hardcore lefties, but you’ll more than make up for it in the number of Reaganites you peel away from the other side. (Or, as Chuck Schumer put it, “For every blue-collar Democrat we lose in western Pennsylvania, we will pick up two moderate Republicans in the suburbs in Philadelphia.”)</p>
<p>But this philosophy is a dead end. For one thing, it doesn’t work. Unless you have Bill Clinton’s special charismatic magic, what actually happens is that progressive voters just stay home, disgusted at the failure of both parties to actually try to improve the country. And the mythical “moderate Republicans” never seem to show up. (This is because there are no actual moderate Republicans.)</p></blockquote>
<p>This has been a staple of recent leftist thought. Another example from <A HREF="https://www.dailykos.com/story/2014/11/5/1342347/-CRUSH-the-GOP-don-t-compromise-with-em-how-to-win-in-2016-and-what-not-to-do">Daily Kos</A> (via the paper below):</p>
<blockquote><p>The key data is this, and it&#8217;s important to re-emphasize if only to shut up the useless, overpaid political consultants who idiotically babble about &#8220;moving to the center&#8221; or &#8220;compromising with the other side&#8221;&#8230;What matters is turning out our voters. That&#8217;s it. The Democrats win when we fire up and turn out our base.</p></blockquote>
<p>This sounds like a win-win situation. We can stick to our principles, and that actually makes us <i>more</i> electable. Big if true. But is it?</p>
<p><b>II.</b></p>
<p>First: do more extreme views increase base turnout? This is the subject of <A HREF="http://www.andrewbenjaminhall.com/Hall_Thompson_Base_Turnout.pdf">Hall &#038; Thompson (2017)</A>. They examine 1658 House races from 2006 to 2012 and start by noticing that the more distant a candidate from the median voter in their district, the fewer votes they get <i>and</i> the lower their party&#8217;s share of turnout in the general election (ie the various other races that go on at the same time). This suggests that not only are the voters who do turn out less likely to vote for the extremist, but that many of their voters are staying home (or many of their opponents&#8217; voters have been galvanized to show up).</p>
<p><center><IMG SRC="http://slatestarcodex.com/blog_images/turnout1.png"></center></p>
<p>These raw results could be driven by exogenous factors. For example, maybe in swing states, parties nominate more centrist candidates (to get a broader appeal) and have higher turnout (because people&#8217;s votes actually matter). To eliminate this possibility, the researchers try a regression discontinuity design &#8211; ie they compare districts where extremists won the primary by 0.1% to districts where extremists lost the primary by 0.1%. These sorts of tiny margins are likely to be pretty random, so it&#8217;s almost like an experimental trial of what happens when you randomly vary candidate extremism.</p>
<p><center><IMG SRC="http://slatestarcodex.com/blog_images/turnout2.png"></center></p>
<p>This better-controlled data set finds the same thing. The more extreme a candidate, the lower their party&#8217;s share of the turnout. </p>
<p>This actually makes a lot of sense &#8211; a lot of my normally non-voting friends turned out last November because they hated Trump so much, and a lot of #NeverTrump Republicans, unwilling to hold their nose and vote Hillary, just stayed home.</p>
<p>Hall and Thomspon conclude:</p>
<blockquote><p>This paper engages with a longstanding debate over the relative strengths of extreme legislative candidates, thought to boost turnout among their party’s base, and moderate candidates thought to  attract  hypothetical  moderate  swing  voters. Using several different empirical strategies, we have found consistent evidence that extremist nominees do poorly in general elections in large part because they skew turnout in the general election away from their own party and in favor of the opposing party.</p></blockquote>
<p>They crunch a few more numbers and conclude that effects on turnout might be the <i>entire</i> reason why extremist candidates do worse. That is, there is no remaining effect from swing voters who switch from their own party to the other party. Turnout is the only thing that matters:</p>
<blockquote><p>The results suggest that much of moderate candidates’ success may actually be due to the turnout of partisan voters, rather than to swing voters who switch sides.  In fact, our regression discontinuity estimates are consistent with the possibility that the entire vote-share penalty to extremist nominees is the result of changes in partisan turnout.  Seen in this light, the results are more consistent with the behavioral literature’s focus on turnout than they are with the institutional literature’s theoretical focus on swing voters. As such, we see this paper as helping to link the behavioral and institutional literatures together, suggesting that moderate candidates do possess an electoral advantage,  but that this advantage may depend heavily on turnout-based mechanisms.</p></blockquote>
<p>So Thompson and Hall disagree with the theory that a less compromising, more robustly leftist Democratic Party would get more votes. But they tentatively agree with <i>Current Affairs&#8217;</i> claim that &#8220;moderate Republicans&#8221; are a myth and nobody ever switches sides.</p>
<p><b>III.</b></p>
<p>Second: Is base turnout really the only thing that matters?</p>
<p>I&#8217;m reluctant to disagree with real political scientists like Hall and Thompson, but I&#8217;m a little more optimistic about whether people can change their minds.</p>
<p>There&#8217;s little data on vote-switching, and the only directly relevant information I could find was <A HREF="http://www.cnn.com/ELECTION/2008/results/polls/#val=USP00p3">this CNN exit poll</A> from 2008:</p>
<p><center><IMG SRC="http://slatestarcodex.com/blog_images/turnout3.png"></center></p>
<p>Of people who voted Democrat for President in 2004, 9% went Republican in 2008. Of people who voted Republican in 2004, a full 17% went Democrat in 2008. Some analysts of this information caution us that people are bad at remembering their votes so some of this may be wrong. But I feel like this story <i>also</i> doesn&#8217;t fit well with with unchanging-eternal-partisanship narrative &#8211; if you&#8217;ve voted straight Republican for the last ten elections and loathe all Democrats with a burning fury, you&#8217;re not going to just forgot whether you voted Bush or Kerry in &#8217;04.</p>
<p>Rasmussen doesn&#8217;t have a real exit poll, but <A HREF="http://www.rasmussenreports.com/public_content/political_commentary/commentary_by_geoffrey_skelley/just_how_many_obama_2012_trump_2016_voters_were_there">they put</A> a couple of different sources together to guess about how many people switched votes in most recent election. I don&#8217;t really understand their graphs &#8211; in particular, their use of the Other category doesn&#8217;t make much sense. But if I&#8217;m reading them right, of people who voted Democrat in 2012, about 13% voted Trump in 2016. And of people who voted Republican in 2012, about 4% voted Clinton in in 2016. These may seem like small numbers. But in the context of the tiny margins by which Trump won swing states (Michigan by 0.3%, Pennsylvania by 0.7%, Wisconsin by 0.8%), these sorts of changes are absolutely decisive.</p>
<p>So swing voters and moderates aren&#8217;t totally mythical. But how do they compare with turnout as a determining factor in elections?</p>
<p>This is hard to figure out. <s>We know that total turnout decreased 2% between 2012 and 2016</s> [EDIT: More recent sources say turnout increased. Not clear on this right now. See <A HREF="http://slatestarcodex.com/2017/07/10/change-minds-or-drive-turnout/#comment-520890">here</A>]. But it&#8217;s hard to interpret party turnout figures. If the number of Democratic votes dipped more than the number of Republican votes, how much of that is because the Democrats had a bigger turnout problem, and how much is because some Democrats crossed the aisle to vote Republican?</p>
<p>Nate Cohn of the New York Times <A HREF="https://www.nytimes.com/2017/03/28/upshot/a-2016-review-turnout-wasnt-the-driver-of-clintons-defeat.html">tries to solve this</A> by analyzing turnout of <i>predicted</i> partisan voters &#8211; eg a young black gay college graduate will probably vote Democrat, so if he doesn&#8217;t show up it suggests Democratic base turnout declined. Before the election, he made some mechanical projections about how much each demographic would turn out based on how often they&#8217;ve turned out before in situations like this. Sometimes this risks adjusting away exactly the factors we&#8217;re interested in &#8211; eg he predicts black people will have much lower turnout in 2016 because part of their record 2012 turnout was personal loyalty to Obama. But as far as I can tell he doesn&#8217;t adjust for anything about the candidate&#8217;s ideologies, making his predictions okay for our purposes of talking about the effects of candidate extremism.</p>
<p>Cohn finds that blacks voted a little bit less than he predicted, and Hispanics a little bit more. Whites likely to support Trump (eg older, less educated, etc) turned out about 7% more than expected. Whites likely to support Clinton turned out about 4% more (sic!) than expected. But overall, these differences were &#8220;only a modest effect&#8221;, and probably not enough to affect the election:</p>
<blockquote><p>Turnout improved Mr. Trump&#8217;s standing by a modest margin compared with pre-election expectations. If the turnout had gone exactly as we thought it would, the election would have been extremely close. But by this measure, Mrs. Clinton still would have lost both Florida and Pennsylvania &#8211; albeit very narrowly&#8230;Democrats are right to blame many of their midterm election losses on weak turnout. They&#8217;re on far shakier ground if they complain about the turnout last November.</p></blockquote>
<p>He thinks that it was the much-maligned swing voters who were more important:</p>
<blockquote><p>If turnout played only a modest role in Mr. Trump’s victory, then the big driver of his gains was persuasion: He flipped millions of white working-class Obama supporters to his side.</p>
<p>The voter file data makes it impossible to avoid this conclusion. It’s not just that the electorate looks far too Democratic. In many cases, turnout cannot explain Mrs. Clinton’s losses.</p>
<p>Take Schuylkill County, Pa., the county where Mr. Trump made his biggest gains in Pennsylvania. He won, 69 percent to 26 percent, compared with Mitt Romney’s 56-42 victory. Mrs. Clinton’s vote tally fell by 7,776 compared with Mr. Obama’s 2012 result, even though the overall turnout was up.</p>
<p>Did 8,000 of Mr. Obama’s supporters stay home? No. There were 5,995 registered voters who voted in 2012, remain registered in Schuylkill County, and stayed home in 2016.</p>
<p>And there’s no way these 2016 drop-off voters were all Obama supporters. There were 2,680 registered Democrats, 2,629 registered Republicans and 686 who were unaffiliated or registered with a different party. This is a place where registered Democrats often vote Republican in presidential elections, so Mr. Obama’s standing among these voters was most likely even lower [&#8230;]</p>
<p>Survey data, along with countless journalistic accounts, also suggest that voters switched in huge numbers.</p>
<p>Throughout the campaign, polls of registered voters — which are not subject to changes in turnout — showed Mrs. Clinton faring much worse than Mr. Obama among white working-class voters.</p>
<p>The postelection survey data tells a similar story: Mrs. Clinton won Mr. Obama’s white-working class supporters by a margin of only 78 percent to 18 percent against Mr. Trump, according to the Cooperative Congressional Election Study.</p>
<p>In the Midwestern battleground states and Pennsylvania, Mrs. Clinton had an advantage of 76 percent to 20 percent among white working-class Obama voters.</p>
<p>The survey data isn’t perfect. It relies on voters’ accurate recall of their 2012 vote, and that type of recall is often biased toward the winner. Indeed, the C.C.E.S. found that Mr. Obama had 54 percent of support among 2012 voters, compared with his actual 51 percent finish.</p>
<p>But the data all points in the same direction: Shifts in turnout were not the dominant factor in Mr. Trump’s success among white working-class voters.</p></blockquote>
<p>I tried to model some of this myself to get actual numbers I could compare. It doesn&#8217;t work. If I apply the exit poll models of voter defections to the real numbers, I get implausibly high numbers for Trump and implausibly low numbers for Hillary. I would have to add a huge jump in Democratic turnout, and a corresponding crash in Republican turnout, to produce the modest Hillary popular-vote win we actually saw. Nobody&#8217;s claimed this and I don&#8217;t think that it happened. So I&#8217;m confused. I hate to have to go off of Cohn&#8217;s analysis, especially since he never really explains what goes into his projections, but right now it&#8217;s all I have. And it matches what Rasmussen thought in a lot of ways.</p>
<p>So I very tentatively conclude that swing voters might have changed the result of the 2016 election. I can&#8217;t directly compare to decreased turnout, but it seems at least as important, especially if you discount the non-ideology-related black turnout decrease.</p>
<p>Granted, the 2016 election was weird, we might be in some kind of unique realignment of the two-party system, maybe this doesn&#8217;t happen too often. But the Obama/Trump defections don&#8217;t seem much greater than the Bush/Obama defections on the 2008 CNN exit poll. And <i>Current Affairs</i> admits that Bill Clinton did pretty well attracting moderates and Republicans to  his banner. I think there&#8217;s enough examples to think that a large effect from swing voters might not just be possible, but common.</p>
<p>As far as I can tell, the evidence leans against the win-by-extremism-turning-out-the-base argument. Extremists tend to do worse in elections. They don&#8217;t raise turnout of their base; in fact, they probably lower it. They may fire up their opponents&#8217; base. And swing voters can make a big difference when a candidate appeals to them.</p>
<p>This doesn&#8217;t mean only boring centrists can win; Donald Trump is the obvious counterexample. But Trump&#8217;s extremism wasn&#8217;t just &#8220;Paul Ryan but much more so&#8221;. He won not by moving straight right, but by coming up with new ideas that held the attention of the Republican base while also appealing to some disaffected Democrats. And the Bernie Sanders wing of the Democratic Party might be able to do something similar from the left if it gets the chance.</p>
<p>Just don&#8217;t frame it as &#8220;extremism turns out the base&#8221;, and especially not as &#8220;swing voters don&#8217;t matter&#8221;.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://slatestarcodex.com/2017/07/10/change-minds-or-drive-turnout/feed/</wfw:commentRss>
			<slash:comments>1005</slash:comments>
		
		
			</item>
		<item>
		<title>SSC Journal Club: Childhood Trauma And Cognition</title>
		<link>https://slatestarcodex.com/2017/04/21/ssc-journal-club-childhood-trauma-and-cognition/</link>
					<comments>https://slatestarcodex.com/2017/04/21/ssc-journal-club-childhood-trauma-and-cognition/#comments</comments>
		
		<dc:creator><![CDATA[Scott Alexander]]></dc:creator>
		<pubDate>Fri, 21 Apr 2017 06:41:12 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[genetics]]></category>
		<category><![CDATA[journal club]]></category>
		<category><![CDATA[psychiatry]]></category>
		<category><![CDATA[psychology]]></category>
		<category><![CDATA[statistics]]></category>
		<guid isPermaLink="false">http://slatestarcodex.com/?p=4371</guid>

					<description><![CDATA[This month&#8217;s American Journal of Psychiatry includes Danese et al, Origins Of Cognitive Deficits In Victimized Children. Previous studies had found that abused children had lower IQ. They concluded that the severe stress of being abused must decrease brain function. &#8230; <a href="https://slatestarcodex.com/2017/04/21/ssc-journal-club-childhood-trauma-and-cognition/">Continue reading <span class="pjgm-metanav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>This month&#8217;s <i>American Journal of Psychiatry</i> includes Danese et al, <A HREF="https://moffittcaspi.com/sites/moffittcaspi.com/files/field/publication_uploads/Danese2016-ChildVictimisation-Cognition.pdf">Origins Of Cognitive Deficits In Victimized Children</A>. Previous studies had found that abused children had lower IQ. They concluded that the severe stress of being abused must decrease brain function. Danese et al challenge that assumption in the context of the new paradigm of shared-environment-skeptical psychiatry. </p>
<p>They looked at some of the big developmental studies that had followed thousands of children from birth and evaluated them throughout the years. Most of these kids had gotten a bunch of different IQ tests every year, starting as young as 3. These children had also been screened for abuse, both by questions that tended to predict abuse (for example, asking parents whether they ever &#8220;punished their children harshly&#8221;) and by asking the kids, once they were grown up, whether they&#8217;d ever been abused or not. </p>
<p>The hope was to find children who hadn&#8217;t been abused before their first IQ test at age 3, but who started being abused before their last IQ test at age 18. Then they would see whether IQ at age 3 predicted IQ at age 18 in the same way it would in unabused children, or whether IQ was lower than predicted, presumably because of the abuse. </p>
<p>Before the adjustments, they found the same thing as every other study &#8211; abused children had lower IQ than unabused children (p < 0.01, B = 0.1):

<blockquote>Consistent with previous research, we found that adolescents and adults with a history of childhood victimization have pervasive deficits in clinically significant cognitive functions, including both general intelligence and more specific measures of executive unction, processing speed, memory, perceptual reasoning, and verbal comprehension.</p></blockquote>
<p>After the adjustments, there was no difference between IQs in the two groups (p = 0.13, B = 0.03):</p>
<blockquote><p>In contrast to the conventional causal interpretation of these findings, our longitudinal prospective design revealed that cognitive deficits in victimized adolescents and adults were largely explained by cognitive deficits present before the observational period for childhood victimization and by nonspecific effects of childhood socioeconomic disadvantage. On the one hand, the results are consistent with the high heritability of cognitive functions, their strong continuity across the life course, and the stable cognitive deficits previously described in children exposed to adversity. On the other hand, they are inconsistent with the causal effects of early-life stress on brain function reported in experimental animal models.</p></blockquote>
<p>The study raises the possibility that kids from poorer families were more likely to get abused and to have lower IQs. Other possibilities include abusers (like criminals) having lower-than-average IQs and passing them on to their kids, or low-IQ kids being less able to figure out ways to escape abuse.</p>
<p>I find this interesting for two reasons.</p>
<p>First, twin studies famously find that early family experiences (the &#8220;shared environment&#8221;) are less important than previously believed. This has led to a small cottage industry of trying to deny twin studies and saying they don&#8217;t work. There&#8217;s a lot of interesting debate in this area, and by &#8220;interesting&#8221; I mean &#8220;annoying and interminable&#8221;. Maybe the best solution is to route around the whole problem by viewing twin studies as an incentive to conduct <i>really rigorous</i> non-twin studies to see if alternate methodologies can confirm the same surprising conclusions. Danese et al is a rare example of this being done right.</p>
<p>Second, a lot of discussion of the limited role of the shared environment adds the caveat &#8220;of course, we&#8217;re not talking about actual abuse here. Of course <i>that</i> affects outcomes.&#8221; It probably affects some of them. But here we see that it&#8217;s not necessarily true along every axis. </p>
<p><A HREF="https://openarchive.ki.se/xmlui/bitstream/handle/10616/45576/Manuscript_Dinkler.pdf?sequence=3">A recent twin study</A> showed that child abuse doesn&#8217;t increase risk for ADHD, autism, or learning disorders. But I don&#8217;t think most people expected it to increase the risk of these disorders, which are traditionally viewed as really genetic.</p>
<p><A HREF="https://www.ncbi.nlm.nih.gov/pubmed/20024671">A 2010 study</A> found that it didn&#8217;t increase the risk of &#8220;conduct problems&#8221; broadly defined. These <i>are</i> traditionally viewed as linked with child abuse, so that&#8217;s interesting.</p>
<p><A HREF="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3992260/">A 2013 study</A> found that child abuse mostly didn&#8217;t explain personality disorders. Traditionally borderline personality disorder in particular is viewed as almost invariably associated with abuse, but if I&#8217;m reading the study right abuse really only explained about 1% of the variance in who had the condition. And although this called itself a &#8220;co-twin analysis&#8221;, it wasn&#8217;t a real twin study &#8211; it used non-abused twins as the control group for abused twins, but it didn&#8217;t check if they were identical or non-identical; this controls for family socioeconomic status but not for genetics. This leaves open the possibility that even the 1% remaining abuse-related variance might not be real.</p>
<p>The strongest study I could find was <A HREF="https://www.ncbi.nlm.nih.gov/pubmed/10722174">this one</A> linking childhood sexual abuse to a threefold lifetime increase in psychiatric disorders. It, too, was a cotwin analysis and so not genetically controlled. And although I am not going to win any sensitivity points for saying that genetics can affect likelihood of being sexually abused, this is my impression of a lot of the cases I&#8217;ve seen &#8211; for example, children with a lot of of preexisting issues and bad relationships with their parents are more willing to secretly meet predators who have been grooming them online. </p>
<p>On the other hand, there are also reasons to believe these studies might understate the dangers of abuse. For example, the Danese et al study sampled 3000 kids who were supposed to be representative of the communities they came from. Suppose only 1% of children experience very severe child abuse. Then they would only have a sample of 30 &#8211; far too small to find any interesting conclusions. Maybe most of what they&#8217;re picking up is differences between not-abused-at-all kids and only-mildly-abused kids, and we shouldn&#8217;t be too surprised if they don&#8217;t find anything major.</p>
<p>I don&#8217;t want to underplay the role of child abuse. My impression from anecdotal evidence is that it does seem to have serious negative effects on kids later in life. For example, I&#8217;ve seen some kids from well-off, genetically-healthy families who were sexually abused by a coach or a neighbor or a priest or someone and who became nervous wrecks later in life, unable to stop thinking about it or to stop feeling guilty. Even good studies usually find that <A HREF="http://www.reuters.com/article/us-twin-ptsd-idUSTRE68T47B20100930">trauma is a risk factor for PTSD</A> (thank goodness), proving that highly stressful events can be psychologically scarring and lead to future problems. Once you&#8217;ve accepted that, you&#8217;ve got most of the assumptions you need to reconstruct the case for child abuse causing psychological problems. </p>
<p>I think the safest assumption is that child abuse that rises to the level of trauma is associated with a PTSD-like picture (although I&#8217;ve heard some experts discourage thinking of it as literally PTSD) that seriously affects later-life outcomes. I think it&#8217;s less likely that child abuse has many effects that are wildly different from what we see in normal PTSD. And since normal PTSD doesn&#8217;t decrease IQ, I&#8217;m not surprised to hear that child abuse doesn&#8217;t do that either.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://slatestarcodex.com/2017/04/21/ssc-journal-club-childhood-trauma-and-cognition/feed/</wfw:commentRss>
			<slash:comments>152</slash:comments>
		
		
			</item>
	</channel>
</rss>
