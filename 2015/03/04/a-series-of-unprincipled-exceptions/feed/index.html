<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	
	xmlns:georss="http://www.georss.org/georss"
	xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#"
	
	>
<channel>
	<title>
	Comments on: A Series Of Unprincipled Exceptions	</title>
	<atom:link href="https://slatestarcodex.com/2015/03/04/a-series-of-unprincipled-exceptions/feed/" rel="self" type="application/rss+xml" />
	<link>https://slatestarcodex.com/2015/03/04/a-series-of-unprincipled-exceptions/</link>
	<description>SELF-RECOMMENDING!</description>
	<lastBuildDate>Fri, 24 Mar 2017 20:25:40 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.4.1</generator>
	<item>
		<title>
		By: Daneel		</title>
		<link>https://slatestarcodex.com/2015/03/04/a-series-of-unprincipled-exceptions/#comment-267684</link>

		<dc:creator><![CDATA[Daneel]]></dc:creator>
		<pubDate>Sat, 21 Nov 2015 08:01:05 +0000</pubDate>
		<guid isPermaLink="false">http://slatestarcodex.com/?p=3571#comment-267684</guid>

					<description><![CDATA[In terms of helping people closer to you more than those further away, I toyed with the idea for a while that one should rank all people in terms of how close they are to you and allocate an amount of your income proportional to 1/n to helping the closest n people in the best possible way.

So 1/log(7 billion) ~ 1/22 of your money gets allocated to just yourself.
half that much goes to helping you and your closest friend (though if they do not particularly need money it may be more efficient to spend it on yourself since you know your own needs)
If you live in the US, about log(300 million)/log(7 billion) ~ 85% of your money should go to people in the US (assuming they are your 300million closest) and so on.

It seemed like a way that made this idea concrete without making it feel totally insane in either direction (though I guess these numbers probably end up slightly on the overly burdensome end, but not nearly as much as the &quot;you must donate all your income to Africa/animal rights/MIRI&quot; theory).]]></description>
			<content:encoded><![CDATA[<p>In terms of helping people closer to you more than those further away, I toyed with the idea for a while that one should rank all people in terms of how close they are to you and allocate an amount of your income proportional to 1/n to helping the closest n people in the best possible way.</p>
<p>So 1/log(7 billion) ~ 1/22 of your money gets allocated to just yourself.<br />
half that much goes to helping you and your closest friend (though if they do not particularly need money it may be more efficient to spend it on yourself since you know your own needs)<br />
If you live in the US, about log(300 million)/log(7 billion) ~ 85% of your money should go to people in the US (assuming they are your 300million closest) and so on.</p>
<p>It seemed like a way that made this idea concrete without making it feel totally insane in either direction (though I guess these numbers probably end up slightly on the overly burdensome end, but not nearly as much as the &#8220;you must donate all your income to Africa/animal rights/MIRI&#8221; theory).</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: HarveyNof		</title>
		<link>https://slatestarcodex.com/2015/03/04/a-series-of-unprincipled-exceptions/#comment-258131</link>

		<dc:creator><![CDATA[HarveyNof]]></dc:creator>
		<pubDate>Wed, 04 Nov 2015 17:37:08 +0000</pubDate>
		<guid isPermaLink="false">http://slatestarcodex.com/?p=3571#comment-258131</guid>

					<description><![CDATA[Part of my misunderstanding was because sometimes when the Unprincipled Exception is cited, the former case appears to be in view, i.e. someone holds to a position but doesn t take it to its logical conclusion in situations where it would clearly be untenable yet still holds to the position overall.]]></description>
			<content:encoded><![CDATA[<p>Part of my misunderstanding was because sometimes when the Unprincipled Exception is cited, the former case appears to be in view, i.e. someone holds to a position but doesn t take it to its logical conclusion in situations where it would clearly be untenable yet still holds to the position overall.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Citizensearth		</title>
		<link>https://slatestarcodex.com/2015/03/04/a-series-of-unprincipled-exceptions/#comment-190252</link>

		<dc:creator><![CDATA[Citizensearth]]></dc:creator>
		<pubDate>Mon, 16 Mar 2015 05:35:43 +0000</pubDate>
		<guid isPermaLink="false">http://slatestarcodex.com/?p=3571#comment-190252</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://slatestarcodex.com/2015/03/04/a-series-of-unprincipled-exceptions/#comment-187446&quot;&gt;TheAncientGeek&lt;/a&gt;.

I don&#039;t see how to reach you inside that meticulous self-deception. I hope things work out.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a rel="nofollow"href="https://slatestarcodex.com/2015/03/04/a-series-of-unprincipled-exceptions/#comment-187446">TheAncientGeek</a>.</p>
<p>I don&#8217;t see how to reach you inside that meticulous self-deception. I hope things work out.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: efnrer		</title>
		<link>https://slatestarcodex.com/2015/03/04/a-series-of-unprincipled-exceptions/#comment-189384</link>

		<dc:creator><![CDATA[efnrer]]></dc:creator>
		<pubDate>Fri, 13 Mar 2015 11:57:26 +0000</pubDate>
		<guid isPermaLink="false">http://slatestarcodex.com/?p=3571#comment-189384</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://slatestarcodex.com/2015/03/04/a-series-of-unprincipled-exceptions/#comment-187446&quot;&gt;TheAncientGeek&lt;/a&gt;.

&#062;This is a selective misrepresentation and nonsense. Rationality is frequently used on LessWrong as a shorthand for instrumental rationality. And that’s the way you were using the word. 

Wrong. We were discussing the difference between having truth as a terminal value and having epistemic rationality, the method for finding out the truth, as a terminal value. As I said above, if you cannot understand the difference between these two concepts you have a lot of reading to do.

&#062;In every post on this thread you insinuating that your interlocutor is stupid, rather than politely addressing their arguments. Perhaps it is you that needs to self-examine, my friend.

I&#039;m not the one who was caught lying and is desperately trying to backtrack and shift blame. Being able to admit when you&#039;re wrong is considered one of the first steps to become more rational, hopefully you&#039;ll learn it someday.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a rel="nofollow"href="https://slatestarcodex.com/2015/03/04/a-series-of-unprincipled-exceptions/#comment-187446">TheAncientGeek</a>.</p>
<p>&gt;This is a selective misrepresentation and nonsense. Rationality is frequently used on LessWrong as a shorthand for instrumental rationality. And that’s the way you were using the word. </p>
<p>Wrong. We were discussing the difference between having truth as a terminal value and having epistemic rationality, the method for finding out the truth, as a terminal value. As I said above, if you cannot understand the difference between these two concepts you have a lot of reading to do.</p>
<p>&gt;In every post on this thread you insinuating that your interlocutor is stupid, rather than politely addressing their arguments. Perhaps it is you that needs to self-examine, my friend.</p>
<p>I&#8217;m not the one who was caught lying and is desperately trying to backtrack and shift blame. Being able to admit when you&#8217;re wrong is considered one of the first steps to become more rational, hopefully you&#8217;ll learn it someday.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: CriticallyDisappointed		</title>
		<link>https://slatestarcodex.com/2015/03/04/a-series-of-unprincipled-exceptions/#comment-189244</link>

		<dc:creator><![CDATA[CriticallyDisappointed]]></dc:creator>
		<pubDate>Thu, 12 Mar 2015 21:17:54 +0000</pubDate>
		<guid isPermaLink="false">http://slatestarcodex.com/?p=3571#comment-189244</guid>

					<description><![CDATA[I have been on the hunt for something that really deals with the issue of animal rights with complexity since reading this post. I found this: http://michaelpollan.com/articles-archive/an-animals-place/

[Trigger warning, I want to talk about an &quot;edge case&quot; that includes elements of violence and dehumanization]
As an addendum to the above, I think I&#039;ve also come to agree with Daniel Dennet&#039;s view that there are fundamental differences in cognition between humans and most animals. There are certainly interesting edge cases around this. I would propose one: how should you morally treat a brain-injured human reduced to chicken-level intelligence, requiring constant care to keep alive, and constant supervision or restraint to prevent the human from assaulting other people? 
Suppose I were to, in advance, declare that I regard such as state as nightmarish and that I would request physician-assisted-suicide were I in such a state, to be added to my standing request to have my life terminated if I enter a persistent vegetative state. 
Would we consider the brain-injured human &quot;human,&quot; or morally imperative to treat as human, only for surface, aesthetic, or emotional reasons? 

I also think the earlier mentioned argument about how some ideas about animal rights could be argued to give humans a moral imperative to destroy all of the natural world to prevent suffering is, well, both funny and a good point against the way that whole mess of arguments forms a flawed paradigm.  I think the whole series of arguments really does hinge on the premise that animal pain can be considered to have moral equivalence to human suffering. Without that assertion, it doesn&#039;t fly, and I think there are reasonable arguments against that assertion. 

The difficulty with this topic, I think, is that it is immensely easy to accuse anyone who defends meat-eating as selfish, whether they are doing it for pleasure or for genuine health reasons (a whole other barrel of eels). This overlays all the arguments in a very detrimental way, I think.]]></description>
			<content:encoded><![CDATA[<p>I have been on the hunt for something that really deals with the issue of animal rights with complexity since reading this post. I found this: <a rel="nofollow"href="http://michaelpollan.com/articles-archive/an-animals-place/" rel="nofollow ugc">http://michaelpollan.com/articles-archive/an-animals-place/</a></p>
<p>[Trigger warning, I want to talk about an &#8220;edge case&#8221; that includes elements of violence and dehumanization]<br />
As an addendum to the above, I think I&#8217;ve also come to agree with Daniel Dennet&#8217;s view that there are fundamental differences in cognition between humans and most animals. There are certainly interesting edge cases around this. I would propose one: how should you morally treat a brain-injured human reduced to chicken-level intelligence, requiring constant care to keep alive, and constant supervision or restraint to prevent the human from assaulting other people?<br />
Suppose I were to, in advance, declare that I regard such as state as nightmarish and that I would request physician-assisted-suicide were I in such a state, to be added to my standing request to have my life terminated if I enter a persistent vegetative state.<br />
Would we consider the brain-injured human &#8220;human,&#8221; or morally imperative to treat as human, only for surface, aesthetic, or emotional reasons? </p>
<p>I also think the earlier mentioned argument about how some ideas about animal rights could be argued to give humans a moral imperative to destroy all of the natural world to prevent suffering is, well, both funny and a good point against the way that whole mess of arguments forms a flawed paradigm.  I think the whole series of arguments really does hinge on the premise that animal pain can be considered to have moral equivalence to human suffering. Without that assertion, it doesn&#8217;t fly, and I think there are reasonable arguments against that assertion. </p>
<p>The difficulty with this topic, I think, is that it is immensely easy to accuse anyone who defends meat-eating as selfish, whether they are doing it for pleasure or for genuine health reasons (a whole other barrel of eels). This overlays all the arguments in a very detrimental way, I think.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Citizensearth		</title>
		<link>https://slatestarcodex.com/2015/03/04/a-series-of-unprincipled-exceptions/#comment-189086</link>

		<dc:creator><![CDATA[Citizensearth]]></dc:creator>
		<pubDate>Thu, 12 Mar 2015 12:05:42 +0000</pubDate>
		<guid isPermaLink="false">http://slatestarcodex.com/?p=3571#comment-189086</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://slatestarcodex.com/2015/03/04/a-series-of-unprincipled-exceptions/#comment-187446&quot;&gt;TheAncientGeek&lt;/a&gt;.

This is a selective misrepresentation and nonsense. Rationality is frequently used on LessWrong as a shorthand for instrumental rationality. And that&#039;s the way &lt;i&gt;you were using the word&lt;/i&gt;. I enjoy reading on LessWrong and I have no problem with Eliezer, just this particular way of using the term. In every post on this thread you insinuating that your interlocutor is stupid, rather than politely addressing their arguments. Perhaps it is you that needs to self-examine, my friend.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a rel="nofollow"href="https://slatestarcodex.com/2015/03/04/a-series-of-unprincipled-exceptions/#comment-187446">TheAncientGeek</a>.</p>
<p>This is a selective misrepresentation and nonsense. Rationality is frequently used on LessWrong as a shorthand for instrumental rationality. And that&#8217;s the way <i>you were using the word</i>. I enjoy reading on LessWrong and I have no problem with Eliezer, just this particular way of using the term. In every post on this thread you insinuating that your interlocutor is stupid, rather than politely addressing their arguments. Perhaps it is you that needs to self-examine, my friend.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: mental models as a fandom &#124; englebright		</title>
		<link>https://slatestarcodex.com/2015/03/04/a-series-of-unprincipled-exceptions/#comment-188898</link>

		<dc:creator><![CDATA[mental models as a fandom &#124; englebright]]></dc:creator>
		<pubDate>Tue, 10 Mar 2015 20:04:57 +0000</pubDate>
		<guid isPermaLink="false">http://slatestarcodex.com/?p=3571#comment-188898</guid>

					<description><![CDATA[[&#8230;] tremendous, and reminds me of “megagames” that comrades have described to me. Then I read this, which is one of the most that kind of thing things I can imagine. As Comrade Josef pointed out to [&#8230;]]]></description>
			<content:encoded><![CDATA[<p>[&#8230;] tremendous, and reminds me of “megagames” that comrades have described to me. Then I read this, which is one of the most that kind of thing things I can imagine. As Comrade Josef pointed out to [&#8230;]</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: efnrer		</title>
		<link>https://slatestarcodex.com/2015/03/04/a-series-of-unprincipled-exceptions/#comment-188830</link>

		<dc:creator><![CDATA[efnrer]]></dc:creator>
		<pubDate>Tue, 10 Mar 2015 11:25:27 +0000</pubDate>
		<guid isPermaLink="false">http://slatestarcodex.com/?p=3571#comment-188830</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://slatestarcodex.com/2015/03/04/a-series-of-unprincipled-exceptions/#comment-187446&quot;&gt;TheAncientGeek&lt;/a&gt;.

@Citizensearth If you can&#039;t see the need to differentiate between the truth and the method for finding the truth, then you have a lot of reading to do.

Also:
&#062;The problem is that Eliezer redefined rationality to mean what philosophy has always generally considered to be a subset of rationality, probably best mapped to instrumental rationality. Going against established terms is not forbidden if you really must, but expecting people outside Less Wrong to conform to an unconventional definintion and go against the established norm is, well, irrational. I’ll leave you to speculate as to what motivates someone to explicitly exclude things like truth-seeking as part of their definition of rationality.

This is false. Had you read the article I linked above you would have found this part:

&#062;What Do We Mean By &quot;Rationality&quot;? 

&#062;We mean:

&#062;Epistemic rationality: believing, and updating on evidence, so as to systematically improve the correspondence between your map and the territory.  The art of obtaining beliefs that correspond to reality as closely as possible.  This correspondence is commonly termed &quot;truth&quot; or &quot;accuracy&quot;, and we&#039;re happy to call it that.

&#062;Instrumental rationality: achieving your values.  Not necessarily &quot;your values&quot; in the sense of being selfish values or unshared values: &quot;your values&quot; means anything you care about.  The art of choosing actions that steer the future toward outcomes ranked higher in your preferences.  On LW we sometimes refer to this as &quot;winning&quot;.

Spreading lies about things you haven&#039;t even read is a character flaw I suggest you work on.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a rel="nofollow"href="https://slatestarcodex.com/2015/03/04/a-series-of-unprincipled-exceptions/#comment-187446">TheAncientGeek</a>.</p>
<p>@Citizensearth If you can&#8217;t see the need to differentiate between the truth and the method for finding the truth, then you have a lot of reading to do.</p>
<p>Also:<br />
&gt;The problem is that Eliezer redefined rationality to mean what philosophy has always generally considered to be a subset of rationality, probably best mapped to instrumental rationality. Going against established terms is not forbidden if you really must, but expecting people outside Less Wrong to conform to an unconventional definintion and go against the established norm is, well, irrational. I’ll leave you to speculate as to what motivates someone to explicitly exclude things like truth-seeking as part of their definition of rationality.</p>
<p>This is false. Had you read the article I linked above you would have found this part:</p>
<p>&gt;What Do We Mean By &#8220;Rationality&#8221;? </p>
<p>&gt;We mean:</p>
<p>&gt;Epistemic rationality: believing, and updating on evidence, so as to systematically improve the correspondence between your map and the territory.  The art of obtaining beliefs that correspond to reality as closely as possible.  This correspondence is commonly termed &#8220;truth&#8221; or &#8220;accuracy&#8221;, and we&#8217;re happy to call it that.</p>
<p>&gt;Instrumental rationality: achieving your values.  Not necessarily &#8220;your values&#8221; in the sense of being selfish values or unshared values: &#8220;your values&#8221; means anything you care about.  The art of choosing actions that steer the future toward outcomes ranked higher in your preferences.  On LW we sometimes refer to this as &#8220;winning&#8221;.</p>
<p>Spreading lies about things you haven&#8217;t even read is a character flaw I suggest you work on.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: David Friedman		</title>
		<link>https://slatestarcodex.com/2015/03/04/a-series-of-unprincipled-exceptions/#comment-188803</link>

		<dc:creator><![CDATA[David Friedman]]></dc:creator>
		<pubDate>Tue, 10 Mar 2015 06:31:27 +0000</pubDate>
		<guid isPermaLink="false">http://slatestarcodex.com/?p=3571#comment-188803</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://slatestarcodex.com/2015/03/04/a-series-of-unprincipled-exceptions/#comment-188084&quot;&gt;CJB&lt;/a&gt;.

I&#039;m not sure how one goes about proving moral arguments, but let me suggest that your intuition, while very natural, reflects the fact that humans are  bad at intuiting very large numbers or very small probabilities. 

But let me try:

We observe that humans are willing to trade a very small probability of dying against quite minor pleasures, such as the pleasure of eating an ice cream cone when you know you are a little overweight or of going to a movie when you know that if you went to no movies this year you could spend the money on an extra medical checkup which would have some (very small) chance of saving your life.

So suppose you make it a gamble, offered to a billion people—a certainty of a dollar to spend against one chance in a billion of being murdered. Any reason to think they would not accept, given that people routinely accept larger risks than that for smaller payoffs? If they would all accept, what&#039;s your basis for saying that murder is more important than the billion dollars?]]></description>
			<content:encoded><![CDATA[<p>In reply to <a rel="nofollow"href="https://slatestarcodex.com/2015/03/04/a-series-of-unprincipled-exceptions/#comment-188084">CJB</a>.</p>
<p>I&#8217;m not sure how one goes about proving moral arguments, but let me suggest that your intuition, while very natural, reflects the fact that humans are  bad at intuiting very large numbers or very small probabilities. </p>
<p>But let me try:</p>
<p>We observe that humans are willing to trade a very small probability of dying against quite minor pleasures, such as the pleasure of eating an ice cream cone when you know you are a little overweight or of going to a movie when you know that if you went to no movies this year you could spend the money on an extra medical checkup which would have some (very small) chance of saving your life.</p>
<p>So suppose you make it a gamble, offered to a billion people—a certainty of a dollar to spend against one chance in a billion of being murdered. Any reason to think they would not accept, given that people routinely accept larger risks than that for smaller payoffs? If they would all accept, what&#8217;s your basis for saying that murder is more important than the billion dollars?</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: David Friedman		</title>
		<link>https://slatestarcodex.com/2015/03/04/a-series-of-unprincipled-exceptions/#comment-188802</link>

		<dc:creator><![CDATA[David Friedman]]></dc:creator>
		<pubDate>Tue, 10 Mar 2015 06:22:09 +0000</pubDate>
		<guid isPermaLink="false">http://slatestarcodex.com/?p=3571#comment-188802</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://slatestarcodex.com/2015/03/04/a-series-of-unprincipled-exceptions/#comment-187774&quot;&gt;Peter&lt;/a&gt;.

The problem with average utilitarianism was pointed out long ago by Mead, one of several distinguished scholars who stole ideas of mine before I thought of them. His version is to imagine a world with two cities, A and B. Both are very attractive places filled with happy people, but the people in A are a little happier than the people in B and the two cities have no significant dealings with each other.

Would it be a good thing if a plague painlessly killed everyone in B? Average utility goes up.

My version was to imagine someone who lives his entire life on an island, never interacting with another human being. Does it make sense to say that his life is a good thing if the rest of the world&#039;s population averages a little less happy than he does, a bad thing if they average little happier--his life being exactly the same in both cases?]]></description>
			<content:encoded><![CDATA[<p>In reply to <a rel="nofollow"href="https://slatestarcodex.com/2015/03/04/a-series-of-unprincipled-exceptions/#comment-187774">Peter</a>.</p>
<p>The problem with average utilitarianism was pointed out long ago by Mead, one of several distinguished scholars who stole ideas of mine before I thought of them. His version is to imagine a world with two cities, A and B. Both are very attractive places filled with happy people, but the people in A are a little happier than the people in B and the two cities have no significant dealings with each other.</p>
<p>Would it be a good thing if a plague painlessly killed everyone in B? Average utility goes up.</p>
<p>My version was to imagine someone who lives his entire life on an island, never interacting with another human being. Does it make sense to say that his life is a good thing if the rest of the world&#8217;s population averages a little less happy than he does, a bad thing if they average little happier&#8211;his life being exactly the same in both cases?</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
